{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Importing Libraries & Functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from IPython.lib.display import Audio\n",
    "framerate = 4410\n",
    "# play_time_seconds = 3\n",
    "# t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "# audio_data = np.sin(2*np.pi*300*t) + np.sin(2*np.pi*240*t)\n",
    "# Audio(audio_data, rate=framerate, autoplay=True)\n",
    "\n",
    "#SciKit Learn \n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def outputs(cm):\n",
    "    acc = np.round((cm.ravel()[0]+cm.ravel()[3])/sum(cm.ravel()),4)\n",
    "    tpr = np.round(cm.ravel()[3] / (cm.ravel()[3] + cm.ravel()[2]),4)\n",
    "    fpr = np.round(cm.ravel()[1] / (cm.ravel()[1] + cm.ravel()[0]),4)\n",
    "    outputs = [acc, tpr, fpr]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data + Train/Test Split\n",
    "\n",
    "First, we load in the two CSVs from the FP1_CUM_STATS and FP2_XFACTOR notebooks and join them on the 'GAME_ID' as the key. We then set our training data to be all data from the 2010 Season to the 2016 Season, and our testing data to be all data from the 2017 Season to the 2018 Season. The cell below performs this task, and outputs the Counts and Percentages of Training and Testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 12841 rows -- 83.0%\n",
      "Testing Data: 2624 rows -- 17.0%\n"
     ]
    }
   ],
   "source": [
    "# Splitting Data\n",
    "FP1 = pd.read_csv('FP1.csv')\n",
    "FP2 = pd.read_csv('FP2.csv')\n",
    "df = pd.merge(FP1, FP2, on = 'GAME_ID').rename(columns={'SEASON_y':'SEASON'})\n",
    "df = df.drop(['TIMESTAMP','HOME_TEAM_NAME', 'SEASON_x'], axis=1)\n",
    "\n",
    "train_df = df[df['SEASON'].between(2007,2016)]\n",
    "test_df = df[df['SEASON'].between(2017,2018)]\n",
    "\n",
    "train_df, test_df = train_df.drop(['SEASON', 'GAME_ID'], axis = 1), test_df.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train, y_train = train_df.drop('HOME_TEAM_WINS', axis = 1), train_df['HOME_TEAM_WINS']\n",
    "X_test, y_test = test_df.drop('HOME_TEAM_WINS', axis = 1), test_df['HOME_TEAM_WINS']\n",
    "\n",
    "train_prop = np.round(len(X_train) / (len(df)), 3)\n",
    "test_prop = np.round(len(X_test) / (len(df)), 3)\n",
    "\n",
    "print('Training Data: ' + str(len(X_train)) + ' rows -- ' + str(np.round(train_prop*100,2)) + '%')\n",
    "print('Testing Data: ' + str(len(X_test)) + ' rows -- ' + str(np.round(test_prop*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#0: Baseline (Dummy) Model\n",
    "\n",
    "Our dummy model predicts the most frequent label in the training set, which is HOME_TEAM_WINS = 1. This is the same as always predicting the home team to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[   0 1079]\n",
      " [   0 1545]]\n",
      "\n",
      "Accuracy :  0.5888\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model \n",
    "model_0 = DummyClassifier(strategy = \"most_frequent\")\n",
    "model_0.fit(X_train, y_train)\n",
    "y_pred_0 = model_0.predict(X_test)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_0 = confusion_matrix(y_test, y_pred_0)\n",
    "outputs_0 = outputs(cm_0)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_0) \n",
    "print (\"\\nAccuracy : \", outputs_0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Feature Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Implementing PCA\n",
    "\n",
    "# # Fitting Scaler to X_train \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# # Apply transform to both the training set and the test set.\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Make an instance of the Model ==> 0.95 means 95% of variance explained\n",
    "# pca = PCA(n_components = 0.999999999999999)\n",
    "# pca.fit(X_train, y_train)\n",
    "# print('Number of Components: ', pca.n_components_)\n",
    "\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Scree Plots\n",
    "# yy = pca.explained_variance_ratio_\n",
    "# xx = np.arange(1,len(yy)+1)\n",
    "\n",
    "# sns.set_style('darkgrid')\n",
    "# fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "# plt.plot(xx, yy)\n",
    "# plt.xlabel('PCA #', fontsize = 16), plt.ylabel('Explained Variance %', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#1: Logistic Regression\n",
    "\n",
    "The cell below cretes a Logistic Regression model and calculates the outputs to be presented later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 474  605]\n",
      " [ 282 1263]]\n",
      "\n",
      "Accuracy :  0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "# formula = 'HOME_TEAM_WINS' + '~' + ' + '.join([col for col in train_df.columns if 'HOME_TEAM_WINS' not in col])\n",
    "# model = smf.logit(formula, data = train_df).fit_regularized(maxiter = 1000)\n",
    "\n",
    "# y_prob_1 = model_1.predict_proba(test_df)\n",
    "# y_pred_1 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_1[:,1]], index = y_test.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_1 = confusion_matrix(y_test, y_pred_1)\n",
    "# outputs_1 = outputs(cm_1)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_1) \n",
    "# print (\"\\nAccuracy : \", outputs_1[0]) \n",
    "# out = pd.DataFrame(index = model.params.index, data = {'Coefficient' : np.round(model.params,4), 'P-Values' : model.pvalues})\n",
    "\n",
    "# # Logistic Regression\n",
    "\n",
    "model_1 = LogisticRegression(random_state = 69, tol = 0.001, max_iter = 100, \n",
    "                             verbose = 3, n_jobs = -1, penalty = 'l2', \n",
    "                             fit_intercept = True, C = 0.25)\n",
    "model_1.fit(X_train, y_train)\n",
    "y_prob_1 = model_1.predict_proba(X_test)\n",
    "y_pred_1 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_1[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_1 = confusion_matrix(y_test, y_pred_1)\n",
    "outputs_1 = outputs(cm_1)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_1) \n",
    "print (\"\\nAccuracy : \", outputs_1[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# cm_df = pd.DataFrame(cm_1).rename(columns = {0: 'TRUE HOME WINS', 1: 'TRUE AWAY WINS'})\n",
    "# cm_df = cm_df.rename({0: 'PREDICT HOME WINS', 1: 'PREDICT AWAY WINS'})\n",
    "# print(cm_df.to_latex())\n",
    "\n",
    "# out_df = pd.DataFrame(outputs_1).rename({0:'Accuracy', 1:'TPR', 2:'FPR'})\n",
    "# out_df = out_df.rename(columns = {0:'Test Set Statistic'})\n",
    "# print(out_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#2: Random Forest \n",
    "\n",
    "This cell below is a Random Forest model with cross-validation on various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Random Forest\n",
    "\n",
    "# tic = time.time()\n",
    "\n",
    "# model_2_acc_list = []\n",
    "# max_feature_list = np.linspace(1, len(X_train.columns), 100, dtype = 'int32')\n",
    "\n",
    "# for i in max_feature_list: \n",
    "    \n",
    "#     model_2 = RandomForestClassifier(max_features = i, n_estimators = 250, random_state = 69, \n",
    "#                                      n_jobs = -1, min_samples_leaf = 5)\n",
    "#     model_2.fit(X_train, y_train)\n",
    "   \n",
    "#     y_prob_2 = model_2.predict_proba(X_val)\n",
    "#     y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_val.index)\n",
    "    \n",
    "#     # Confusion Matrix & Outputs \n",
    "#     cm_2 = confusion_matrix(y_val, y_pred_2)\n",
    "#     accuracy = outputs(cm_2)[0]\n",
    "    \n",
    "#     model_2_acc_list.append(accuracy)\n",
    "#     print(f'Max Features = {str(i)} -- {time.time()-tic}')\n",
    "    \n",
    "# opt_max_features = max_feature_list[np.argmax(model_2_acc_list)]\n",
    "    \n",
    "# print('Optimal Max Features: ', opt_max_features)\n",
    "# print('Optimal Validation Accuracy: ', np.max(model_2_acc_list))\n",
    "\n",
    "# toc = time.time()\n",
    "# minutes, seconds = np.floor((toc-tic)/60), np.round((toc-tic) - (60*minutes),3)\n",
    "# print(f'Elapsed Time:  {int(minutes)} min  {seconds} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 431  648]\n",
      " [ 247 1298]]\n",
      "\n",
      "Accuracy :  0.6589\n"
     ]
    }
   ],
   "source": [
    "# First Model w Reasonable Values \n",
    "\n",
    "model_2 = RandomForestClassifier(n_estimators = 1000, random_state = 69, n_jobs = -1, \n",
    "                                 min_samples_split = 5, min_samples_leaf = 5, verbose = 3)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "y_prob_2 = model_2.predict_proba(X_test)\n",
    "y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "# \n",
    "# Confusion Matrix & Outputs \n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "outputs_2 = outputs(cm_2)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "print (\"\\nAccuracy : \", outputs_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>cum_10_G_PLUS_MINUS_away_H2H</td>\n",
       "      <td>0.009961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>cum_5_F_PLUS_MINUS_away_H2H</td>\n",
       "      <td>0.009459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>cum_5_G_PLUS_MINUS_away_H2H</td>\n",
       "      <td>0.009407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>cum_5_G_PLUS_MINUS_home_H2H</td>\n",
       "      <td>0.009193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>cum_10_G_PLUS_MINUS_home</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>cum_10_F_PLUS_MINUS_home</td>\n",
       "      <td>0.009064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>cum_10_G_PLUS_MINUS_home_H2H</td>\n",
       "      <td>0.008848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>cum_10_F_PLUS_MINUS_away_H2H</td>\n",
       "      <td>0.008273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>cum_5_F_PLUS_MINUS_home_H2H</td>\n",
       "      <td>0.008163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>cum_10_G_PLUS_MINUS_away</td>\n",
       "      <td>0.007913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>cum_10_F_PLUS_MINUS_home_H2H</td>\n",
       "      <td>0.007853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>cum_10_C_PLUS_MINUS_home</td>\n",
       "      <td>0.007350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>cum_10_F_PLUS_MINUS_away</td>\n",
       "      <td>0.007156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>cum_5_G_PLUS_MINUS_home</td>\n",
       "      <td>0.006975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>cum_5_C_PLUS_MINUS_away_H2H</td>\n",
       "      <td>0.006288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>cum_10_C_PLUS_MINUS_away_H2H</td>\n",
       "      <td>0.005799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>cum_5_F_PLUS_MINUS_home</td>\n",
       "      <td>0.005384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>cum_5_C_PLUS_MINUS_home_H2H</td>\n",
       "      <td>0.005299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>cum_10_C_PLUS_MINUS_home_H2H</td>\n",
       "      <td>0.005109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>cum_5_C_PLUS_MINUS_home</td>\n",
       "      <td>0.005049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>cum_10_C_PLUS_MINUS_away</td>\n",
       "      <td>0.004666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>cum_5_G_PLUS_MINUS_away</td>\n",
       "      <td>0.004218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>cum_5_F_PLUS_MINUS_away</td>\n",
       "      <td>0.003904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>cum_F_PLUS_MINUS_home</td>\n",
       "      <td>0.003849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>HOME_TEAM_home_win_pct_past4yrs</td>\n",
       "      <td>0.003824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>cum_G_PLUS_MINUS_home</td>\n",
       "      <td>0.003601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>cum_G_PLUS_MINUS_away</td>\n",
       "      <td>0.003314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>cum_5_FG_PCT_away_H2H</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>cum_10_FG_PCT_away_H2H</td>\n",
       "      <td>0.002928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cum_10_FG_PCT_home_H2H</td>\n",
       "      <td>0.002823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Feature  Importance Score\n",
       "773     cum_10_G_PLUS_MINUS_away_H2H          0.009961\n",
       "766      cum_5_F_PLUS_MINUS_away_H2H          0.009459\n",
       "772      cum_5_G_PLUS_MINUS_away_H2H          0.009407\n",
       "376      cum_5_G_PLUS_MINUS_home_H2H          0.009193\n",
       "374         cum_10_G_PLUS_MINUS_home          0.009114\n",
       "368         cum_10_F_PLUS_MINUS_home          0.009064\n",
       "377     cum_10_G_PLUS_MINUS_home_H2H          0.008848\n",
       "767     cum_10_F_PLUS_MINUS_away_H2H          0.008273\n",
       "370      cum_5_F_PLUS_MINUS_home_H2H          0.008163\n",
       "770         cum_10_G_PLUS_MINUS_away          0.007913\n",
       "371     cum_10_F_PLUS_MINUS_home_H2H          0.007853\n",
       "362         cum_10_C_PLUS_MINUS_home          0.007350\n",
       "764         cum_10_F_PLUS_MINUS_away          0.007156\n",
       "373          cum_5_G_PLUS_MINUS_home          0.006975\n",
       "760      cum_5_C_PLUS_MINUS_away_H2H          0.006288\n",
       "761     cum_10_C_PLUS_MINUS_away_H2H          0.005799\n",
       "367          cum_5_F_PLUS_MINUS_home          0.005384\n",
       "364      cum_5_C_PLUS_MINUS_home_H2H          0.005299\n",
       "365     cum_10_C_PLUS_MINUS_home_H2H          0.005109\n",
       "361          cum_5_C_PLUS_MINUS_home          0.005049\n",
       "758         cum_10_C_PLUS_MINUS_away          0.004666\n",
       "769          cum_5_G_PLUS_MINUS_away          0.004218\n",
       "763          cum_5_F_PLUS_MINUS_away          0.003904\n",
       "366            cum_F_PLUS_MINUS_home          0.003849\n",
       "829  HOME_TEAM_home_win_pct_past4yrs          0.003824\n",
       "372            cum_G_PLUS_MINUS_home          0.003601\n",
       "768            cum_G_PLUS_MINUS_away          0.003314\n",
       "406            cum_5_FG_PCT_away_H2H          0.003008\n",
       "407           cum_10_FG_PCT_away_H2H          0.002928\n",
       "11            cum_10_FG_PCT_home_H2H          0.002823"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting top features\n",
    "out = pd.DataFrame({'Feature' : list(X_train.columns), \n",
    "                   'Importance Score': model_2.feature_importances_}).sort_values('Importance Score', ascending = False)\n",
    "top = out.iloc[:30,:]['Feature'].values\n",
    "# top = out[out['Importance Score'] > 0.0018]['Feature'].values\n",
    "out.iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Max Features:  {'max_features': 2}\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 476  603]\n",
      " [ 305 1240]]\n",
      "\n",
      "Accuracy :  0.654\n"
     ]
    }
   ],
   "source": [
    "X_train_top = X_train[top]\n",
    "\n",
    "grid = {'max_features': np.arange(1, len(X_train_top.columns)+1)}\n",
    "\n",
    "\n",
    "model_2 = RandomForestClassifier(n_estimators = 500, random_state = 69, n_jobs = -1, \n",
    "                                 min_samples_split = 5, min_samples_leaf = 5)\n",
    "\n",
    "model_2 = GridSearchCV(model_2, param_grid=grid, scoring='accuracy', \n",
    "                       cv=5, verbose=2, n_jobs = -1).fit(X_train_top, y_train)\n",
    "\n",
    "print('Optimal Max Features: ', model_2.best_params_)\n",
    "y_prob_2 = model_2.best_estimator_.predict_proba(X_test[top])\n",
    "y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "outputs_2 = outputs(cm_2)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "print (\"\\nAccuracy : \", outputs_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 476  603]\n",
      " [ 305 1240]]\n",
      "\n",
      "Accuracy :  0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Run our Best Model TOP\n",
    "\n",
    "model_2 = RandomForestClassifier(max_features = 2, n_estimators = 500, random_state = 69, \n",
    "                                 n_jobs = -1, min_samples_split = 5, min_samples_leaf = 5, verbose = 1)\n",
    "model_2.fit(X_train_top, y_train)\n",
    "\n",
    "y_prob_2 = model_2.predict_proba(X_test[top])\n",
    "y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "# \n",
    "# Confusion Matrix & Outputs \n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "outputs_2 = outputs(cm_2)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "print (\"\\nAccuracy : \", outputs_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# max_features_RF = RF_model.cv_results_['param_max_features'].data\n",
    "# acc_scores_RF = RF_model.cv_results_['mean_test_score']\n",
    "\n",
    "# sns.set_style('darkgrid')\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.title('\\nMean Cross-Validation Accuracy vs. max_features\\n', fontsize=16)\n",
    "# plt.xlabel('max_features', fontsize=16)\n",
    "# plt.ylabel('Mean Cross-Validation Accuracy', fontsize=16)\n",
    "# plt.plot(max_features_RF, acc_scores_RF, linewidth=3, color='orange')\n",
    "# plt.scatter(max_features_RF[acc_scores_RF.argmax(axis=0)], np.max(acc_scores_RF), s=125, marker='o')\n",
    "# plt.grid(True, which='both')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#3: XGBoost\n",
    "\n",
    "The cells below go through an entire process of creating and cross-validating an XGBoost model. First, we set default parameters and using the XGB Cross-Validation function to determine the optimal number of trees for the specified learning rate. We then use this number of estimators to cross-validate and select an approximate values for *max_depth* and *min_child_weight*, which we then find more optimally by shrinking the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Combining train and val\n",
    "\n",
    "train_df_3 = df[df['SEASON'].between(2007,2016)]\n",
    "\n",
    "train_df_3 = train_df_3.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train_3, y_train_3 = train_df_3.drop('HOME_TEAM_WINS', axis = 1), train_df_3['HOME_TEAM_WINS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Defining the Cross-Validation Function \n",
    "def modelfit(alg, train, predictors, useTrainCV = True, cv_folds = 5, early_stopping_rounds = 50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train[predictors].values, label=train[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold = cv_folds,\n",
    "                          metrics = 'error', early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "        alg.set_params(n_estimators = cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train[predictors], train['HOME_TEAM_WINS'], eval_metric = 'error')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train[predictors])\n",
    "    train_predprob = alg.predict_proba(train[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(train['HOME_TEAM_WINS'].values, train_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(train['HOME_TEAM_WINS'], train_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the first cell below and the final number it outputs in square brackets is the n_estimators that you plug into gridsearchcv object below.\n",
    "\n",
    "STEP \\#1 : Calculate n_estimators for learning_rate = 0.1 (VERY HIGH)\n",
    "\n",
    "STEP \\#2 : Use Cross-Validation to tune the other hyperparameters\n",
    "\n",
    "STEP \\#3 : Now reduce the learning rate to 0.01 or something smaller and calculate the new n_estimators for the hyperparameters found above. Obviously, this will mean much more trees are used.\n",
    "\n",
    "STEP \\#4 : Run our final model using the learning rate set above, the hyperparameters found using cross-validation, and the corresponding (now higher) n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.30743+0.00701\ttest-error:0.34211+0.00763\n",
      "[1]\ttrain-error:0.29597+0.00441\ttest-error:0.33541+0.01080\n",
      "[2]\ttrain-error:0.28857+0.00521\ttest-error:0.33284+0.00777\n",
      "[3]\ttrain-error:0.28421+0.00481\ttest-error:0.33043+0.00546\n",
      "[4]\ttrain-error:0.28035+0.00391\ttest-error:0.32786+0.00736\n",
      "[5]\ttrain-error:0.27778+0.00277\ttest-error:0.32934+0.00575\n",
      "[6]\ttrain-error:0.27632+0.00419\ttest-error:0.32871+0.00749\n",
      "[7]\ttrain-error:0.27459+0.00516\ttest-error:0.32614+0.00784\n",
      "[8]\ttrain-error:0.27217+0.00425\ttest-error:0.32404+0.01012\n",
      "[9]\ttrain-error:0.26955+0.00450\ttest-error:0.32326+0.00956\n",
      "[10]\ttrain-error:0.26778+0.00359\ttest-error:0.32256+0.00879\n",
      "[11]\ttrain-error:0.26598+0.00419\ttest-error:0.32311+0.00988\n",
      "[12]\ttrain-error:0.26474+0.00307\ttest-error:0.32069+0.00922\n",
      "[13]\ttrain-error:0.26308+0.00306\ttest-error:0.32030+0.00915\n",
      "[14]\ttrain-error:0.26020+0.00282\ttest-error:0.31952+0.00832\n",
      "[15]\ttrain-error:0.25859+0.00295\ttest-error:0.32100+0.00865\n",
      "[16]\ttrain-error:0.25621+0.00329\ttest-error:0.31976+0.00959\n",
      "[17]\ttrain-error:0.25524+0.00322\ttest-error:0.31898+0.00874\n",
      "[18]\ttrain-error:0.25284+0.00370\ttest-error:0.31836+0.00815\n",
      "[19]\ttrain-error:0.25037+0.00228\ttest-error:0.31711+0.00957\n",
      "[20]\ttrain-error:0.24842+0.00251\ttest-error:0.31742+0.00882\n",
      "[21]\ttrain-error:0.24729+0.00255\ttest-error:0.31719+0.00890\n",
      "[22]\ttrain-error:0.24486+0.00183\ttest-error:0.31750+0.00947\n",
      "[23]\ttrain-error:0.24223+0.00224\ttest-error:0.31750+0.00867\n",
      "[24]\ttrain-error:0.24095+0.00251\ttest-error:0.31711+0.00693\n",
      "[25]\ttrain-error:0.23978+0.00260\ttest-error:0.31625+0.00830\n",
      "[26]\ttrain-error:0.23799+0.00274\ttest-error:0.31672+0.00836\n",
      "[27]\ttrain-error:0.23590+0.00239\ttest-error:0.31602+0.00856\n",
      "[28]\ttrain-error:0.23419+0.00264\ttest-error:0.31617+0.00859\n",
      "[29]\ttrain-error:0.23224+0.00260\ttest-error:0.31586+0.00832\n",
      "[30]\ttrain-error:0.23131+0.00296\ttest-error:0.31571+0.00953\n",
      "[31]\ttrain-error:0.22960+0.00333\ttest-error:0.31586+0.00988\n",
      "[32]\ttrain-error:0.22763+0.00354\ttest-error:0.31415+0.00967\n",
      "[33]\ttrain-error:0.22670+0.00411\ttest-error:0.31508+0.00936\n",
      "[34]\ttrain-error:0.22485+0.00508\ttest-error:0.31586+0.00867\n",
      "[35]\ttrain-error:0.22383+0.00485\ttest-error:0.31703+0.00761\n",
      "[36]\ttrain-error:0.22185+0.00532\ttest-error:0.31672+0.00770\n",
      "[37]\ttrain-error:0.21963+0.00500\ttest-error:0.31594+0.00744\n",
      "[38]\ttrain-error:0.21883+0.00491\ttest-error:0.31524+0.00760\n",
      "[39]\ttrain-error:0.21696+0.00561\ttest-error:0.31602+0.00664\n",
      "[40]\ttrain-error:0.21484+0.00547\ttest-error:0.31656+0.00592\n",
      "[41]\ttrain-error:0.21355+0.00500\ttest-error:0.31734+0.00514\n",
      "[42]\ttrain-error:0.21102+0.00474\ttest-error:0.31874+0.00589\n",
      "[43]\ttrain-error:0.20968+0.00471\ttest-error:0.31742+0.00695\n",
      "[44]\ttrain-error:0.20804+0.00473\ttest-error:0.31664+0.00768\n",
      "[45]\ttrain-error:0.20658+0.00440\ttest-error:0.31789+0.00720\n",
      "[46]\ttrain-error:0.20440+0.00399\ttest-error:0.31750+0.00785\n",
      "[47]\ttrain-error:0.20255+0.00447\ttest-error:0.31882+0.00939\n",
      "[48]\ttrain-error:0.20100+0.00448\ttest-error:0.31773+0.00840\n",
      "[49]\ttrain-error:0.19948+0.00415\ttest-error:0.31820+0.00675\n",
      "[50]\ttrain-error:0.19749+0.00362\ttest-error:0.31726+0.00823\n",
      "[51]\ttrain-error:0.19547+0.00406\ttest-error:0.31726+0.00725\n",
      "[52]\ttrain-error:0.19395+0.00422\ttest-error:0.31711+0.00841\n",
      "[53]\ttrain-error:0.19261+0.00467\ttest-error:0.31688+0.00838\n",
      "[54]\ttrain-error:0.19109+0.00410\ttest-error:0.31688+0.00807\n",
      "[55]\ttrain-error:0.18955+0.00464\ttest-error:0.31703+0.00700\n",
      "[56]\ttrain-error:0.18834+0.00460\ttest-error:0.31828+0.00839\n",
      "[57]\ttrain-error:0.18731+0.00477\ttest-error:0.31758+0.00717\n",
      "[58]\ttrain-error:0.18540+0.00461\ttest-error:0.31695+0.00575\n",
      "[59]\ttrain-error:0.18390+0.00477\ttest-error:0.31750+0.00587\n",
      "[60]\ttrain-error:0.18235+0.00501\ttest-error:0.31649+0.00578\n",
      "[61]\ttrain-error:0.18141+0.00497\ttest-error:0.31672+0.00647\n",
      "[62]\ttrain-error:0.17978+0.00428\ttest-error:0.31625+0.00566\n",
      "[63]\ttrain-error:0.17843+0.00368\ttest-error:0.31625+0.00530\n",
      "[64]\ttrain-error:0.17709+0.00379\ttest-error:0.31789+0.00566\n",
      "[65]\ttrain-error:0.17627+0.00380\ttest-error:0.31789+0.00557\n",
      "[66]\ttrain-error:0.17520+0.00359\ttest-error:0.31703+0.00612\n",
      "[67]\ttrain-error:0.17403+0.00412\ttest-error:0.31602+0.00608\n",
      "[68]\ttrain-error:0.17142+0.00377\ttest-error:0.31664+0.00558\n",
      "[69]\ttrain-error:0.17026+0.00399\ttest-error:0.31649+0.00656\n",
      "[70]\ttrain-error:0.16911+0.00370\ttest-error:0.31820+0.00552\n",
      "[71]\ttrain-error:0.16802+0.00326\ttest-error:0.31812+0.00615\n",
      "[72]\ttrain-error:0.16613+0.00327\ttest-error:0.31812+0.00631\n",
      "[73]\ttrain-error:0.16488+0.00293\ttest-error:0.31843+0.00710\n",
      "[74]\ttrain-error:0.16420+0.00305\ttest-error:0.31882+0.00616\n",
      "[75]\ttrain-error:0.16266+0.00325\ttest-error:0.31781+0.00662\n",
      "[76]\ttrain-error:0.16163+0.00317\ttest-error:0.31781+0.00635\n",
      "[77]\ttrain-error:0.16040+0.00269\ttest-error:0.31688+0.00699\n",
      "[78]\ttrain-error:0.15918+0.00236\ttest-error:0.31719+0.00770\n",
      "[79]\ttrain-error:0.15807+0.00315\ttest-error:0.31703+0.00746\n",
      "[80]\ttrain-error:0.15598+0.00394\ttest-error:0.31711+0.00740\n",
      "[81]\ttrain-error:0.15437+0.00405\ttest-error:0.31727+0.00767\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.758\n",
      "AUC Score (Train): 0.835561\n"
     ]
    }
   ],
   "source": [
    "# Determining Optimal Number of Estimators\n",
    "target = 'HOME_TEAM_WINS'\n",
    "train = pd.concat([X_train_3, y_train_3], axis = 1)\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "\n",
    "model_3 = XGBClassifier(learning_rate = 0.1, n_estimators = 1000, max_depth = 5, min_child_weight = 1,\n",
    "                     gamma = 0, subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic',\n",
    "                     nthread = 4, scale_pos_weight = 1, seed = 69)\n",
    "modelfit(model_3, train, X_train_3.columns, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:  7.2min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:57:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Approx. Parameters: \n",
      " {'max_depth': 3, 'min_child_weight': 3} 0.6632677963594208\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validating MAX_DEPTH and MIN_CHILD_WEIGHT\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=81, gamma=0, \n",
    "                                                  subsample=0.8, colsample_bytree=0.8, \n",
    "                                                  objective= 'binary:logistic', nthread=4, \n",
    "                                                  scale_pos_weight=1, seed=27), \n",
    "                        param_grid = param_test1, \n",
    "                        scoring = 'accuracy', n_jobs = -1, iid=False , cv=5, verbose = 3)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "print('Best Approx. Parameters: \\n', gsearch1.best_params_, gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  45 | elapsed:  1.5min remaining:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:01:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Parameters: \n",
      " {'max_depth': 2, 'min_child_weight': 4} 0.6689529727193024\n"
     ]
    }
   ],
   "source": [
    "# Reducing Search Space\n",
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[2,3,4]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=81, gamma=0, subsample=0.8, \n",
    "                                                  colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test2, scoring='accuracy', n_jobs=-1, iid=False, cv=5, verbose = 3)\n",
    "gsearch2.fit(train[predictors],train[target])\n",
    "print('Best Parameters: \\n', gsearch2.best_params_, gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tune *gamma* in the same way and then calculate the new optimal number of parameters for the new set of hyperparameters. We then tune *subsample*, *colsample_bytree*, and *reg_alpha* in the same way as the rest with the new optimal estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  25 | elapsed:   33.2s remaining:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  25 | elapsed:   33.3s remaining:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  25 | elapsed:   56.6s remaining:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   56.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:02:47] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.0}, 0.6689529727193024)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validating GAMMA\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=81, max_depth=2, \n",
    "                                                  min_child_weight=4, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, \n",
    "                                                  scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test3, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.34569+0.00356\ttest-error:0.35021+0.00850\n",
      "[1]\ttrain-error:0.34242+0.00348\ttest-error:0.34694+0.01157\n",
      "[2]\ttrain-error:0.33590+0.00651\ttest-error:0.34257+0.00894\n",
      "[3]\ttrain-error:0.33235+0.00343\ttest-error:0.33525+0.01369\n",
      "[4]\ttrain-error:0.32994+0.00409\ttest-error:0.33596+0.00954\n",
      "[5]\ttrain-error:0.32719+0.00269\ttest-error:0.33222+0.01084\n",
      "[6]\ttrain-error:0.32605+0.00258\ttest-error:0.33191+0.00894\n",
      "[7]\ttrain-error:0.32540+0.00152\ttest-error:0.32973+0.01075\n",
      "[8]\ttrain-error:0.32404+0.00173\ttest-error:0.32972+0.00819\n",
      "[9]\ttrain-error:0.32313+0.00142\ttest-error:0.32902+0.00993\n",
      "[10]\ttrain-error:0.32112+0.00161\ttest-error:0.32887+0.00861\n",
      "[11]\ttrain-error:0.31925+0.00157\ttest-error:0.32832+0.00851\n",
      "[12]\ttrain-error:0.31976+0.00107\ttest-error:0.32677+0.00939\n",
      "[13]\ttrain-error:0.31915+0.00109\ttest-error:0.32568+0.00929\n",
      "[14]\ttrain-error:0.31799+0.00118\ttest-error:0.32474+0.00996\n",
      "[15]\ttrain-error:0.31691+0.00177\ttest-error:0.32451+0.00963\n",
      "[16]\ttrain-error:0.31713+0.00189\ttest-error:0.32497+0.00971\n",
      "[17]\ttrain-error:0.31573+0.00158\ttest-error:0.32474+0.01028\n",
      "[18]\ttrain-error:0.31491+0.00223\ttest-error:0.32552+0.01086\n",
      "[19]\ttrain-error:0.31419+0.00184\ttest-error:0.32443+0.01016\n",
      "[20]\ttrain-error:0.31341+0.00161\ttest-error:0.32459+0.01002\n",
      "[21]\ttrain-error:0.31323+0.00163\ttest-error:0.32396+0.01025\n",
      "[22]\ttrain-error:0.31216+0.00242\ttest-error:0.32381+0.01045\n",
      "[23]\ttrain-error:0.31154+0.00210\ttest-error:0.32342+0.01015\n",
      "[24]\ttrain-error:0.31090+0.00172\ttest-error:0.32124+0.00979\n",
      "[25]\ttrain-error:0.31080+0.00192\ttest-error:0.32124+0.00972\n",
      "[26]\ttrain-error:0.31031+0.00165\ttest-error:0.31983+0.00894\n",
      "[27]\ttrain-error:0.30977+0.00168\ttest-error:0.31983+0.00891\n",
      "[28]\ttrain-error:0.30948+0.00196\ttest-error:0.31991+0.00857\n",
      "[29]\ttrain-error:0.30798+0.00220\ttest-error:0.32030+0.00923\n",
      "[30]\ttrain-error:0.30732+0.00191\ttest-error:0.31983+0.00889\n",
      "[31]\ttrain-error:0.30757+0.00210\ttest-error:0.32038+0.00961\n",
      "[32]\ttrain-error:0.30699+0.00202\ttest-error:0.32054+0.00991\n",
      "[33]\ttrain-error:0.30702+0.00183\ttest-error:0.32061+0.00957\n",
      "[34]\ttrain-error:0.30623+0.00204\ttest-error:0.32108+0.00927\n",
      "[35]\ttrain-error:0.30554+0.00195\ttest-error:0.32077+0.01033\n",
      "[36]\ttrain-error:0.30506+0.00165\ttest-error:0.32100+0.01003\n",
      "[37]\ttrain-error:0.30455+0.00174\ttest-error:0.32046+0.01056\n",
      "[38]\ttrain-error:0.30442+0.00120\ttest-error:0.31913+0.01004\n",
      "[39]\ttrain-error:0.30406+0.00160\ttest-error:0.31929+0.01051\n",
      "[40]\ttrain-error:0.30317+0.00156\ttest-error:0.31906+0.00951\n",
      "[41]\ttrain-error:0.30282+0.00138\ttest-error:0.31820+0.00886\n",
      "[42]\ttrain-error:0.30261+0.00152\ttest-error:0.31898+0.00911\n",
      "[43]\ttrain-error:0.30288+0.00141\ttest-error:0.31906+0.00965\n",
      "[44]\ttrain-error:0.30210+0.00133\ttest-error:0.31781+0.01039\n",
      "[45]\ttrain-error:0.30146+0.00095\ttest-error:0.31726+0.00989\n",
      "[46]\ttrain-error:0.30132+0.00188\ttest-error:0.31750+0.00915\n",
      "[47]\ttrain-error:0.30109+0.00170\ttest-error:0.31719+0.00912\n",
      "[48]\ttrain-error:0.30095+0.00171\ttest-error:0.31726+0.00906\n",
      "[49]\ttrain-error:0.30048+0.00186\ttest-error:0.31726+0.00827\n",
      "[50]\ttrain-error:0.30000+0.00177\ttest-error:0.31727+0.00811\n",
      "[51]\ttrain-error:0.29984+0.00168\ttest-error:0.31711+0.00844\n",
      "[52]\ttrain-error:0.29910+0.00174\ttest-error:0.31695+0.00800\n",
      "[53]\ttrain-error:0.29867+0.00132\ttest-error:0.31641+0.00892\n",
      "[54]\ttrain-error:0.29828+0.00134\ttest-error:0.31695+0.00787\n",
      "[55]\ttrain-error:0.29799+0.00146\ttest-error:0.31625+0.00850\n",
      "[56]\ttrain-error:0.29762+0.00145\ttest-error:0.31579+0.00791\n",
      "[57]\ttrain-error:0.29698+0.00165\ttest-error:0.31617+0.00688\n",
      "[58]\ttrain-error:0.29678+0.00163\ttest-error:0.31617+0.00721\n",
      "[59]\ttrain-error:0.29614+0.00134\ttest-error:0.31602+0.00696\n",
      "[60]\ttrain-error:0.29618+0.00169\ttest-error:0.31617+0.00817\n",
      "[61]\ttrain-error:0.29624+0.00175\ttest-error:0.31579+0.00730\n",
      "[62]\ttrain-error:0.29593+0.00181\ttest-error:0.31563+0.00735\n",
      "[63]\ttrain-error:0.29548+0.00136\ttest-error:0.31578+0.00748\n",
      "[64]\ttrain-error:0.29585+0.00148\ttest-error:0.31586+0.00769\n",
      "[65]\ttrain-error:0.29573+0.00154\ttest-error:0.31493+0.00819\n",
      "[66]\ttrain-error:0.29532+0.00145\ttest-error:0.31501+0.00730\n",
      "[67]\ttrain-error:0.29515+0.00158\ttest-error:0.31547+0.00711\n",
      "[68]\ttrain-error:0.29484+0.00134\ttest-error:0.31532+0.00696\n",
      "[69]\ttrain-error:0.29453+0.00162\ttest-error:0.31571+0.00713\n",
      "[70]\ttrain-error:0.29394+0.00158\ttest-error:0.31610+0.00742\n",
      "[71]\ttrain-error:0.29371+0.00208\ttest-error:0.31649+0.00702\n",
      "[72]\ttrain-error:0.29344+0.00206\ttest-error:0.31586+0.00736\n",
      "[73]\ttrain-error:0.29324+0.00204\ttest-error:0.31524+0.00813\n",
      "[74]\ttrain-error:0.29283+0.00214\ttest-error:0.31680+0.00736\n",
      "[75]\ttrain-error:0.29347+0.00192\ttest-error:0.31664+0.00691\n",
      "[76]\ttrain-error:0.29303+0.00175\ttest-error:0.31641+0.00710\n",
      "[77]\ttrain-error:0.29264+0.00181\ttest-error:0.31617+0.00782\n",
      "[78]\ttrain-error:0.29213+0.00206\ttest-error:0.31610+0.00802\n",
      "[79]\ttrain-error:0.29178+0.00149\ttest-error:0.31586+0.00807\n",
      "[80]\ttrain-error:0.29151+0.00184\ttest-error:0.31579+0.00725\n",
      "[81]\ttrain-error:0.29125+0.00196\ttest-error:0.31641+0.00772\n",
      "[82]\ttrain-error:0.29106+0.00199\ttest-error:0.31563+0.00880\n",
      "[83]\ttrain-error:0.29069+0.00187\ttest-error:0.31571+0.00915\n",
      "[84]\ttrain-error:0.29059+0.00171\ttest-error:0.31555+0.00925\n",
      "[85]\ttrain-error:0.28970+0.00219\ttest-error:0.31415+0.00970\n",
      "[86]\ttrain-error:0.28937+0.00243\ttest-error:0.31485+0.00973\n",
      "[87]\ttrain-error:0.28944+0.00212\ttest-error:0.31470+0.00987\n",
      "[88]\ttrain-error:0.28874+0.00197\ttest-error:0.31493+0.00874\n",
      "[89]\ttrain-error:0.28814+0.00220\ttest-error:0.31555+0.00920\n",
      "[90]\ttrain-error:0.28744+0.00235\ttest-error:0.31516+0.00927\n",
      "[91]\ttrain-error:0.28726+0.00214\ttest-error:0.31508+0.00984\n",
      "[92]\ttrain-error:0.28705+0.00193\ttest-error:0.31555+0.01015\n",
      "[93]\ttrain-error:0.28697+0.00183\ttest-error:0.31524+0.00952\n",
      "[94]\ttrain-error:0.28629+0.00170\ttest-error:0.31454+0.00810\n",
      "[95]\ttrain-error:0.28633+0.00182\ttest-error:0.31485+0.00834\n",
      "[96]\ttrain-error:0.28596+0.00174\ttest-error:0.31532+0.00861\n",
      "[97]\ttrain-error:0.28567+0.00200\ttest-error:0.31532+0.00849\n",
      "[98]\ttrain-error:0.28543+0.00225\ttest-error:0.31485+0.00823\n",
      "[99]\ttrain-error:0.28539+0.00225\ttest-error:0.31485+0.00805\n",
      "[100]\ttrain-error:0.28504+0.00192\ttest-error:0.31532+0.00836\n",
      "[101]\ttrain-error:0.28475+0.00222\ttest-error:0.31532+0.00893\n",
      "[102]\ttrain-error:0.28450+0.00218\ttest-error:0.31501+0.00943\n",
      "[103]\ttrain-error:0.28446+0.00190\ttest-error:0.31399+0.00891\n",
      "[104]\ttrain-error:0.28411+0.00189\ttest-error:0.31368+0.00873\n",
      "[105]\ttrain-error:0.28399+0.00152\ttest-error:0.31353+0.00863\n",
      "[106]\ttrain-error:0.28345+0.00186\ttest-error:0.31392+0.00773\n",
      "[107]\ttrain-error:0.28376+0.00201\ttest-error:0.31368+0.00823\n",
      "[108]\ttrain-error:0.28306+0.00157\ttest-error:0.31298+0.00876\n",
      "[109]\ttrain-error:0.28302+0.00145\ttest-error:0.31376+0.00798\n",
      "[110]\ttrain-error:0.28281+0.00157\ttest-error:0.31431+0.00858\n",
      "[111]\ttrain-error:0.28267+0.00158\ttest-error:0.31322+0.00862\n",
      "[112]\ttrain-error:0.28208+0.00197\ttest-error:0.31345+0.00840\n",
      "[113]\ttrain-error:0.28207+0.00185\ttest-error:0.31283+0.00832\n",
      "[114]\ttrain-error:0.28171+0.00227\ttest-error:0.31345+0.00939\n",
      "[115]\ttrain-error:0.28134+0.00200\ttest-error:0.31329+0.00895\n",
      "[116]\ttrain-error:0.28113+0.00239\ttest-error:0.31399+0.00883\n",
      "[117]\ttrain-error:0.28088+0.00213\ttest-error:0.31384+0.00892\n",
      "[118]\ttrain-error:0.28045+0.00212\ttest-error:0.31446+0.00957\n",
      "[119]\ttrain-error:0.28008+0.00218\ttest-error:0.31407+0.00876\n",
      "[120]\ttrain-error:0.27988+0.00242\ttest-error:0.31298+0.00883\n",
      "[121]\ttrain-error:0.27975+0.00225\ttest-error:0.31415+0.00930\n",
      "[122]\ttrain-error:0.27932+0.00228\ttest-error:0.31392+0.00911\n",
      "[123]\ttrain-error:0.27942+0.00220\ttest-error:0.31329+0.00857\n",
      "[124]\ttrain-error:0.27930+0.00230\ttest-error:0.31384+0.00920\n",
      "[125]\ttrain-error:0.27878+0.00240\ttest-error:0.31360+0.00946\n",
      "[126]\ttrain-error:0.27842+0.00279\ttest-error:0.31267+0.00982\n",
      "[127]\ttrain-error:0.27819+0.00261\ttest-error:0.31283+0.00946\n",
      "[128]\ttrain-error:0.27825+0.00253\ttest-error:0.31306+0.00936\n",
      "[129]\ttrain-error:0.27788+0.00239\ttest-error:0.31298+0.00992\n",
      "[130]\ttrain-error:0.27737+0.00203\ttest-error:0.31353+0.00923\n",
      "[131]\ttrain-error:0.27685+0.00161\ttest-error:0.31322+0.00916\n",
      "[132]\ttrain-error:0.27640+0.00180\ttest-error:0.31306+0.00863\n",
      "[133]\ttrain-error:0.27626+0.00201\ttest-error:0.31267+0.00852\n",
      "[134]\ttrain-error:0.27597+0.00168\ttest-error:0.31275+0.00872\n",
      "[135]\ttrain-error:0.27578+0.00137\ttest-error:0.31298+0.00843\n",
      "[136]\ttrain-error:0.27541+0.00156\ttest-error:0.31251+0.00881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137]\ttrain-error:0.27531+0.00165\ttest-error:0.31259+0.00865\n",
      "[138]\ttrain-error:0.27478+0.00165\ttest-error:0.31197+0.00825\n",
      "[139]\ttrain-error:0.27441+0.00149\ttest-error:0.31205+0.00770\n",
      "[140]\ttrain-error:0.27408+0.00142\ttest-error:0.31205+0.00767\n",
      "[141]\ttrain-error:0.27401+0.00165\ttest-error:0.31181+0.00754\n",
      "[142]\ttrain-error:0.27387+0.00152\ttest-error:0.31189+0.00758\n",
      "[143]\ttrain-error:0.27358+0.00162\ttest-error:0.31189+0.00808\n",
      "[144]\ttrain-error:0.27327+0.00175\ttest-error:0.31150+0.00807\n",
      "[145]\ttrain-error:0.27291+0.00142\ttest-error:0.31135+0.00885\n",
      "[146]\ttrain-error:0.27291+0.00165\ttest-error:0.31127+0.00882\n",
      "[147]\ttrain-error:0.27268+0.00178\ttest-error:0.31174+0.00943\n",
      "[148]\ttrain-error:0.27233+0.00166\ttest-error:0.31220+0.00978\n",
      "[149]\ttrain-error:0.27216+0.00200\ttest-error:0.31283+0.00940\n",
      "[150]\ttrain-error:0.27179+0.00182\ttest-error:0.31259+0.00978\n",
      "[151]\ttrain-error:0.27167+0.00171\ttest-error:0.31275+0.00963\n",
      "[152]\ttrain-error:0.27173+0.00180\ttest-error:0.31298+0.00893\n",
      "[153]\ttrain-error:0.27114+0.00187\ttest-error:0.31314+0.00891\n",
      "[154]\ttrain-error:0.27048+0.00207\ttest-error:0.31267+0.00842\n",
      "[155]\ttrain-error:0.27046+0.00159\ttest-error:0.31197+0.00902\n",
      "[156]\ttrain-error:0.26990+0.00158\ttest-error:0.31158+0.00859\n",
      "[157]\ttrain-error:0.26961+0.00178\ttest-error:0.31244+0.00792\n",
      "[158]\ttrain-error:0.26945+0.00185\ttest-error:0.31236+0.00816\n",
      "[159]\ttrain-error:0.26949+0.00204\ttest-error:0.31181+0.00842\n",
      "[160]\ttrain-error:0.26910+0.00197\ttest-error:0.31220+0.00846\n",
      "[161]\ttrain-error:0.26888+0.00158\ttest-error:0.31189+0.00857\n",
      "[162]\ttrain-error:0.26881+0.00147\ttest-error:0.31135+0.00918\n",
      "[163]\ttrain-error:0.26844+0.00127\ttest-error:0.31135+0.00866\n",
      "[164]\ttrain-error:0.26838+0.00133\ttest-error:0.31181+0.00830\n",
      "[165]\ttrain-error:0.26842+0.00128\ttest-error:0.31220+0.00907\n",
      "[166]\ttrain-error:0.26770+0.00161\ttest-error:0.31197+0.00865\n",
      "[167]\ttrain-error:0.26735+0.00150\ttest-error:0.31236+0.00790\n",
      "[168]\ttrain-error:0.26754+0.00126\ttest-error:0.31228+0.00872\n",
      "[169]\ttrain-error:0.26705+0.00088\ttest-error:0.31174+0.00812\n",
      "[170]\ttrain-error:0.26672+0.00104\ttest-error:0.31150+0.00903\n",
      "[171]\ttrain-error:0.26678+0.00111\ttest-error:0.31127+0.00935\n",
      "[172]\ttrain-error:0.26653+0.00101\ttest-error:0.31181+0.00923\n",
      "[173]\ttrain-error:0.26669+0.00095\ttest-error:0.31135+0.00918\n",
      "[174]\ttrain-error:0.26602+0.00083\ttest-error:0.31119+0.00898\n",
      "[175]\ttrain-error:0.26626+0.00054\ttest-error:0.31096+0.00871\n",
      "[176]\ttrain-error:0.26594+0.00089\ttest-error:0.31072+0.00843\n",
      "[177]\ttrain-error:0.26548+0.00081\ttest-error:0.31111+0.00798\n",
      "[178]\ttrain-error:0.26497+0.00090\ttest-error:0.31142+0.00786\n",
      "[179]\ttrain-error:0.26478+0.00073\ttest-error:0.31166+0.00773\n",
      "[180]\ttrain-error:0.26460+0.00078\ttest-error:0.31244+0.00800\n",
      "[181]\ttrain-error:0.26417+0.00104\ttest-error:0.31142+0.00883\n",
      "[182]\ttrain-error:0.26398+0.00072\ttest-error:0.31181+0.00817\n",
      "[183]\ttrain-error:0.26359+0.00101\ttest-error:0.31197+0.00805\n",
      "[184]\ttrain-error:0.26326+0.00102\ttest-error:0.31251+0.00789\n",
      "[185]\ttrain-error:0.26310+0.00079\ttest-error:0.31197+0.00792\n",
      "[186]\ttrain-error:0.26301+0.00110\ttest-error:0.31251+0.00757\n",
      "[187]\ttrain-error:0.26328+0.00113\ttest-error:0.31213+0.00736\n",
      "[188]\ttrain-error:0.26347+0.00145\ttest-error:0.31111+0.00702\n",
      "[189]\ttrain-error:0.26341+0.00131\ttest-error:0.31197+0.00758\n",
      "[190]\ttrain-error:0.26357+0.00145\ttest-error:0.31275+0.00741\n",
      "[191]\ttrain-error:0.26367+0.00128\ttest-error:0.31290+0.00794\n",
      "[192]\ttrain-error:0.26339+0.00152\ttest-error:0.31259+0.00855\n",
      "[193]\ttrain-error:0.26312+0.00161\ttest-error:0.31251+0.00847\n",
      "[194]\ttrain-error:0.26293+0.00164\ttest-error:0.31275+0.00893\n",
      "[195]\ttrain-error:0.26242+0.00138\ttest-error:0.31251+0.00882\n",
      "[196]\ttrain-error:0.26260+0.00167\ttest-error:0.31228+0.00914\n",
      "[197]\ttrain-error:0.26240+0.00158\ttest-error:0.31213+0.00892\n",
      "[198]\ttrain-error:0.26193+0.00154\ttest-error:0.31205+0.00878\n",
      "[199]\ttrain-error:0.26203+0.00191\ttest-error:0.31150+0.00945\n",
      "[200]\ttrain-error:0.26168+0.00185\ttest-error:0.31205+0.00876\n",
      "[201]\ttrain-error:0.26119+0.00209\ttest-error:0.31236+0.00866\n",
      "[202]\ttrain-error:0.26131+0.00195\ttest-error:0.31306+0.00868\n",
      "[203]\ttrain-error:0.26118+0.00206\ttest-error:0.31181+0.00840\n",
      "[204]\ttrain-error:0.26094+0.00180\ttest-error:0.31142+0.00824\n",
      "[205]\ttrain-error:0.26073+0.00166\ttest-error:0.31158+0.00846\n",
      "[206]\ttrain-error:0.26012+0.00155\ttest-error:0.31166+0.00807\n",
      "[207]\ttrain-error:0.25999+0.00175\ttest-error:0.31150+0.00946\n",
      "[208]\ttrain-error:0.25979+0.00138\ttest-error:0.31104+0.00888\n",
      "[209]\ttrain-error:0.25964+0.00146\ttest-error:0.31119+0.00864\n",
      "[210]\ttrain-error:0.25952+0.00175\ttest-error:0.31088+0.00966\n",
      "[211]\ttrain-error:0.25878+0.00151\ttest-error:0.31096+0.00819\n",
      "[212]\ttrain-error:0.25882+0.00148\ttest-error:0.31127+0.00871\n",
      "[213]\ttrain-error:0.25859+0.00138\ttest-error:0.31104+0.00881\n",
      "[214]\ttrain-error:0.25851+0.00147\ttest-error:0.31049+0.00761\n",
      "[215]\ttrain-error:0.25790+0.00132\ttest-error:0.31080+0.00796\n",
      "[216]\ttrain-error:0.25746+0.00126\ttest-error:0.31080+0.00825\n",
      "[217]\ttrain-error:0.25744+0.00080\ttest-error:0.31104+0.00842\n",
      "[218]\ttrain-error:0.25738+0.00105\ttest-error:0.31080+0.00784\n",
      "[219]\ttrain-error:0.25746+0.00093\ttest-error:0.31072+0.00816\n",
      "[220]\ttrain-error:0.25685+0.00113\ttest-error:0.31142+0.00836\n",
      "[221]\ttrain-error:0.25662+0.00142\ttest-error:0.30987+0.00967\n",
      "[222]\ttrain-error:0.25654+0.00121\ttest-error:0.31018+0.00885\n",
      "[223]\ttrain-error:0.25652+0.00142\ttest-error:0.30932+0.00932\n",
      "[224]\ttrain-error:0.25625+0.00176\ttest-error:0.31026+0.00952\n",
      "[225]\ttrain-error:0.25633+0.00175\ttest-error:0.31002+0.00904\n",
      "[226]\ttrain-error:0.25582+0.00172\ttest-error:0.31033+0.00885\n",
      "[227]\ttrain-error:0.25570+0.00165\ttest-error:0.31026+0.00904\n",
      "[228]\ttrain-error:0.25559+0.00174\ttest-error:0.31026+0.00806\n",
      "[229]\ttrain-error:0.25528+0.00185\ttest-error:0.31072+0.00794\n",
      "[230]\ttrain-error:0.25545+0.00158\ttest-error:0.31041+0.00778\n",
      "[231]\ttrain-error:0.25518+0.00204\ttest-error:0.31057+0.00847\n",
      "[232]\ttrain-error:0.25504+0.00226\ttest-error:0.31065+0.00832\n",
      "[233]\ttrain-error:0.25493+0.00236\ttest-error:0.31049+0.00822\n",
      "[234]\ttrain-error:0.25469+0.00196\ttest-error:0.31041+0.00761\n",
      "[235]\ttrain-error:0.25452+0.00230\ttest-error:0.31041+0.00794\n",
      "[236]\ttrain-error:0.25421+0.00210\ttest-error:0.31072+0.00784\n",
      "[237]\ttrain-error:0.25391+0.00216\ttest-error:0.31010+0.00788\n",
      "[238]\ttrain-error:0.25387+0.00214\ttest-error:0.30987+0.00780\n",
      "[239]\ttrain-error:0.25372+0.00207\ttest-error:0.30948+0.00754\n",
      "[240]\ttrain-error:0.25387+0.00224\ttest-error:0.30909+0.00778\n",
      "[241]\ttrain-error:0.25395+0.00221\ttest-error:0.30956+0.00813\n",
      "[242]\ttrain-error:0.25323+0.00242\ttest-error:0.30979+0.00860\n",
      "[243]\ttrain-error:0.25298+0.00228\ttest-error:0.30995+0.00891\n",
      "[244]\ttrain-error:0.25298+0.00206\ttest-error:0.30979+0.00854\n",
      "[245]\ttrain-error:0.25218+0.00212\ttest-error:0.31018+0.00899\n",
      "[246]\ttrain-error:0.25210+0.00224\ttest-error:0.31026+0.00884\n",
      "[247]\ttrain-error:0.25189+0.00203\ttest-error:0.30987+0.00878\n",
      "[248]\ttrain-error:0.25154+0.00236\ttest-error:0.31111+0.00931\n",
      "[249]\ttrain-error:0.25136+0.00173\ttest-error:0.31065+0.00864\n",
      "[250]\ttrain-error:0.25132+0.00181\ttest-error:0.31127+0.00891\n",
      "[251]\ttrain-error:0.25103+0.00197\ttest-error:0.31119+0.00906\n",
      "[252]\ttrain-error:0.25086+0.00180\ttest-error:0.31104+0.00918\n",
      "[253]\ttrain-error:0.25076+0.00184\ttest-error:0.31127+0.00865\n",
      "[254]\ttrain-error:0.25045+0.00227\ttest-error:0.31065+0.00945\n",
      "[255]\ttrain-error:0.25012+0.00249\ttest-error:0.31065+0.00941\n",
      "[256]\ttrain-error:0.25018+0.00209\ttest-error:0.31119+0.00917\n",
      "[257]\ttrain-error:0.24988+0.00238\ttest-error:0.31104+0.00857\n",
      "[258]\ttrain-error:0.24977+0.00212\ttest-error:0.31049+0.00895\n",
      "[259]\ttrain-error:0.24961+0.00220\ttest-error:0.31096+0.00870\n",
      "[260]\ttrain-error:0.24967+0.00186\ttest-error:0.31111+0.00912\n",
      "[261]\ttrain-error:0.24951+0.00190\ttest-error:0.31135+0.00838\n",
      "[262]\ttrain-error:0.24940+0.00184\ttest-error:0.31236+0.00857\n",
      "[263]\ttrain-error:0.24897+0.00181\ttest-error:0.31236+0.00888\n",
      "[264]\ttrain-error:0.24866+0.00164\ttest-error:0.31189+0.00839\n",
      "[265]\ttrain-error:0.24825+0.00193\ttest-error:0.31166+0.00829\n",
      "[266]\ttrain-error:0.24807+0.00208\ttest-error:0.31135+0.00834\n",
      "[267]\ttrain-error:0.24790+0.00202\ttest-error:0.31197+0.00811\n",
      "[268]\ttrain-error:0.24755+0.00194\ttest-error:0.31135+0.00892\n",
      "[269]\ttrain-error:0.24741+0.00208\ttest-error:0.31150+0.00876\n",
      "[270]\ttrain-error:0.24718+0.00204\ttest-error:0.31174+0.00844\n",
      "[271]\ttrain-error:0.24702+0.00225\ttest-error:0.31135+0.00899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272]\ttrain-error:0.24722+0.00240\ttest-error:0.31150+0.00916\n",
      "[273]\ttrain-error:0.24679+0.00230\ttest-error:0.31080+0.00949\n",
      "[274]\ttrain-error:0.24653+0.00195\ttest-error:0.31127+0.00896\n",
      "[275]\ttrain-error:0.24628+0.00188\ttest-error:0.31111+0.00966\n",
      "[276]\ttrain-error:0.24609+0.00211\ttest-error:0.31158+0.00910\n",
      "[277]\ttrain-error:0.24626+0.00195\ttest-error:0.31189+0.00904\n",
      "[278]\ttrain-error:0.24566+0.00206\ttest-error:0.31142+0.00968\n",
      "[279]\ttrain-error:0.24554+0.00184\ttest-error:0.31213+0.00881\n",
      "[280]\ttrain-error:0.24556+0.00184\ttest-error:0.31158+0.00903\n",
      "[281]\ttrain-error:0.24535+0.00170\ttest-error:0.31142+0.00923\n",
      "[282]\ttrain-error:0.24517+0.00175\ttest-error:0.31072+0.00961\n",
      "[283]\ttrain-error:0.24504+0.00189\ttest-error:0.31002+0.00932\n",
      "[284]\ttrain-error:0.24445+0.00204\ttest-error:0.31018+0.00976\n",
      "[285]\ttrain-error:0.24435+0.00209\ttest-error:0.31033+0.00956\n",
      "[286]\ttrain-error:0.24451+0.00220\ttest-error:0.31096+0.00948\n",
      "[287]\ttrain-error:0.24400+0.00220\ttest-error:0.31096+0.01008\n",
      "[288]\ttrain-error:0.24418+0.00231\ttest-error:0.31119+0.00996\n",
      "[289]\ttrain-error:0.24346+0.00238\ttest-error:0.31143+0.00949\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.7385\n",
      "AUC Score (Train): 0.807366\n"
     ]
    }
   ],
   "source": [
    "# Determining NEW Optimal Number of Estimators\n",
    "xgb2 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=2, min_child_weight=4,\n",
    "                     gamma=0.0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n",
    "                     nthread=4, scale_pos_weight=1,seed=27)\n",
    "modelfit(xgb2, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  2.1min remaining: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  2.2min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  2.8min remaining:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:09:36] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'subsample': 0.7}, 0.6544674461498164)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validating SUBSAMPLE and COLSAMPLE_BYTREE\n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(7,9)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(7,9)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=289, max_depth=2, \n",
    "                                                  min_child_weight=4, gamma=0.0, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test4, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch4.fit(train[predictors],train[target])\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  25 | elapsed:  2.0min remaining: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  25 | elapsed:  2.0min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  25 | elapsed:  3.0min remaining:   34.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:13:47] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 100}, 0.6755701819804548)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validating REG_ALPHA\n",
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=289, max_depth=2, \n",
    "                                                  min_child_weight=4, gamma=0.0, subsample=0.7, \n",
    "                                                  colsample_bytree=0.7, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test6, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch6.fit(train[predictors],train[target])\n",
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:  1.3min remaining:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  1.4min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:16:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 100}, 0.6755701819804548)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validating REG_ALPHA\n",
    "param_test6 = {\n",
    " 'reg_alpha':[50, 100, 150]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=289, max_depth=2, \n",
    "                                                  min_child_weight=4, gamma=0.0, subsample=0.7, \n",
    "                                                  colsample_bytree=0.7, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test6, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch6.fit(train[predictors],train[target])\n",
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the optimal number of trees for our final set of hyperparameters, but decrease the learning rate to 0.01 to get our final hyperparameters and run this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.35611+0.00759\ttest-error:0.36321+0.01368\n",
      "[1]\ttrain-error:0.34729+0.00540\ttest-error:0.35208+0.01187\n",
      "[2]\ttrain-error:0.34178+0.00800\ttest-error:0.34725+0.01427\n",
      "[3]\ttrain-error:0.33975+0.00624\ttest-error:0.34655+0.01388\n",
      "[4]\ttrain-error:0.33732+0.00264\ttest-error:0.34530+0.01364\n",
      "[5]\ttrain-error:0.33599+0.00301\ttest-error:0.34522+0.01395\n",
      "[6]\ttrain-error:0.33529+0.00270\ttest-error:0.34491+0.01446\n",
      "[7]\ttrain-error:0.33337+0.00169\ttest-error:0.34172+0.01755\n",
      "[8]\ttrain-error:0.33292+0.00135\ttest-error:0.34141+0.01555\n",
      "[9]\ttrain-error:0.33202+0.00135\ttest-error:0.34156+0.01610\n",
      "[10]\ttrain-error:0.33220+0.00138\ttest-error:0.34180+0.01615\n",
      "[11]\ttrain-error:0.33226+0.00094\ttest-error:0.34125+0.01588\n",
      "[12]\ttrain-error:0.33165+0.00111\ttest-error:0.34071+0.01417\n",
      "[13]\ttrain-error:0.33173+0.00129\ttest-error:0.33907+0.01395\n",
      "[14]\ttrain-error:0.33144+0.00177\ttest-error:0.34008+0.01154\n",
      "[15]\ttrain-error:0.33167+0.00140\ttest-error:0.33876+0.01034\n",
      "[16]\ttrain-error:0.33173+0.00155\ttest-error:0.33876+0.01052\n",
      "[17]\ttrain-error:0.33146+0.00134\ttest-error:0.33969+0.01251\n",
      "[18]\ttrain-error:0.33087+0.00144\ttest-error:0.33899+0.01015\n",
      "[19]\ttrain-error:0.33052+0.00127\ttest-error:0.33814+0.01160\n",
      "[20]\ttrain-error:0.33117+0.00128\ttest-error:0.33821+0.01234\n",
      "[21]\ttrain-error:0.33099+0.00137\ttest-error:0.33814+0.01280\n",
      "[22]\ttrain-error:0.33109+0.00143\ttest-error:0.33814+0.01308\n",
      "[23]\ttrain-error:0.33031+0.00184\ttest-error:0.33845+0.01280\n",
      "[24]\ttrain-error:0.33023+0.00209\ttest-error:0.33837+0.01220\n",
      "[25]\ttrain-error:0.33037+0.00208\ttest-error:0.33821+0.01219\n",
      "[26]\ttrain-error:0.33011+0.00149\ttest-error:0.33736+0.01166\n",
      "[27]\ttrain-error:0.33033+0.00157\ttest-error:0.33697+0.01211\n",
      "[28]\ttrain-error:0.32982+0.00105\ttest-error:0.33712+0.01094\n",
      "[29]\ttrain-error:0.33009+0.00143\ttest-error:0.33650+0.01017\n",
      "[30]\ttrain-error:0.32982+0.00148\ttest-error:0.33619+0.01112\n",
      "[31]\ttrain-error:0.32965+0.00150\ttest-error:0.33549+0.01128\n",
      "[32]\ttrain-error:0.32959+0.00151\ttest-error:0.33471+0.01099\n",
      "[33]\ttrain-error:0.32947+0.00164\ttest-error:0.33370+0.00991\n",
      "[34]\ttrain-error:0.32930+0.00159\ttest-error:0.33401+0.01044\n",
      "[35]\ttrain-error:0.32928+0.00154\ttest-error:0.33432+0.01012\n",
      "[36]\ttrain-error:0.32945+0.00204\ttest-error:0.33448+0.01057\n",
      "[37]\ttrain-error:0.32924+0.00257\ttest-error:0.33440+0.01046\n",
      "[38]\ttrain-error:0.32943+0.00194\ttest-error:0.33448+0.00989\n",
      "[39]\ttrain-error:0.32978+0.00170\ttest-error:0.33487+0.00995\n",
      "[40]\ttrain-error:0.33015+0.00151\ttest-error:0.33471+0.01014\n",
      "[41]\ttrain-error:0.32976+0.00130\ttest-error:0.33526+0.01004\n",
      "[42]\ttrain-error:0.32971+0.00139\ttest-error:0.33619+0.01086\n",
      "[43]\ttrain-error:0.33013+0.00155\ttest-error:0.33596+0.01122\n",
      "[44]\ttrain-error:0.33002+0.00174\ttest-error:0.33557+0.01054\n",
      "[45]\ttrain-error:0.33002+0.00168\ttest-error:0.33533+0.01096\n",
      "[46]\ttrain-error:0.32994+0.00170\ttest-error:0.33541+0.01106\n",
      "[47]\ttrain-error:0.32976+0.00176\ttest-error:0.33564+0.01092\n",
      "[48]\ttrain-error:0.32984+0.00169\ttest-error:0.33487+0.01045\n",
      "[49]\ttrain-error:0.32996+0.00161\ttest-error:0.33502+0.01076\n",
      "[50]\ttrain-error:0.32984+0.00211\ttest-error:0.33494+0.01005\n",
      "[51]\ttrain-error:0.32976+0.00212\ttest-error:0.33471+0.01040\n",
      "[52]\ttrain-error:0.32978+0.00221\ttest-error:0.33549+0.01014\n",
      "[53]\ttrain-error:0.32980+0.00226\ttest-error:0.33533+0.00994\n",
      "[54]\ttrain-error:0.32984+0.00189\ttest-error:0.33549+0.01018\n",
      "[55]\ttrain-error:0.33046+0.00201\ttest-error:0.33479+0.00966\n",
      "[56]\ttrain-error:0.32998+0.00152\ttest-error:0.33510+0.00965\n",
      "[57]\ttrain-error:0.33000+0.00180\ttest-error:0.33525+0.00896\n",
      "[58]\ttrain-error:0.33008+0.00176\ttest-error:0.33510+0.00955\n",
      "[59]\ttrain-error:0.32976+0.00185\ttest-error:0.33510+0.00974\n",
      "[60]\ttrain-error:0.32965+0.00181\ttest-error:0.33510+0.00922\n",
      "[61]\ttrain-error:0.32965+0.00181\ttest-error:0.33572+0.00931\n",
      "[62]\ttrain-error:0.32984+0.00200\ttest-error:0.33541+0.01006\n",
      "[63]\ttrain-error:0.32980+0.00180\ttest-error:0.33487+0.00979\n",
      "[64]\ttrain-error:0.32982+0.00163\ttest-error:0.33432+0.00983\n",
      "[65]\ttrain-error:0.32945+0.00180\ttest-error:0.33385+0.00974\n",
      "[66]\ttrain-error:0.32918+0.00202\ttest-error:0.33463+0.00955\n",
      "[67]\ttrain-error:0.32918+0.00183\ttest-error:0.33448+0.00953\n",
      "[68]\ttrain-error:0.32941+0.00178\ttest-error:0.33416+0.00939\n",
      "[69]\ttrain-error:0.32951+0.00189\ttest-error:0.33385+0.00944\n",
      "[70]\ttrain-error:0.32974+0.00212\ttest-error:0.33370+0.00930\n",
      "[71]\ttrain-error:0.32947+0.00222\ttest-error:0.33292+0.00992\n",
      "[72]\ttrain-error:0.32961+0.00210\ttest-error:0.33315+0.01053\n",
      "[73]\ttrain-error:0.32980+0.00209\ttest-error:0.33339+0.01046\n",
      "[74]\ttrain-error:0.32965+0.00190\ttest-error:0.33354+0.01136\n",
      "[75]\ttrain-error:0.32943+0.00212\ttest-error:0.33370+0.01155\n",
      "[76]\ttrain-error:0.32918+0.00203\ttest-error:0.33409+0.01095\n",
      "[77]\ttrain-error:0.32953+0.00207\ttest-error:0.33409+0.01081\n",
      "[78]\ttrain-error:0.32959+0.00229\ttest-error:0.33370+0.01037\n",
      "[79]\ttrain-error:0.32916+0.00224\ttest-error:0.33354+0.01052\n",
      "[80]\ttrain-error:0.32920+0.00198\ttest-error:0.33378+0.01026\n",
      "[81]\ttrain-error:0.32912+0.00192\ttest-error:0.33362+0.01026\n",
      "[82]\ttrain-error:0.32912+0.00205\ttest-error:0.33354+0.00999\n",
      "[83]\ttrain-error:0.32900+0.00195\ttest-error:0.33362+0.00958\n",
      "[84]\ttrain-error:0.32912+0.00178\ttest-error:0.33300+0.00935\n",
      "[85]\ttrain-error:0.32893+0.00180\ttest-error:0.33300+0.00925\n",
      "[86]\ttrain-error:0.32906+0.00216\ttest-error:0.33323+0.00962\n",
      "[87]\ttrain-error:0.32893+0.00209\ttest-error:0.33370+0.00919\n",
      "[88]\ttrain-error:0.32885+0.00210\ttest-error:0.33385+0.00897\n",
      "[89]\ttrain-error:0.32877+0.00231\ttest-error:0.33401+0.00919\n",
      "[90]\ttrain-error:0.32848+0.00225\ttest-error:0.33448+0.00905\n",
      "[91]\ttrain-error:0.32844+0.00203\ttest-error:0.33494+0.00851\n",
      "[92]\ttrain-error:0.32838+0.00220\ttest-error:0.33518+0.00839\n",
      "[93]\ttrain-error:0.32844+0.00218\ttest-error:0.33518+0.00778\n",
      "[94]\ttrain-error:0.32817+0.00202\ttest-error:0.33455+0.00818\n",
      "[95]\ttrain-error:0.32834+0.00208\ttest-error:0.33471+0.00833\n",
      "[96]\ttrain-error:0.32848+0.00199\ttest-error:0.33448+0.00837\n",
      "[97]\ttrain-error:0.32850+0.00207\ttest-error:0.33448+0.00890\n",
      "[98]\ttrain-error:0.32854+0.00226\ttest-error:0.33424+0.00931\n",
      "[99]\ttrain-error:0.32873+0.00242\ttest-error:0.33416+0.00924\n",
      "[100]\ttrain-error:0.32873+0.00249\ttest-error:0.33432+0.00907\n",
      "[101]\ttrain-error:0.32860+0.00233\ttest-error:0.33440+0.00872\n",
      "[102]\ttrain-error:0.32840+0.00242\ttest-error:0.33401+0.00859\n",
      "[103]\ttrain-error:0.32823+0.00242\ttest-error:0.33370+0.00886\n",
      "[104]\ttrain-error:0.32840+0.00232\ttest-error:0.33385+0.00891\n",
      "[105]\ttrain-error:0.32836+0.00228\ttest-error:0.33354+0.00897\n",
      "[106]\ttrain-error:0.32834+0.00214\ttest-error:0.33315+0.00909\n",
      "[107]\ttrain-error:0.32805+0.00201\ttest-error:0.33354+0.00889\n",
      "[108]\ttrain-error:0.32772+0.00218\ttest-error:0.33393+0.00891\n",
      "[109]\ttrain-error:0.32793+0.00237\ttest-error:0.33378+0.00920\n",
      "[110]\ttrain-error:0.32784+0.00217\ttest-error:0.33385+0.00916\n",
      "[111]\ttrain-error:0.32782+0.00184\ttest-error:0.33409+0.00911\n",
      "[112]\ttrain-error:0.32788+0.00203\ttest-error:0.33416+0.00908\n",
      "[113]\ttrain-error:0.32786+0.00200\ttest-error:0.33393+0.00924\n",
      "[114]\ttrain-error:0.32803+0.00207\ttest-error:0.33416+0.00932\n",
      "[115]\ttrain-error:0.32791+0.00207\ttest-error:0.33409+0.00935\n",
      "[116]\ttrain-error:0.32776+0.00184\ttest-error:0.33385+0.00891\n",
      "[117]\ttrain-error:0.32762+0.00169\ttest-error:0.33377+0.00928\n",
      "[118]\ttrain-error:0.32772+0.00175\ttest-error:0.33424+0.00924\n",
      "[119]\ttrain-error:0.32753+0.00186\ttest-error:0.33393+0.00939\n",
      "[120]\ttrain-error:0.32774+0.00177\ttest-error:0.33362+0.00946\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.6712\n",
      "AUC Score (Train): 0.708581\n"
     ]
    }
   ],
   "source": [
    "# Determining Optimal Number of Trees w/ New (LOWER) Learning Rate \n",
    "\n",
    "xgb3 = XGBClassifier(learning_rate = 0.005, n_estimators = 5000, max_depth = 2, min_child_weight = 4, gamma = 0.0, \n",
    "                     subsample = 0.7, colsample_bytree = 0.7, reg_alpha = 100, objective = 'binary:logistic', \n",
    "                     nthread=4, scale_pos_weight = 1, seed = 69)\n",
    "modelfit(xgb3, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 497  582]\n",
      " [ 316 1229]]\n",
      "\n",
      "Accuracy :  0.6578\n"
     ]
    }
   ],
   "source": [
    "# Running our Final XgBoost Model\n",
    "xgb5 = XGBClassifier(learning_rate = 0.005, n_estimators = 120, max_depth = 2, min_child_weight = 4, gamma = 0.0,\n",
    "                     subsample = 0.7, colsample_bytree = 0.7, reg_alpha = 100, objective = 'binary:logistic',\n",
    "                     nthread = 4, scale_pos_weight = 1, seed = 69)\n",
    "\n",
    "model_3 = xgb5\n",
    "model_3.fit(X_train_3, y_train_3)\n",
    "y_prob_3 = model_3.predict_proba(X_test)\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "outputs_3 = outputs(cm_3)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_3.feature_importances_)\n",
    "# model_3.get_booster().get_scores(importance = 'gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting top features\n",
    "out = pd.DataFrame({'Feature' : list(X_train.columns), \n",
    "                   'Importance Score': model_3.feature_importances_}).sort_values('Importance Score', ascending = False)\n",
    "top_3 = out.iloc[:30,:]['Feature'].values\n",
    "# top = out[out['Importance Score'] > 0.0018]['Feature'].values\n",
    "# len(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cum_10_G_PLUS_MINUS_away_H2H',\n",
       " 'cum_10_G_PLUS_MINUS_home_H2H',\n",
       " 'cum_5_G_PLUS_MINUS_away_H2H',\n",
       " 'cum_5_G_PLUS_MINUS_home_H2H',\n",
       " 'cum_5_F_PLUS_MINUS_away_H2H',\n",
       " 'cum_10_F_PLUS_MINUS_away_H2H',\n",
       " 'cum_5_F_PLUS_MINUS_home_H2H',\n",
       " 'cum_10_G_PLUS_MINUS_home',\n",
       " 'cum_5_G_PLUS_MINUS_home',\n",
       " 'cum_10_C_PLUS_MINUS_home',\n",
       " 'cum_10_G_PLUS_MINUS_away',\n",
       " 'cum_10_F_PLUS_MINUS_home',\n",
       " 'cum_10_F_PLUS_MINUS_away',\n",
       " 'HOME_TEAM_home_win_pct_past4yrs']"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in top_3 if i in top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:17:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 497  582]\n",
      " [ 324 1221]]\n",
      "\n",
      "Accuracy :  0.6547\n"
     ]
    }
   ],
   "source": [
    "# Running our Final XgBoost Model\n",
    "xgb5 = XGBClassifier(learning_rate = 0.005, n_estimators = 120, max_depth = 2, min_child_weight = 4, gamma = 0.0,\n",
    "                     subsample = 0.7, colsample_bytree = 0.7, reg_alpha = 100, objective = 'binary:logistic',\n",
    "                     nthread = 4, scale_pos_weight = 1, seed = 69)\n",
    "\n",
    "model_3 = xgb5\n",
    "model_3.fit(X_train_3[top], y_train_3)\n",
    "y_prob_3 = model_3.predict_proba(X_test[top])\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "outputs_3 = outputs(cm_3)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting (round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Running our Final XgBoost Model\n",
    "# grid = {'n_estimators': [500, 1000, 2500], \n",
    "#         'max_leaf_nodes': [4, 5, 6]}\n",
    "\n",
    "# model_3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.005, objective= 'binary:logistic', seed=6969), \n",
    "#                         param_grid = grid, scoring='accuracy', n_jobs=-1, iid=False, cv=3, verbose = 3)\n",
    "# model_3.fit(train[predictors],train[target])\n",
    "# print('Optimal Hyperparameters: ', model_3.best_params_, model_3.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "# y_prob_3 = model_3.best_estimator_.predict_proba(X_test)\n",
    "# y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "# outputs_3 = outputs(cm_3)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "# print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Running our Final XgBoost Model\n",
    "# grid = {'n_estimators': [485], \n",
    "#         'max_leaf_nodes': [2,3,4]}\n",
    "\n",
    "# model_3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.005, objective= 'binary:logistic', seed=6969), \n",
    "#                         param_grid = grid, scoring='accuracy', n_jobs=-1, iid=False, cv=3, verbose = 3)\n",
    "# model_3.fit(train[predictors],train[target])\n",
    "# print('Optimal Hyperparameters: ', model_3.best_params_, model_3.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "# y_prob_3 = model_3.best_estimator_.predict_proba(X_test)\n",
    "# y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "# outputs_3 = outputs(cm_3)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "# print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#4: Neural Network\n",
    "\n",
    "The cells below go through an entire process of creating and cross-validating our neural network model using Sci-Kit Learn's MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 500  579]\n",
      " [ 322 1223]]\n",
      "\n",
      "Accuracy :  0.6566\n"
     ]
    }
   ],
   "source": [
    "# Neural Network \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# model_3 = MLPClassifier(hidden_layer_sizes =(75,), max_iter = 200,\n",
    "#                         activation = 'relu', solver = 'adam', random_state = 69)\n",
    "# model_3.fit(X_train, y_train)\n",
    "# y_prob_3 = model_3.predict_proba(X_test)\n",
    "# y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# grid = {'hidden_layer_sizes': [(50,),(100,),(200,),(400,),(800,),(50,50),(100,50)]}\n",
    "\n",
    "model_4 = MLPClassifier(max_iter = 10000, alpha = 0.5, activation = 'relu', hidden_layer_sizes=(100,25), \n",
    "                        solver = 'adam', learning_rate_init = 0.001, random_state = 69, tol = 0.0000000000000001)\n",
    "# model_4 = GridSearchCV(model_4, grid, n_jobs = -1, cv = 2, verbose = 3)\n",
    "model_4.fit(X_train_3, y_train_3)\n",
    "\n",
    "y_prob_4 = model_4.predict_proba(X_test)\n",
    "# print(model_4.best_params_)\n",
    "# y_pred_4 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_4[:,1]])\n",
    "y_pred_4 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_4[:,1]], index = y_test.index)\n",
    "# Confusion Matrix & Outputs \n",
    "cm_4 = confusion_matrix(y_test, y_pred_4)\n",
    "outputs_4 = outputs(cm_4)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_4) \n",
    "print (\"\\nAccuracy : \", outputs_4[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 10216 rows -- 66.1%\n",
      "Validation Data: 2625 rows -- 17.0%\n",
      "Testing Data: 2624 rows -- 17.0%\n"
     ]
    }
   ],
   "source": [
    "# Splitting Data\n",
    "FP1 = pd.read_csv('FP1.csv')\n",
    "FP2 = pd.read_csv('FP2.csv')\n",
    "df = pd.merge(FP1, FP2, on = 'GAME_ID').rename(columns={'SEASON_y':'SEASON'})\n",
    "df = df.drop(['TIMESTAMP','HOME_TEAM_NAME', 'SEASON_x'], axis=1)\n",
    "\n",
    "train_df = df[df['SEASON'].between(2007,2014)]\n",
    "val_df = df[df['SEASON'].between(2015,2016)]\n",
    "test_df = df[df['SEASON'].between(2017,2018)]\n",
    "\n",
    "train_df, val_df, test_df = train_df.drop(['SEASON', 'GAME_ID'], axis = 1), val_df.drop(['SEASON', 'GAME_ID'], axis = 1), test_df.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train, y_train = train_df.drop('HOME_TEAM_WINS', axis = 1), train_df['HOME_TEAM_WINS']\n",
    "X_val, y_val = val_df.drop('HOME_TEAM_WINS', axis = 1), val_df['HOME_TEAM_WINS']\n",
    "X_test, y_test = test_df.drop('HOME_TEAM_WINS', axis = 1), test_df['HOME_TEAM_WINS']\n",
    "\n",
    "train_prop = np.round(len(X_train) / (len(df)), 3)\n",
    "val_prop = np.round(len(X_val) / (len(df)), 3)\n",
    "test_prop = np.round(len(X_test) / (len(df)), 3)\n",
    "\n",
    "print('Training Data: ' + str(len(X_train)) + ' rows -- ' + str(np.round(train_prop*100,2)) + '%')\n",
    "print('Validation Data: ' + str(len(X_val)) + ' rows -- ' + str(np.round(val_prop*100,2)) + '%')\n",
    "print('Testing Data: ' + str(len(X_test)) + ' rows -- ' + str(np.round(test_prop*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[03:13:59] /Users/travis/build/dmlc/xgboost/src/predictor/cpu_predictor.cc:258: Check failed: m->NumColumns() == model.learner_model_param->num_feature (832 vs. 833) : Number of columns in data must equal to trained model.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000114936074 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n  [bt] (1) 2   libxgboost.dylib                    0x0000000114a5157d void xgboost::predictor::CPUPredictor::DispatchedInplacePredict<xgboost::data::ArrayAdapter>(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, xgboost::gbm::GBTreeModel const&, float, xgboost::PredictionCacheEntry*, unsigned int, unsigned int) const + 365\n  [bt] (2) 3   libxgboost.dylib                    0x0000000114a4d8b1 xgboost::predictor::CPUPredictor::InplacePredict(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, xgboost::gbm::GBTreeModel const&, float, xgboost::PredictionCacheEntry*, unsigned int, unsigned int) const + 401\n  [bt] (3) 4   libxgboost.dylib                    0x00000001149d48a8 xgboost::gbm::GBTree::InplacePredict(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, float, xgboost::PredictionCacheEntry*, unsigned int, unsigned int) const + 424\n  [bt] (4) 5   libxgboost.dylib                    0x00000001149e932b xgboost::LearnerImpl::InplacePredict(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, xgboost::PredictionType, float, xgboost::HostDeviceVector<float>**, unsigned int, unsigned int) + 123\n  [bt] (5) 6   libxgboost.dylib                    0x000000011492fb5b void InplacePredictImpl<xgboost::data::ArrayAdapter>(std::__1::shared_ptr<xgboost::data::ArrayAdapter>, std::__1::shared_ptr<xgboost::DMatrix>, char const*, xgboost::Learner*, unsigned long, unsigned long, unsigned long long const**, unsigned long long*, float const**) + 843\n  [bt] (6) 7   libxgboost.dylib                    0x000000011492f4e3 XGBoosterPredictFromDense + 339\n  [bt] (7) 8   libffi.7.dylib                      0x000000010182fead ffi_call_unix64 + 85\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-409-a583dfd02c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_prob_1_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_prob_2_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_prob_3_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_prob_4_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;31m# binary:logistic: Expand the prob vector into 2-class matrix after predict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;31m# binary:logitraw: Unsupported by predict_proba()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m         class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1274\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi:softmax\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m    821\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_maybe_np_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_np_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m             _check_call(\n\u001b[0m\u001b[1;32m   1847\u001b[0m                 _LIB.XGBoosterPredictFromDense(\n\u001b[1;32m   1848\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [03:13:59] /Users/travis/build/dmlc/xgboost/src/predictor/cpu_predictor.cc:258: Check failed: m->NumColumns() == model.learner_model_param->num_feature (832 vs. 833) : Number of columns in data must equal to trained model.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000114936074 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n  [bt] (1) 2   libxgboost.dylib                    0x0000000114a5157d void xgboost::predictor::CPUPredictor::DispatchedInplacePredict<xgboost::data::ArrayAdapter>(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, xgboost::gbm::GBTreeModel const&, float, xgboost::PredictionCacheEntry*, unsigned int, unsigned int) const + 365\n  [bt] (2) 3   libxgboost.dylib                    0x0000000114a4d8b1 xgboost::predictor::CPUPredictor::InplacePredict(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, xgboost::gbm::GBTreeModel const&, float, xgboost::PredictionCacheEntry*, unsigned int, unsigned int) const + 401\n  [bt] (3) 4   libxgboost.dylib                    0x00000001149d48a8 xgboost::gbm::GBTree::InplacePredict(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, float, xgboost::PredictionCacheEntry*, unsigned int, unsigned int) const + 424\n  [bt] (4) 5   libxgboost.dylib                    0x00000001149e932b xgboost::LearnerImpl::InplacePredict(dmlc::any const&, std::__1::shared_ptr<xgboost::DMatrix>, xgboost::PredictionType, float, xgboost::HostDeviceVector<float>**, unsigned int, unsigned int) + 123\n  [bt] (5) 6   libxgboost.dylib                    0x000000011492fb5b void InplacePredictImpl<xgboost::data::ArrayAdapter>(std::__1::shared_ptr<xgboost::data::ArrayAdapter>, std::__1::shared_ptr<xgboost::DMatrix>, char const*, xgboost::Learner*, unsigned long, unsigned long, unsigned long long const**, unsigned long long*, float const**) + 843\n  [bt] (6) 7   libxgboost.dylib                    0x000000011492f4e3 XGBoosterPredictFromDense + 339\n  [bt] (7) 8   libffi.7.dylib                      0x000000010182fead ffi_call_unix64 + 85\n\n"
     ]
    }
   ],
   "source": [
    "# Creating blend_df \n",
    "y_prob_1_val = model_1.predict_proba(X_val)\n",
    "y_prob_2_val = model_2.predict_proba(X_val[top])\n",
    "y_prob_3_val = model_3.predict_proba(X_val)\n",
    "y_prob_4_val = model_4.predict_proba(X_val)\n",
    "\n",
    "\n",
    "\n",
    "val_df_blend = pd.DataFrame({'Logistic Regression': list(y_prob_1_val[:,1]), \n",
    "                             'Random Forest': list(y_prob_2_val[:,1]), \n",
    "                             'XGBoost': list(y_prob_3_val[:,1]), \n",
    "                             'Neural Network': y_prob_4_val[:,1], \n",
    "                             'HOME_TEAM_WINS': y_val})\n",
    "test_df_blend = pd.DataFrame({'Logistic Regression': y_prob_1[:,1], \n",
    "                             'Random Forest': y_prob_2[:,1], \n",
    "                             'XGBoost': y_prob_3[:,1], \n",
    "                             'Neural Network': y_prob_4[:,1], \n",
    "                             'HOME_TEAM_WINS': y_test})\n",
    "X_val_blend, y_val_blend = val_df_blend.drop('HOME_TEAM_WINS', axis=1), val_df_blend['HOME_TEAM_WINS']\n",
    "X_test_blend, y_test_blend = test_df_blend.drop('HOME_TEAM_WINS', axis=1), test_df_blend['HOME_TEAM_WINS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 482  597]\n",
      " [ 294 1251]]\n",
      "\n",
      "Accuracy :  0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Blended Logistic Regression\n",
    "\n",
    "model_5 = LogisticRegression(random_state = 69,  verbose = 3, n_jobs = -1)\n",
    "model_5.fit(X_val_blend, y_val_blend)\n",
    "y_prob_5 = model_5.predict_proba(X_test_blend)\n",
    "y_pred_5 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_5[:,1]], index = y_test_blend.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_5 = confusion_matrix(y_test_blend, y_pred_5)\n",
    "outputs_5 = outputs(cm_5)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_5) \n",
    "print (\"\\nAccuracy : \", outputs_5[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Results\n",
    "\n",
    "The cell below calculates the Accuracy (our primary performance metric), TPR, and FPR for each of the four models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Generating the Output Table\n",
    "# df = pd.DataFrame({'Baseline Model': np.round(outputs_0,4),\n",
    "#                   'Logistic Regression': np.round(outputs_1,4),\n",
    "#                   'Random Forest': np.round(outputs_2,4),\n",
    "#                   'XGBoost': np.round(outputs_3,4),\n",
    "#                   'Neural Network': np.round(outputs_4,4)}, index = ['Accuracy', 'TPR', 'FPR'], ).T\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
