{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Importing Libraries & Functions\n",
    "\n",
    "\n",
    "#Normal Stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#StatsModels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "#SciKit Learn \n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error, confusion_matrix, mean_absolute_error, accuracy_score, auc, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def outputs(cm):\n",
    "    acc = np.round((cm.ravel()[0]+cm.ravel()[3])/sum(cm.ravel()),4)\n",
    "    tpr = np.round(cm.ravel()[3] / (cm.ravel()[3] + cm.ravel()[2]),4)\n",
    "    fpr = np.round(cm.ravel()[1] / (cm.ravel()[1] + cm.ravel()[0]),4)\n",
    "    outputs = [acc, tpr, fpr]\n",
    "    return outputs\n",
    "\n",
    "def cross_val_plot(x, y): \n",
    "    sns.set_style('darkgrid')\n",
    "    fig, ax = plt.subplots(figsize = (12,6))\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "\n",
    "    plt.title('Accuracy v.s. Parameter', fontsize = 20, fontweight = 'bold', pad = 15)\n",
    "    plt.xlabel('Parameter Value', fontsize = 14, labelpad = 15), plt.ylabel('Accuracy', fontsize = 14, labelpad = 15)\n",
    "    plt.xticks(fontsize = 12), plt.yticks(fontsize = 12);\n",
    "    plt.show();\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Loading Data + Only Newer seasons for simplicity + Filtering Unnecessary Columns \n",
    "games = pd.read_csv('games.csv')\n",
    "timestamp = pd.to_datetime(games['GAME_DATE_EST'])\n",
    "games.insert(0, 'TIMESTAMP', timestamp)\n",
    "games = games[games['SEASON'].isin([2010,2011,2012,2013,2014,2015,2016,2017,2018, 2019])].sort_values('TIMESTAMP')\n",
    "\n",
    "teams = pd.read_csv('teams.csv')\n",
    "teams = teams[['TEAM_ID','NICKNAME']]\n",
    "team_dict = teams.set_index('TEAM_ID').T.to_dict('list')\n",
    "games['HOME_TEAM_NAME'] = games['HOME_TEAM_ID'].map(team_dict)\n",
    "games['AWAY_TEAM_NAME'] = games['VISITOR_TEAM_ID'].map(team_dict)\n",
    "\n",
    "\n",
    "drop_cols = ['GAME_DATE_EST', 'GAME_STATUS_TEXT', 'HOME_TEAM_ID', \n",
    "             'VISITOR_TEAM_ID', 'TEAM_ID_home', 'TEAM_ID_away', 'SEASON']\n",
    "games = games.drop(drop_cols, axis = 1).reset_index(drop = True)\n",
    "front_cols = ['TIMESTAMP', 'GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']\n",
    "temp_games = games[front_cols]\n",
    "games = pd.concat([temp_games, games.drop(front_cols, axis = 1)], axis = 1)\n",
    "games['HOME_TEAM_NAME'] = games['HOME_TEAM_NAME'].apply(lambda x: x[0])\n",
    "games['AWAY_TEAM_NAME'] = games['AWAY_TEAM_NAME'].apply(lambda x: x[0])\n",
    "\n",
    "#Sanity Check: Making sure each teams plays 82 regular szn games\n",
    "# reg_start = '2018-10-16'\n",
    "# playoff_start = '2019-04-13'\n",
    "# reg = games[(games[\"TIMESTAMP\"] >= reg_start) & (games[\"TIMESTAMP\"] < playoff_start)].reset_index(drop = True)\n",
    "# game_counts = reg['HOME_TEAM_NAME'].value_counts().sort_index() + reg['AWAY_TEAM_NAME'].value_counts().sort_index()\n",
    "# print('Number of teams: ', len(reg['HOME_TEAM_NAME'].value_counts()))\n",
    "# print('Number of teams playing 82 games: ', np.count_nonzero(game_counts == 82))\n",
    "reg = games.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2: Cumulative Player Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pos_stat(stat_list):\n",
    "    \n",
    "    \n",
    "    ### Takes in a list of statistics to consider from the DETAILS data frame and calculates\n",
    "    ### the stats by position in each home for the home and away team.\n",
    "    ### NOTE: THESE ARE NOT CUMULATIVE STATS!!!!!\n",
    "    \n",
    "\n",
    "    # Removing non-starters, getting rid of unnecessary columns\n",
    "    details = pd.read_csv('games_details.csv')\n",
    "    details = details[~details['START_POSITION'].isna()]\n",
    "    details['TEAM_NAME'] = details['TEAM_ID'].map(team_dict).apply(lambda x: x[0])\n",
    "    details['MINS'] = details['MIN'].str.split(':').apply(lambda x: float(x[0]) + (float(x[1])/60))\n",
    "    details = details.drop(['TEAM_ABBREVIATION', 'TEAM_CITY', 'COMMENT', 'MIN'], axis = 1)\n",
    "    temp_cols = ['GAME_ID','START_POSITION', 'TEAM_NAME']\n",
    "    final_cols = np.append(temp_cols, stat_list).flatten()\n",
    "    details = details[final_cols]\n",
    "    #details = details.iloc[:300, :]\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    for stat in stat_list: \n",
    "        \n",
    "        \n",
    "        # Groupby the GAME, POSITION and TEAM and add up all stats by each position \n",
    "        df = details.copy()\n",
    "        df = df.groupby(['GAME_ID', 'START_POSITION', 'TEAM_NAME']).sum().reset_index()\n",
    "        df = pd.merge(df, games[['GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']], on = 'GAME_ID')\n",
    "\n",
    "        # Need to Re-Order the DF s.t. home and away are known \n",
    "        home_condition = (df['TEAM_NAME'] == df['HOME_TEAM_NAME'])\n",
    "        away_condition = (df['TEAM_NAME'] != df['HOME_TEAM_NAME'])\n",
    "\n",
    "        # Adding HOME Columns Manually \n",
    "        df['C_' + str(stat) + '_home'] = df[ home_condition & (df['START_POSITION'] == 'C')][stat]\n",
    "        df['F_' + str(stat) + '_home'] = df[ home_condition & (df['START_POSITION'] == 'F')][stat]\n",
    "        df['G_' + str(stat) + '_home'] = df[ home_condition & (df['START_POSITION'] == 'G')][stat]\n",
    "\n",
    "        # Adding AWAY Columns Manually \n",
    "        df['C_' + str(stat) + '_away'] = df[ away_condition & (df['START_POSITION'] == 'C')][stat]\n",
    "        df['F_' + str(stat) + '_away'] = df[ away_condition & (df['START_POSITION'] == 'F')][stat]\n",
    "        df['G_' + str(stat) + '_away'] = df[ away_condition & (df['START_POSITION'] == 'G')][stat]\n",
    "\n",
    "        # Grouping Again to Get Rid of NaN \n",
    "        df = df.groupby('GAME_ID', as_index = False).sum()\n",
    "        if stat_list.index(stat) > 0: \n",
    "            df = df.drop('GAME_ID', axis = 1)\n",
    "        # Append to final data frame \n",
    "        output_df = pd.concat([output_df, df], axis = 1)\n",
    "\n",
    "    output_df = output_df.drop(stat_list, axis = 1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_list = ['FGM', 'FGA','FG_PCT', 'FG3M', 'FG3A', \n",
    "             'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB',\n",
    "             'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', \n",
    "             'PF', 'PTS', 'PLUS_MINUS']\n",
    "pos_df = pos_stat(stat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Calculing Player Efficiency Rating\n",
    "# Formula: (PTS + REB + AST + STL + BLK − Missed FG − Missed FT - TO) / GP\n",
    "p1 = pos_df['C_PTS_home'] + pos_df['C_REB_home'] + pos_df['C_AST_home'] + pos_df['C_STL_home']\n",
    "missed_fg = (pos_df['C_FGA_home'] - pos_df['C_FGM_home'])\n",
    "missed_ft = pos_df['C_FTA_home'] - pos_df['C_FTM_home']\n",
    "p2 = (missed_fg + missed_ft + pos_df['C_TO_home'])\n",
    "per = (p1 - p2)/82\n",
    "\n",
    "for pos in ['C', 'F', 'G']: \n",
    "    p1 = pos_df[pos + '_PTS_home'] + pos_df[pos + '_REB_home'] + pos_df[pos + '_AST_home'] + pos_df[pos + '_STL_home']\n",
    "    missed_fg = (pos_df[pos + '_FGA_home'] - pos_df[pos + '_FGM_home'])\n",
    "    missed_ft = pos_df[pos + '_FTA_home'] - pos_df[pos + '_FTM_home']\n",
    "    p2 = (missed_fg + missed_ft + pos_df[pos + '_TO_home'])\n",
    "    per = (p1-p2)/82\n",
    "    pos_df[pos + '_PER_home'] = per\n",
    "    \n",
    "for pos in ['C', 'F', 'G']: \n",
    "    p1 = pos_df[pos + '_PTS_away'] + pos_df[pos + '_REB_away'] + pos_df[pos + '_AST_away'] + pos_df[pos + '_STL_away']\n",
    "    missed_fg = (pos_df[pos + '_FGA_away'] - pos_df[pos + '_FGM_away'])\n",
    "    missed_ft = pos_df[pos + '_FTA_away'] - pos_df[pos + '_FTM_away']\n",
    "    p2 = (missed_fg + missed_ft + pos_df[pos + '_TO_away'])\n",
    "    per = (p1-p2)/82\n",
    "    pos_df[pos + '_PER_away'] = per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3: Momentum & Recency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "code_folding": [
     0,
     4,
     7,
     13,
     26,
     35,
     48,
     57
    ]
   },
   "outputs": [],
   "source": [
    "def cum_avg(arr): \n",
    "    temp_list = [np.mean(arr[:i]) for i in np.arange(len(arr)+1)][1:]\n",
    "    return np.append(np.nan, temp_list)[:-1]\n",
    "\n",
    "def cum_5_avg(arr): \n",
    "    means = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 5: \n",
    "            means = np.append(means, np.nan)\n",
    "        else: \n",
    "            means = np.append(means, np.mean(arr[i-5: i+1]))\n",
    "    return means\n",
    "\n",
    "def cum_10_avg(arr): \n",
    "    means = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 10: \n",
    "            means = np.append(means, np.nan)\n",
    "        else: \n",
    "            means = np.append(means, np.mean(arr[i-10: i+1]))\n",
    "    return means\n",
    "\n",
    "def cum_wins(arr): \n",
    "    temp_list = [np.sum(arr[:i]) for i in np.arange(len(arr)+1)][1:]\n",
    "    return np.append(np.nan, temp_list)[:-1]\n",
    "\n",
    "def cum_5_wins(arr): \n",
    "    wins = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 5: \n",
    "            wins = np.append(wins, np.nan)\n",
    "        else: \n",
    "            wins = np.append(wins, np.sum(arr[i-5: i+1]))\n",
    "    return wins\n",
    "\n",
    "def cum_10_wins(arr): \n",
    "    wins = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 10: \n",
    "            wins = np.append(wins, np.nan)\n",
    "        else: \n",
    "            wins = np.append(wins, np.sum(arr[i-10: i+1]))\n",
    "    return wins\n",
    "\n",
    "def cum_losses(arr): \n",
    "    temp_list = [np.count_nonzero(arr[:i]==0) for i in np.arange(len(arr)+1)][1:]\n",
    "    return np.append(np.nan, temp_list)[:-1]\n",
    "\n",
    "def cum_5_losses(arr): \n",
    "    losses = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 5: \n",
    "            losses = np.append(losses, np.nan)\n",
    "        else: \n",
    "            losses = np.append(losses, np.count_nonzero(arr[i-5: i+1]==0))\n",
    "    return losses\n",
    "\n",
    "def cum_10_losses(arr): \n",
    "    losses = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 10: \n",
    "            losses = np.append(losses, np.nan)\n",
    "        else: \n",
    "            losses = np.append(losses, np.count_nonzero(arr[i-10: i+1]==0))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 of 66 COMPLETE:  PTS_home -- TIME:  19.914891958236694\n",
      "# 1 of 66 COMPLETE:  FG_PCT_home -- TIME:  38.72488498687744\n",
      "# 2 of 66 COMPLETE:  FT_PCT_home -- TIME:  59.661585092544556\n",
      "# 3 of 66 COMPLETE:  FG3_PCT_home -- TIME:  79.63998293876648\n",
      "# 4 of 66 COMPLETE:  AST_home -- TIME:  101.40286898612976\n",
      "# 5 of 66 COMPLETE:  REB_home -- TIME:  126.62532591819763\n",
      "# 6 of 66 COMPLETE:  C_FGM_home -- TIME:  156.09960889816284\n",
      "# 7 of 66 COMPLETE:  F_FGM_home -- TIME:  187.67229008674622\n",
      "# 8 of 66 COMPLETE:  G_FGM_home -- TIME:  211.35696506500244\n",
      "# 9 of 66 COMPLETE:  C_FGA_home -- TIME:  236.4054229259491\n",
      "# 10 of 66 COMPLETE:  F_FGA_home -- TIME:  262.60438084602356\n",
      "# 11 of 66 COMPLETE:  G_FGA_home -- TIME:  292.63565492630005\n",
      "# 12 of 66 COMPLETE:  C_FG_PCT_home -- TIME:  318.6850309371948\n",
      "# 13 of 66 COMPLETE:  F_FG_PCT_home -- TIME:  349.52812099456787\n",
      "# 14 of 66 COMPLETE:  G_FG_PCT_home -- TIME:  380.8778989315033\n",
      "# 15 of 66 COMPLETE:  C_FG3M_home -- TIME:  415.42972683906555\n",
      "# 16 of 66 COMPLETE:  F_FG3M_home -- TIME:  444.47140288352966\n",
      "# 17 of 66 COMPLETE:  G_FG3M_home -- TIME:  473.3414080142975\n",
      "# 18 of 66 COMPLETE:  C_FG3A_home -- TIME:  502.8589828014374\n",
      "# 19 of 66 COMPLETE:  F_FG3A_home -- TIME:  546.882120847702\n",
      "# 20 of 66 COMPLETE:  G_FG3A_home -- TIME:  576.860426902771\n",
      "# 21 of 66 COMPLETE:  C_FG3_PCT_home -- TIME:  608.332316160202\n",
      "# 22 of 66 COMPLETE:  F_FG3_PCT_home -- TIME:  641.8922009468079\n",
      "# 23 of 66 COMPLETE:  G_FG3_PCT_home -- TIME:  672.8683049678802\n",
      "# 24 of 66 COMPLETE:  C_FTM_home -- TIME:  704.5586338043213\n",
      "# 25 of 66 COMPLETE:  F_FTM_home -- TIME:  733.8504490852356\n",
      "# 26 of 66 COMPLETE:  G_FTM_home -- TIME:  763.6075780391693\n",
      "# 27 of 66 COMPLETE:  C_FTA_home -- TIME:  794.4685187339783\n",
      "# 28 of 66 COMPLETE:  F_FTA_home -- TIME:  825.9228789806366\n",
      "# 29 of 66 COMPLETE:  G_FTA_home -- TIME:  857.0432288646698\n",
      "# 30 of 66 COMPLETE:  C_FT_PCT_home -- TIME:  888.6448748111725\n",
      "# 31 of 66 COMPLETE:  F_FT_PCT_home -- TIME:  921.4865109920502\n",
      "# 32 of 66 COMPLETE:  G_FT_PCT_home -- TIME:  954.2229087352753\n",
      "# 33 of 66 COMPLETE:  C_OREB_home -- TIME:  987.906054019928\n",
      "# 34 of 66 COMPLETE:  F_OREB_home -- TIME:  1022.4298229217529\n",
      "# 35 of 66 COMPLETE:  G_OREB_home -- TIME:  1059.8129420280457\n",
      "# 36 of 66 COMPLETE:  C_DREB_home -- TIME:  1104.9775857925415\n",
      "# 37 of 66 COMPLETE:  F_DREB_home -- TIME:  1141.4535529613495\n",
      "# 38 of 66 COMPLETE:  G_DREB_home -- TIME:  1178.1523480415344\n",
      "# 39 of 66 COMPLETE:  C_REB_home -- TIME:  1215.2890720367432\n",
      "# 40 of 66 COMPLETE:  F_REB_home -- TIME:  1253.0907499790192\n",
      "# 41 of 66 COMPLETE:  G_REB_home -- TIME:  1292.0396268367767\n",
      "# 42 of 66 COMPLETE:  C_AST_home -- TIME:  1331.7168989181519\n",
      "# 43 of 66 COMPLETE:  F_AST_home -- TIME:  1376.7668330669403\n",
      "# 44 of 66 COMPLETE:  G_AST_home -- TIME:  1424.8194868564606\n",
      "# 45 of 66 COMPLETE:  C_STL_home -- TIME:  1493.4711620807648\n",
      "# 46 of 66 COMPLETE:  F_STL_home -- TIME:  1548.373039007187\n",
      "# 47 of 66 COMPLETE:  G_STL_home -- TIME:  1598.0704770088196\n",
      "# 48 of 66 COMPLETE:  C_BLK_home -- TIME:  1662.914988040924\n",
      "# 49 of 66 COMPLETE:  F_BLK_home -- TIME:  1709.158716917038\n",
      "# 50 of 66 COMPLETE:  G_BLK_home -- TIME:  1757.97851896286\n",
      "# 51 of 66 COMPLETE:  C_TO_home -- TIME:  1803.9462578296661\n",
      "# 52 of 66 COMPLETE:  F_TO_home -- TIME:  1850.8839609622955\n",
      "# 53 of 66 COMPLETE:  G_TO_home -- TIME:  1899.6114518642426\n",
      "# 54 of 66 COMPLETE:  C_PF_home -- TIME:  1944.8514349460602\n",
      "# 55 of 66 COMPLETE:  F_PF_home -- TIME:  1991.9340970516205\n",
      "# 56 of 66 COMPLETE:  G_PF_home -- TIME:  2044.5467410087585\n",
      "# 57 of 66 COMPLETE:  C_PTS_home -- TIME:  2104.345713853836\n",
      "# 58 of 66 COMPLETE:  F_PTS_home -- TIME:  2158.7209980487823\n",
      "# 59 of 66 COMPLETE:  G_PTS_home -- TIME:  2215.5427570343018\n",
      "# 60 of 66 COMPLETE:  C_PLUS_MINUS_home -- TIME:  2270.2475638389587\n",
      "# 61 of 66 COMPLETE:  F_PLUS_MINUS_home -- TIME:  2327.1652557849884\n",
      "# 62 of 66 COMPLETE:  G_PLUS_MINUS_home -- TIME:  2381.849566936493\n",
      "# 63 of 66 COMPLETE:  C_PER_home -- TIME:  2437.9239931106567\n",
      "# 64 of 66 COMPLETE:  F_PER_home -- TIME:  2495.1106028556824\n",
      "# 65 of 66 COMPLETE:  G_PER_home -- TIME:  2561.693340063095\n"
     ]
    }
   ],
   "source": [
    "# Cum Stats Home\n",
    "df = reg.copy()\n",
    "df = df.merge(pos_df, on = 'GAME_ID')\n",
    "df['TEAM_LIST'] = df['HOME_TEAM_NAME'] + ', ' + df['AWAY_TEAM_NAME']\n",
    "df_home = df.drop('AWAY_TEAM_NAME', axis = 1)\n",
    "df_home = df.sort_values('TIMESTAMP', ascending = True)\n",
    "column_list = df_home.columns[4:]\n",
    "home_stat_list = [stat for stat in column_list if 'home' in stat]\n",
    "\n",
    "t = time.time()\n",
    "for stat in home_stat_list: \n",
    "    \n",
    "    keep_cols = np.append(['TIMESTAMP', 'TEAM_LIST','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME'], stat)\n",
    "    df_home_temp = df_home[keep_cols]\n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1 = df_home_temp.groupby(['HOME_TEAM_NAME', 'GAME_ID']).mean()\n",
    "    df_2 = df_home_temp.groupby(['HOME_TEAM_NAME']).agg(list)\n",
    "\n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists = df_2.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists = df_2.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists = df_2.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals = cum_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals = cum_5_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals = cum_10_lists.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    df_1 = df_1.reset_index()\n",
    "    df_1['cum_' + str(stat)] = cum_vals\n",
    "    df_1['cum_5_' + str(stat)] = cum_5_vals\n",
    "    df_1['cum_10_' + str(stat)] = cum_10_vals\n",
    "    \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict = df_1.set_index('GAME_ID').iloc[:, -3].T.to_dict()\n",
    "    cum_dict_5 = df_1.set_index('GAME_ID').iloc[:, -2].T.to_dict()\n",
    "    cum_dict_10 = df_1.set_index('GAME_ID').iloc[:, -1].T.to_dict()\n",
    "\n",
    "    df_home['cum_' + str(stat)] = df['GAME_ID'].map(cum_dict)\n",
    "    df_home['cum_5_' + str(stat)] = df['GAME_ID'].map(cum_dict_5)\n",
    "    df_home['cum_10_' + str(stat)] = df['GAME_ID'].map(cum_dict_10)\n",
    "    \n",
    "    \n",
    "    # HEAD TO HEAD STATS ==> SAME THING BASICALLY \n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1_H2H = df_home.groupby(['TEAM_LIST', 'GAME_ID']).mean()\n",
    "    df_2_H2H = df_home.groupby('TEAM_LIST').agg(list)\n",
    "    \n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists_H2H = df_2_H2H.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists_H2H = df_2_H2H.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists_H2H = df_2_H2H.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals_H2H = cum_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    #df_1_H2H = df_1_H2H.reset_index()\n",
    "    df_1_H2H['cum_' + str(stat) + '_H2H'] = cum_vals_H2H\n",
    "    df_1_H2H['cum_5_' + str(stat) + '_H2H'] = cum_5_vals_H2H\n",
    "    df_1_H2H['cum_10_' + str(stat) + '_H2H'] = cum_10_vals_H2H\n",
    "    \n",
    "   \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -3].T.to_dict()\n",
    "    cum_5_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -2].T.to_dict()\n",
    "    cum_10_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -1].T.to_dict()\n",
    "    \n",
    "    df_home['cum_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_dict_H2H)\n",
    "    df_home['cum_5_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_5_dict_H2H)\n",
    "    df_home['cum_10_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_10_dict_H2H)\n",
    "  \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print('# ' + str(home_stat_list.index(stat)) + ' of ' + str(len(home_stat_list)) + ' COMPLETE: ', str(stat), '-- TIME: ', time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 of 65 COMPLETE:  PTS_away -- TIME:  16.15410804748535\n",
      "# 2 of 65 COMPLETE:  FG_PCT_away -- TIME:  32.72529602050781\n",
      "# 3 of 65 COMPLETE:  FT_PCT_away -- TIME:  49.19173812866211\n",
      "# 4 of 65 COMPLETE:  FG3_PCT_away -- TIME:  66.24934792518616\n",
      "# 5 of 65 COMPLETE:  AST_away -- TIME:  83.94797992706299\n",
      "# 6 of 65 COMPLETE:  REB_away -- TIME:  102.28905582427979\n",
      "# 7 of 65 COMPLETE:  C_FGM_away -- TIME:  121.6736581325531\n",
      "# 8 of 65 COMPLETE:  F_FGM_away -- TIME:  140.9667067527771\n",
      "# 9 of 65 COMPLETE:  G_FGM_away -- TIME:  160.62837076187134\n",
      "# 10 of 65 COMPLETE:  C_FGA_away -- TIME:  180.8727867603302\n",
      "# 11 of 65 COMPLETE:  F_FGA_away -- TIME:  201.69956493377686\n",
      "# 12 of 65 COMPLETE:  G_FGA_away -- TIME:  223.4031798839569\n",
      "# 13 of 65 COMPLETE:  C_FG_PCT_away -- TIME:  245.4473750591278\n",
      "# 14 of 65 COMPLETE:  F_FG_PCT_away -- TIME:  267.7765049934387\n",
      "# 15 of 65 COMPLETE:  G_FG_PCT_away -- TIME:  290.7130858898163\n",
      "# 16 of 65 COMPLETE:  C_FG3M_away -- TIME:  315.6560769081116\n",
      "# 17 of 65 COMPLETE:  F_FG3M_away -- TIME:  342.7436110973358\n",
      "# 18 of 65 COMPLETE:  G_FG3M_away -- TIME:  369.86386489868164\n",
      "# 19 of 65 COMPLETE:  C_FG3A_away -- TIME:  395.42582607269287\n",
      "# 20 of 65 COMPLETE:  F_FG3A_away -- TIME:  421.2626910209656\n",
      "# 21 of 65 COMPLETE:  G_FG3A_away -- TIME:  447.78188586235046\n",
      "# 22 of 65 COMPLETE:  C_FG3_PCT_away -- TIME:  475.04918599128723\n",
      "# 23 of 65 COMPLETE:  F_FG3_PCT_away -- TIME:  502.28175616264343\n",
      "# 24 of 65 COMPLETE:  G_FG3_PCT_away -- TIME:  531.4586532115936\n",
      "# 25 of 65 COMPLETE:  C_FTM_away -- TIME:  561.5218908786774\n",
      "# 26 of 65 COMPLETE:  F_FTM_away -- TIME:  601.5759170055389\n",
      "# 27 of 65 COMPLETE:  G_FTM_away -- TIME:  636.994626045227\n",
      "# 28 of 65 COMPLETE:  C_FTA_away -- TIME:  678.2358419895172\n",
      "# 29 of 65 COMPLETE:  F_FTA_away -- TIME:  718.062089920044\n",
      "# 30 of 65 COMPLETE:  G_FTA_away -- TIME:  762.7726480960846\n",
      "# 31 of 65 COMPLETE:  C_FT_PCT_away -- TIME:  800.3880591392517\n",
      "# 32 of 65 COMPLETE:  F_FT_PCT_away -- TIME:  838.6728630065918\n",
      "# 33 of 65 COMPLETE:  G_FT_PCT_away -- TIME:  873.960855960846\n",
      "# 34 of 65 COMPLETE:  C_OREB_away -- TIME:  914.462121963501\n",
      "# 35 of 65 COMPLETE:  F_OREB_away -- TIME:  954.0239081382751\n",
      "# 36 of 65 COMPLETE:  G_OREB_away -- TIME:  994.9388468265533\n",
      "# 37 of 65 COMPLETE:  C_DREB_away -- TIME:  1042.9607779979706\n",
      "# 38 of 65 COMPLETE:  F_DREB_away -- TIME:  1085.7877171039581\n",
      "# 39 of 65 COMPLETE:  G_DREB_away -- TIME:  1131.393427848816\n",
      "# 40 of 65 COMPLETE:  C_REB_away -- TIME:  1173.068946838379\n",
      "# 41 of 65 COMPLETE:  F_REB_away -- TIME:  1214.1506960391998\n",
      "# 42 of 65 COMPLETE:  G_REB_away -- TIME:  1255.413789987564\n",
      "# 43 of 65 COMPLETE:  C_AST_away -- TIME:  1298.2636649608612\n",
      "# 44 of 65 COMPLETE:  F_AST_away -- TIME:  1344.501100063324\n",
      "# 45 of 65 COMPLETE:  G_AST_away -- TIME:  1388.5646450519562\n",
      "# 46 of 65 COMPLETE:  C_STL_away -- TIME:  1431.5565948486328\n",
      "# 47 of 65 COMPLETE:  F_STL_away -- TIME:  1475.0264508724213\n",
      "# 48 of 65 COMPLETE:  G_STL_away -- TIME:  1519.3912138938904\n",
      "# 49 of 65 COMPLETE:  C_BLK_away -- TIME:  1565.7031049728394\n",
      "# 50 of 65 COMPLETE:  F_BLK_away -- TIME:  1612.5942239761353\n",
      "# 51 of 65 COMPLETE:  G_BLK_away -- TIME:  1658.0945708751678\n",
      "# 52 of 65 COMPLETE:  C_TO_away -- TIME:  1707.3665997982025\n",
      "# 53 of 65 COMPLETE:  F_TO_away -- TIME:  1759.4687330722809\n",
      "# 54 of 65 COMPLETE:  G_TO_away -- TIME:  1811.4383058547974\n",
      "# 55 of 65 COMPLETE:  C_PF_away -- TIME:  1867.0618422031403\n",
      "# 56 of 65 COMPLETE:  F_PF_away -- TIME:  1919.2219371795654\n",
      "# 57 of 65 COMPLETE:  G_PF_away -- TIME:  1969.0683779716492\n",
      "# 58 of 65 COMPLETE:  C_PTS_away -- TIME:  2025.7312459945679\n",
      "# 59 of 65 COMPLETE:  F_PTS_away -- TIME:  2082.1388399600983\n",
      "# 60 of 65 COMPLETE:  G_PTS_away -- TIME:  2138.1463890075684\n",
      "# 61 of 65 COMPLETE:  C_PLUS_MINUS_away -- TIME:  2200.9914212226868\n",
      "# 62 of 65 COMPLETE:  F_PLUS_MINUS_away -- TIME:  2258.8778779506683\n",
      "# 63 of 65 COMPLETE:  G_PLUS_MINUS_away -- TIME:  2317.1590571403503\n",
      "# 64 of 65 COMPLETE:  C_PER_away -- TIME:  2371.0430278778076\n",
      "# 65 of 65 COMPLETE:  F_PER_away -- TIME:  2427.3414409160614\n",
      "# 66 of 65 COMPLETE:  G_PER_away -- TIME:  2487.1597769260406\n"
     ]
    }
   ],
   "source": [
    "# Cum Stats Away\n",
    "df_away = df.drop('HOME_TEAM_NAME', axis = 1)\n",
    "df_away = df.sort_values('TIMESTAMP', ascending = True)\n",
    "column_list = df_away.columns[4:]\n",
    "away_stat_list = [stat for stat in column_list if 'away' in stat]\n",
    "\n",
    "t = time.time()\n",
    "for stat in away_stat_list: \n",
    "    \n",
    "    keep_cols = np.append(['TIMESTAMP', 'TEAM_LIST','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME'], stat)\n",
    "    df_away_temp = df_away[keep_cols]\n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1 = df_away_temp.groupby(['AWAY_TEAM_NAME', 'GAME_ID']).mean()\n",
    "    df_2 = df_away_temp.groupby(['AWAY_TEAM_NAME']).agg(list)\n",
    "\n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists = df_2.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists = df_2.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists = df_2.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals = cum_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals = cum_5_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals = cum_10_lists.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    df_1 = df_1.reset_index()\n",
    "    df_1['cum_' + str(stat)] = cum_vals\n",
    "    df_1['cum_5_' + str(stat)] = cum_5_vals\n",
    "    df_1['cum_10_' + str(stat)] = cum_10_vals\n",
    "    \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict = df_1.set_index('GAME_ID').iloc[:, -3].T.to_dict()\n",
    "    cum_dict_5 = df_1.set_index('GAME_ID').iloc[:, -2].T.to_dict()\n",
    "    cum_dict_10 = df_1.set_index('GAME_ID').iloc[:, -1].T.to_dict()\n",
    "\n",
    "    df_away['cum_' + str(stat)] = df['GAME_ID'].map(cum_dict)\n",
    "    df_away['cum_5_' + str(stat)] = df['GAME_ID'].map(cum_dict_5)\n",
    "    df_away['cum_10_' + str(stat)] = df['GAME_ID'].map(cum_dict_10)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # HEAD TO HEAD STATS ==> SAME THING BASICALLY \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1_H2H = df_away.groupby(['TEAM_LIST', 'GAME_ID']).mean()\n",
    "    df_2_H2H = df_away.groupby('TEAM_LIST').agg(list)\n",
    "    \n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists_H2H = df_2_H2H.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists_H2H = df_2_H2H.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists_H2H = df_2_H2H.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals_H2H = cum_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    #df_1_H2H = df_1_H2H.reset_index()\n",
    "    df_1_H2H['cum_' + str(stat) + '_H2H'] = cum_vals_H2H\n",
    "    df_1_H2H['cum_5_' + str(stat) + '_H2H'] = cum_5_vals_H2H\n",
    "    df_1_H2H['cum_10_' + str(stat) + '_H2H'] = cum_10_vals_H2H\n",
    "    \n",
    "   \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -3].T.to_dict()\n",
    "    cum_5_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -2].T.to_dict()\n",
    "    cum_10_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -1].T.to_dict()\n",
    "    \n",
    "    df_away['cum_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_dict_H2H)\n",
    "    df_away['cum_5_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_5_dict_H2H)\n",
    "    df_away['cum_10_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_10_dict_H2H)\n",
    "    \n",
    "    \n",
    "    print('# ' + str(away_stat_list.index(stat)+1) + ' of ' + str(len(away_stat_list)-1) + ' COMPLETE: ', str(stat), '-- TIME: ', time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Combining the Home and Away DataFrames \n",
    "df_home_2 = df_home.copy()\n",
    "df_away_2 = df_away.copy()\n",
    "df_home_2 = df_home_2.drop(np.append(home_stat_list, away_stat_list,), axis = 1)\n",
    "# df_home_2 = df_home_2.drop(['C_PER_away', 'F_PER_away', 'G_PER_away'], axis = 1)\n",
    "# df_away_2 = df_away_2.drop(['C_PER_away', 'F_PER_away', 'G_PER'], axis = 1)\n",
    "df_away_2 = df_away_2.drop(np.append(home_stat_list, away_stat_list), axis = 1)\n",
    "\n",
    "df_clean = pd.merge(df_home_2, df_away_2, on = 'GAME_ID')\n",
    "xcol = [col for col in df_clean.columns if '_x' in col]\n",
    "ycol = [col for col in df_clean.columns if '_y' in col]\n",
    "df_clean = df_clean.drop(ycol, axis = 1)\n",
    "df_clean.columns = df_clean.columns.str.replace(r'_x$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Home Win/Loss Counter\n",
    "df_clean2 = df_clean.copy()\n",
    "\n",
    "game_szn_dict = pd.read_csv('games.csv')[['GAME_ID','SEASON']].set_index('GAME_ID').iloc[:, 0].T.to_dict()\n",
    "df_clean2['SEASON'] = df_clean2['GAME_ID'].map(game_szn_dict)\n",
    "\n",
    "keep_cols = ['TIMESTAMP', 'SEASON', 'HOME_TEAM_WINS','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']\n",
    "df_clean2_temp = df_clean2[keep_cols]\n",
    "df_clean2_temp['AWAY_TEAM_WINS'] = df_clean2_temp['HOME_TEAM_WINS'].map({0:1, 1:0})\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['HOME_TEAM_NAME', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['HOME_TEAM_NAME']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_wins_home = df_g2['HOME_TEAM_WINS'].apply(cum_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_home = df_g2['HOME_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_home = df_g2['HOME_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_losses_home = df_g2['HOME_TEAM_WINS'].apply(cum_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_home = df_g2['HOME_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_home = df_g2['HOME_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_home'] = cum_wins_home\n",
    "df_clean2['cum_LOSSES_home'] = cum_losses_home\n",
    "df_clean2['cum_5_WINS_home'] = cum_5_wins_home\n",
    "df_clean2['cum_10_WINS_home'] = cum_10_wins_home\n",
    "df_clean2['cum_5_LOSSES_home'] = cum_5_losses_home\n",
    "df_clean2['cum_10_LOSSES_home'] = cum_10_losses_home\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Away Win/Loss Counter\n",
    "\n",
    "\n",
    "df_clean2_temp['AWAY_TEAM_WINS'] = df_clean2_temp['HOME_TEAM_WINS'].map({0:1, 1:0})\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['AWAY_TEAM_NAME', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['AWAY_TEAM_NAME']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # AWAY WINS!\n",
    "cum_wins_away = df_g2['AWAY_TEAM_WINS'].apply(cum_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_away = df_g2['AWAY_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_away = df_g2['AWAY_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # AWAY LOSSES\n",
    "cum_losses_away = df_g2['AWAY_TEAM_WINS'].apply(cum_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_away = df_g2['AWAY_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_away = df_g2['AWAY_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_away'] = cum_wins_away\n",
    "df_clean2['cum_LOSSES_away'] = cum_losses_away\n",
    "df_clean2['cum_5_WINS_away'] = cum_5_wins_away\n",
    "df_clean2['cum_10_WINS_away'] = cum_10_wins_away\n",
    "df_clean2['cum_5_LOSSES_away'] = cum_5_losses_away\n",
    "df_clean2['cum_10_LOSSES_away'] = cum_10_losses_away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# HEAD TO HEAD WIN/LOSS COUNTER \n",
    "\n",
    "#HOME HEAD TO HEAD Win/Loss Counter\n",
    "\n",
    "df_clean2_temp['AWAY_TEAM_WINS'] = df_clean2_temp['HOME_TEAM_WINS'].map({0:1, 1:0})\n",
    "df_clean2_temp['TEAM_LIST'] = df_clean2_temp['HOME_TEAM_NAME'] + ', ' + df_clean2_temp['AWAY_TEAM_NAME']\n",
    "\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['TEAM_LIST', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['TEAM_LIST']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_wins_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # HOME Losses!\n",
    "cum_losses_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_home_H2H'] = cum_wins_home_H2H\n",
    "df_clean2['cum_LOSSES_home_H2H'] = cum_losses_home_H2H\n",
    "df_clean2['cum_5_WINS_home_H2H'] = cum_5_wins_home_H2H\n",
    "df_clean2['cum_10_WINS_home_H2H'] = cum_10_wins_home_H2H\n",
    "df_clean2['cum_5_LOSSES_home_H2H'] = cum_5_losses_home_H2H\n",
    "df_clean2['cum_10_LOSSES_home_H2H'] = cum_10_losses_home_H2H\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#____________________\n",
    "\n",
    "# AWAY HEAD TO HEAD Win/Loss Counter\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['TEAM_LIST', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['TEAM_LIST']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_wins_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # HOME Losses!\n",
    "cum_losses_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_away_H2H'] = cum_wins_away_H2H\n",
    "df_clean2['cum_LOSSES_away_H2H'] = cum_losses_away_H2H\n",
    "df_clean2['cum_5_WINS_away_H2H'] = cum_5_wins_away_H2H\n",
    "df_clean2['cum_10_WINS_away_H2H'] = cum_10_wins_away_H2H\n",
    "df_clean2['cum_5_LOSSES_away_H2H'] = cum_5_losses_away_H2H\n",
    "df_clean2['cum_10_LOSSES_away_H2H'] = cum_10_losses_away_H2H\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F5: X-Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Creating Roster Data Frame\n",
    "# details = pd.read_csv('games_details.csv')\n",
    "# details = details[~details['START_POSITION'].isna()]\n",
    "# details['TEAM_NAME'] = details['TEAM_ID'].map(team_dict).apply(lambda x: x[0])\n",
    "\n",
    "# details = details[['GAME_ID', 'TEAM_NAME', 'PLAYER_NAME']]\n",
    "# details_grouped = details.groupby(['GAME_ID', 'TEAM_NAME'], as_index = False).agg(list)\n",
    "\n",
    "# roster = pd.merge(details_grouped, games[['TIMESTAMP','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']], on = 'GAME_ID')\n",
    "# home_condition = (roster['TEAM_NAME'] == roster['HOME_TEAM_NAME'])\n",
    "# away_condition = (roster['TEAM_NAME'] != roster['HOME_TEAM_NAME'])\n",
    "\n",
    "# roster_home = roster[home_condition]\n",
    "# roster_home['ROSTER_home'] = roster_home['PLAYER_NAME']\n",
    "# roster_home = roster_home[['TIMESTAMP','GAME_ID', 'ROSTER_home']]\n",
    "\n",
    "# roster_away = roster[away_condition]\n",
    "# roster_away['ROSTER_away'] = roster_away['PLAYER_NAME']\n",
    "# roster_away = roster_away[['TIMESTAMP','GAME_ID', 'ROSTER_away']]\n",
    "\n",
    "# roster = pd.merge(roster_home, roster_away, on = 'GAME_ID')\n",
    "# roster['SEASON'] = roster['TIMESTAMP_x'].dt.year\n",
    "# roster = roster.drop(['TIMESTAMP_x', 'TIMESTAMP_y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Creating Features for the # of Players on home/away team that were MVP, DPOY, ROY, 6MOY in previous 3 Seasons\n",
    "\n",
    "# def prev_3(award, awards): \n",
    "#     award_list = np.array(list(awards[award]))\n",
    "#     prev_3_list = []\n",
    "#     for i in np.arange(12): \n",
    "#         prev_3_list.append(award_list[np.arange(i+1, i+4)])\n",
    "#     return prev_3_list\n",
    "\n",
    "# awards = pd.read_csv('awards.csv').iloc[:15, :5]\n",
    "# awards['SEASON'] = awards['SEASON'].astype(int)\n",
    "# full_list = []\n",
    "# award_names = ['MVP', 'DPOY', 'ROY', '6MOY']\n",
    "# for award in award_names: \n",
    "#     temp_3_list = prev_3(award, awards)\n",
    "#     full_list.append(temp_3_list)\n",
    "# awards = awards.iloc[:12]\n",
    "# for i in np.arange(len(full_list)): \n",
    "#     awards['PREV_3_' + str(award_names[i])] = full_list[i]\n",
    "#     awards = awards.drop(award_names[i], axis = 1)\n",
    "\n",
    "# df_X = pd.merge(roster, awards, on = 'SEASON')\n",
    "# def count_award_home(row, award_name): \n",
    "#     return sum(item in row['ROSTER_home'] for item in row['PREV_3_' + str(award_name)])\n",
    "# def count_award_away(row, award_name): \n",
    "#     return sum(item in row['ROSTER_away'] for item in row['PREV_3_' + str(award_name)])\n",
    "\n",
    "\n",
    "# award_name = '6MOY'\n",
    "# for award_name in award_names: \n",
    "#     df_X['COUNT_PREV_3_' + award_name + '_home'] = df_X.apply(lambda row: count_award_home(row, award_name), axis = 1)\n",
    "#     df_X['COUNT_PREV_3_' + award_name + '_away'] = df_X.apply(lambda row: count_award_away(row, award_name), axis = 1)\n",
    "#     df_X = df_X.drop('PREV_3_' + str(award_name), axis = 1)\n",
    "# award_df = df_X.drop(['ROSTER_home', 'ROSTER_away', 'SEASON'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODDS DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN Rows:  10152\n",
      "Number of NaN Rows Saved:  3106\n"
     ]
    }
   ],
   "source": [
    "num = len(df_clean2) - len(df_clean2.dropna())\n",
    "print('Number of NaN Rows: ', num)\n",
    "cols = [col for col in df_clean2.columns if np.any(['5' in col, '10' in col])]\n",
    "for col in cols: \n",
    "    new_col = re.sub(r'_\\d{1,2}_', '_', col)\n",
    "    df_clean2[col] = df_clean2[col].fillna(df_clean2[new_col])\n",
    "print('Number of NaN Rows Saved: ', len(df_clean2) - num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Odds \n",
    "odds = pd.DataFrame()\n",
    "season_list = np.arange(2010, 2020)\n",
    "for season in season_list: \n",
    "    df = pd.read_excel('ODDS_DATA/odds_' + str(season) + '.xlsx', usecols = ['Date', 'Team', 'ML'])\n",
    "    df['Date'] = df['Date'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "\n",
    "    new_year_index = df[df['Date'].str.contains(r'01\\d{2}')].index[0]\n",
    "    year1 = np.repeat(str(season), new_year_index-1)\n",
    "    year2 = np.repeat(str(season+1), len(df) - len(year1))\n",
    "    df['Year'] = np.append(year1, year2)\n",
    "    df['TIMESTAMP'] = df['Year'] + df['Date']\n",
    "    df = df.drop('Date', axis = 1)\n",
    "    odds = odds.append(df)\n",
    "home_teams, home_odds = odds.iloc[1::2]['Team'], odds.iloc[1::2]['ML']\n",
    "away_teams, away_odds = odds.iloc[::2]['Team'], odds.iloc[::2]['ML']\n",
    "dates = odds.iloc[1::2]['TIMESTAMP']\n",
    "odds = pd.DataFrame({'TIMESTAMP' : list(dates), \n",
    "                   'HOME_TEAM_NAME' : list(home_teams),\n",
    "                   'AWAY_TEAM_NAME' : list(away_teams), \n",
    "                   'HOME_TEAM_ODDS' : list(home_odds), \n",
    "                   'AWAY_TEAM_ODDS' : list(away_odds)})\n",
    "odds['HOME_TEAM_NAME'] = odds['HOME_TEAM_NAME'].str.replace(r'([a-z])([A-Z])', r'\\1 \\2')\n",
    "odds['AWAY_TEAM_NAME'] = odds['AWAY_TEAM_NAME'].str.replace(r'([a-z])([A-Z])', r'\\1 \\2')\n",
    "\n",
    "odds['TIMESTAMP'] = pd.to_datetime(odds['TIMESTAMP'], format = '%Y%m%d', errors = 'coerce')\n",
    "teams = pd.read_csv('teams.csv', usecols = ['CITY', 'NICKNAME'])\n",
    "teams['CITY'][7] = 'LAClippers'\n",
    "teams['CITY'][8] = 'LALakers'\n",
    "\n",
    "odds['HOME_TEAM_NAME'] = odds['HOME_TEAM_NAME'].str.replace('New Jersey', 'Brooklyn')\n",
    "odds['AWAY_TEAM_NAME'] = odds['AWAY_TEAM_NAME'].str.replace('New Jersey', 'Brooklyn')\n",
    "\n",
    "team_city_dict = teams.set_index('CITY')['NICKNAME'].T.to_dict()\n",
    "odds['HOME_TEAM_NAME'] = odds['HOME_TEAM_NAME'].map(team_city_dict)\n",
    "odds['AWAY_TEAM_NAME'] = odds['AWAY_TEAM_NAME'].map(team_city_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(df_clean2, odds, on = ['TIMESTAMP', 'HOME_TEAM_NAME'])\n",
    "\n",
    "cols = [col for col in new_df.columns if np.any(['5' in col, '10' in col])]\n",
    "for col in cols: \n",
    "    new_col = re.sub(r'_\\d{1,2}_', '_', col)\n",
    "    new_df[col] = new_df[col].fillna(new_df[new_col])\n",
    "\n",
    "new_df = new_df.fillna(0)\n",
    "new_df = new_df.fillna(0)\n",
    "szn_dict = pd.read_csv('games.csv', usecols = ['GAME_ID', 'SEASON']).set_index('GAME_ID')['SEASON'].T.to_dict()\n",
    "new_df['SEASON'] = new_df['GAME_ID'].map(szn_dict)\n",
    "\n",
    "cum_col_list = np.append([col for col in list(df_fin.columns) if 'cum' in col], ['HOME_TEAM_WINS', 'SEASON', 'GAME_ID'])\n",
    "new_df = new_df[cum_col_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfactor = pd.read_csv('XFactor_features.csv')\n",
    "df = pd.merge(new_df, xfactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data -- 8945 rows -- 77.3%\n",
      "Testing Data -- 2624 rows -- 22.7%\n"
     ]
    }
   ],
   "source": [
    "# Splitting Data\n",
    "\n",
    "train_df = df[df['SEASON'].between(2010,2016)]\n",
    "test_df = df[df['SEASON'].between(2017,2018)]\n",
    "\n",
    "train_df, test_df = train_df.drop(['SEASON', 'GAME_ID'], axis = 1), test_df.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train, y_train = train_df.drop('HOME_TEAM_WINS', axis = 1), train_df['HOME_TEAM_WINS']\n",
    "X_test, y_test = test_df.drop('HOME_TEAM_WINS', axis = 1), test_df['HOME_TEAM_WINS']\n",
    "\n",
    "# X, y = df.drop('HOME_TEAM_WINS', axis = 1), df['HOME_TEAM_WINS']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "train_prop = np.round(len(X_train) / (len(X_train) + len(X_test)), 3)\n",
    "test_prop = np.round(len(X_test) / (len(X_train) + len(X_test)), 3)\n",
    "print('Training Data' + ' -- ' + str(len(X_train)) + ' rows -- ' + str(train_prop*100) + '%')\n",
    "print('Testing Data' + ' -- ' + str(len(X_test)) + ' rows -- ' + str(test_prop*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[   0 1079]\n",
      " [   0 1545]]\n",
      "\n",
      "Accuracy :  0.5888\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model \n",
    "model_0 = DummyClassifier(strategy = \"most_frequent\")\n",
    "model_0.fit(X_train, y_train)\n",
    "y_pred_0 = model_0.predict(X_test)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_0 = confusion_matrix(y_test, y_pred_0)\n",
    "outputs_0 = outputs(cm_0)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_0) \n",
    "print (\"\\nAccuracy : \", outputs_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Logistic Regression\n",
    "\n",
    "# model_1 = LogisticRegression(random_state = 69, max_iter = 5000, verbose = 3, n_jobs = -1)\n",
    "# model_1.fit(X_train, y_train)\n",
    "# y_prob_1 = model_1.predict_proba(X_test)\n",
    "# y_pred_1 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_1[:,1]], index = y_test.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_1 = confusion_matrix(y_test, y_pred_1)\n",
    "# outputs_1 = outputs(cm_1)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_1) \n",
    "# print (\"\\nAccuracy : \", outputs_1[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Random Forest\n",
    "\n",
    "# model_2 = RandomForestClassifier(random_state = 69)\n",
    "# model_2.fit(X_train, y_train)\n",
    "# y_prob_2 = model_2.predict_proba(X_test)\n",
    "# y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "# outputs_2 = outputs(cm_2)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "# print (\"\\nAccuracy : \", outputs_2[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# XgBoost\n",
    "\n",
    "model_2 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=108,\n",
    " max_depth=4,\n",
    " min_child_weight=2,\n",
    " gamma=0.3,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.01,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "model_2.fit(X_train, y_train)\n",
    "y_prob_2 = model_2.predict_proba(X_test)\n",
    "y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "outputs_2 = outputs(cm_2)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "print (\"\\nAccuracy : \", outputs_2[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "for i in np.arange(25,475,25): \n",
    "    layer_list.append((i,))\n",
    "    layer_list.append((i,i))\n",
    "    layer_list.append((i,i,i))\n",
    "    layer_list.append((i,np.round(i/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 78.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 280.1min\n"
     ]
    }
   ],
   "source": [
    "# Neural Network \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# model_3 = MLPClassifier(hidden_layer_sizes =(150,100,50), max_iter = 300,\n",
    "#                         activation = 'relu', solver = 'adam', random_state = 69)\n",
    "# model_3.fit(X_train, y_train)\n",
    "# y_prob_3 = model_3.predict_proba(X_test)\n",
    "# y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "grid = {\n",
    "    'hidden_layer_sizes': layer_list\n",
    "}\n",
    "model_3 = MLPClassifier(max_iter = 200, alpha = 0.0001, activation = 'relu',\n",
    "                        solver = 'adam', random_state = 69)\n",
    "model_3 = GridSearchCV(model_3, grid, n_jobs = -1, cv = 5, verbose = 3)\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "y_prob_3 = model_3.best_estimator_.predict(X_test)\n",
    "print(model_3.best_params_)\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3])\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "outputs_3 = outputs(cm_3)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "print (\"\\nAccuracy : \", outputs_3[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Make an instance of the Model ... .95 means 95 % of explained variance\n",
    "pca = PCA(.99) \n",
    "pca.fit(X_train)\n",
    "print(pca.n_components_)\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 278  763]\n",
      " [ 170 1306]]\n",
      "\n",
      "Accuracy :  0.6293\n"
     ]
    }
   ],
   "source": [
    "# XgBoost\n",
    "\n",
    "model_2 = XGBClassifier(random_state = 69)\n",
    "model_2.fit(X_train, y_train)\n",
    "y_prob_2 = model_2.predict_proba(X_test)\n",
    "y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "outputs_2 = outputs(cm_2)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "print (\"\\nAccuracy : \", outputs_2[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prob_2 = model_2.predict_proba(X_train)\n",
    "# y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_train.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_2 = confusion_matrix(y_train, y_pred_2)\n",
    "# outputs_2 = outputs(cm_2)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "# print (\"\\nAccuracy : \", outputs_2[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we should test our model on the training set to see the overfiting potential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train, predictors, useTrainCV = True, cv_folds = 5, early_stopping_rounds = 50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train[predictors].values, label=train[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold = cv_folds,\n",
    "                          metrics = 'error', early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "        alg.set_params(n_estimators = cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train[predictors], train['HOME_TEAM_WINS'], eval_metric = 'error')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train[predictors])\n",
    "    train_predprob = alg.predict_proba(train[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(train['HOME_TEAM_WINS'].values, train_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(train['HOME_TEAM_WINS'], train_predprob))\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.317888+0.00535608\ttest-error:0.357569+0.00622042\n",
      "[1]\ttrain-error:0.301997+0.00184675\ttest-error:0.351511+0.00527198\n",
      "[2]\ttrain-error:0.295019+0.0026074\ttest-error:0.348829+0.00748783\n",
      "[3]\ttrain-error:0.2905+0.00127641\ttest-error:0.343763+0.00615271\n",
      "[4]\ttrain-error:0.28531+0.00066403\ttest-error:0.340087+0.00368869\n",
      "[5]\ttrain-error:0.280567+0.00206109\ttest-error:0.338895+0.00642837\n",
      "[6]\ttrain-error:0.278059+0.00233751\ttest-error:0.337803+0.00892485\n",
      "[7]\ttrain-error:0.275253+0.00302776\ttest-error:0.337207+0.00457361\n",
      "[8]\ttrain-error:0.273664+0.00357941\ttest-error:0.337107+0.00522299\n",
      "[9]\ttrain-error:0.27051+0.00415267\ttest-error:0.338994+0.00650702\n",
      "[10]\ttrain-error:0.268276+0.00346004\ttest-error:0.339292+0.0086499\n",
      "[11]\ttrain-error:0.266413+0.00321433\ttest-error:0.338895+0.00767615\n",
      "[12]\ttrain-error:0.264154+0.00357408\ttest-error:0.338597+0.00732043\n",
      "[13]\ttrain-error:0.26177+0.00394797\ttest-error:0.337902+0.0076211\n",
      "[14]\ttrain-error:0.259411+0.00351274\ttest-error:0.338696+0.00836586\n",
      "[15]\ttrain-error:0.257722+0.00367291\ttest-error:0.337207+0.00725014\n",
      "[16]\ttrain-error:0.255562+0.00356077\ttest-error:0.337405+0.00631916\n",
      "[17]\ttrain-error:0.253178+0.00407436\ttest-error:0.337008+0.00688041\n",
      "[18]\ttrain-error:0.250149+0.00424505\ttest-error:0.338696+0.00839857\n",
      "[19]\ttrain-error:0.248113+0.0038467\ttest-error:0.33671+0.00922513\n",
      "[20]\ttrain-error:0.245679+0.00427133\ttest-error:0.338697+0.0102615\n",
      "[21]\ttrain-error:0.24409+0.00417849\ttest-error:0.339094+0.00914429\n",
      "[22]\ttrain-error:0.243221+0.00415434\ttest-error:0.338895+0.00893942\n",
      "[23]\ttrain-error:0.239968+0.00472075\ttest-error:0.339591+0.0086471\n",
      "[24]\ttrain-error:0.238602+0.00362398\ttest-error:0.339293+0.00917709\n",
      "[25]\ttrain-error:0.236666+0.00324322\ttest-error:0.340683+0.00960284\n",
      "[26]\ttrain-error:0.234878+0.00315373\ttest-error:0.340385+0.0113887\n",
      "[27]\ttrain-error:0.232469+0.00406167\ttest-error:0.339293+0.0108516\n",
      "[28]\ttrain-error:0.229887+0.00477421\ttest-error:0.338994+0.011037\n",
      "[29]\ttrain-error:0.228819+0.00492723\ttest-error:0.339193+0.0107191\n",
      "[30]\ttrain-error:0.225963+0.00411762\ttest-error:0.341478+0.0100808\n",
      "[31]\ttrain-error:0.223877+0.00394618\ttest-error:0.339789+0.00950522\n",
      "[32]\ttrain-error:0.223008+0.00421867\ttest-error:0.340981+0.00874149\n",
      "[33]\ttrain-error:0.22132+0.00447302\ttest-error:0.339889+0.00854437\n",
      "[34]\ttrain-error:0.220128+0.00477204\ttest-error:0.340187+0.00806249\n",
      "[35]\ttrain-error:0.217471+0.00496878\ttest-error:0.340385+0.00922432\n",
      "[36]\ttrain-error:0.215708+0.00533838\ttest-error:0.340286+0.0101617\n",
      "[37]\ttrain-error:0.214019+0.00552146\ttest-error:0.339789+0.0104034\n",
      "[38]\ttrain-error:0.212306+0.00620182\ttest-error:0.339988+0.0103924\n",
      "[39]\ttrain-error:0.210394+0.00653229\ttest-error:0.339292+0.010698\n",
      "[40]\ttrain-error:0.209252+0.00591074\ttest-error:0.340484+0.0095788\n",
      "[41]\ttrain-error:0.206719+0.00575112\ttest-error:0.340484+0.0101283\n",
      "[42]\ttrain-error:0.204708+0.00707524\ttest-error:0.341378+0.0097169\n",
      "[43]\ttrain-error:0.20205+0.00706799\ttest-error:0.340385+0.00897783\n",
      "[44]\ttrain-error:0.200164+0.00567994\ttest-error:0.340782+0.0074061\n",
      "[45]\ttrain-error:0.198699+0.00547256\ttest-error:0.340186+0.00791131\n",
      "[46]\ttrain-error:0.196762+0.00445354\ttest-error:0.339094+0.00705768\n",
      "[47]\ttrain-error:0.194428+0.00401659\ttest-error:0.33969+0.00801574\n",
      "[48]\ttrain-error:0.192267+0.00445226\ttest-error:0.340087+0.00863154\n",
      "[49]\ttrain-error:0.189685+0.00455939\ttest-error:0.34108+0.00829098\n",
      "[50]\ttrain-error:0.187798+0.00502794\ttest-error:0.340187+0.00709294\n",
      "[51]\ttrain-error:0.185935+0.00469837\ttest-error:0.340882+0.00841169\n",
      "[52]\ttrain-error:0.184272+0.00420536\ttest-error:0.340088+0.00869877\n",
      "[53]\ttrain-error:0.182782+0.00389463\ttest-error:0.340882+0.00751798\n",
      "[54]\ttrain-error:0.181938+0.00481511\ttest-error:0.339492+0.00760807\n",
      "[55]\ttrain-error:0.179852+0.00422734\ttest-error:0.339492+0.00717919\n",
      "[56]\ttrain-error:0.177815+0.00452837\ttest-error:0.338697+0.00702254\n",
      "[57]\ttrain-error:0.175829+0.00477609\ttest-error:0.337704+0.00632053\n",
      "[58]\ttrain-error:0.173222+0.00568228\ttest-error:0.338498+0.0070096\n",
      "[59]\ttrain-error:0.171707+0.00574825\ttest-error:0.338499+0.00574734\n",
      "[60]\ttrain-error:0.170466+0.00534153\ttest-error:0.338101+0.0072364\n",
      "[61]\ttrain-error:0.168429+0.0051744\ttest-error:0.338002+0.00695635\n",
      "[62]\ttrain-error:0.165921+0.0048739\ttest-error:0.337903+0.00795946\n",
      "[63]\ttrain-error:0.164258+0.00464164\ttest-error:0.340187+0.00657092\n",
      "[64]\ttrain-error:0.162321+0.00499571\ttest-error:0.338896+0.00761342\n",
      "[65]\ttrain-error:0.160533+0.00429056\ttest-error:0.339293+0.00817904\n",
      "[66]\ttrain-error:0.159416+0.00376\ttest-error:0.338896+0.0104882\n",
      "[67]\ttrain-error:0.157901+0.00439515\ttest-error:0.338201+0.00977094\n",
      "[68]\ttrain-error:0.156908+0.00494023\ttest-error:0.338697+0.00946985\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.7398\n",
      "AUC Score (Train): 0.812264\n"
     ]
    }
   ],
   "source": [
    "target = 'HOME_TEAM_WINS'\n",
    "train = pd.concat([X_train, y_train], axis = 1)\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "\n",
    "model_3 = XGBClassifier(learning_rate = 0.1, n_estimators = 1000, max_depth = 5, min_child_weight = 1,\n",
    "                     gamma = 0, subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic',\n",
    "                     nthread = 4, scale_pos_weight = 1, seed = 69)\n",
    "modelfit(model_3, train, X_train.columns, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 21.9min finished\n",
      "/Users/bennettcohen/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'min_child_weight': 1}, 0.6637865295637936)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=68, max_depth=5, \n",
    "                                                  min_child_weight=1, gamma=0, subsample=0.8, \n",
    "                                                  colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1, seed=27), \n",
    "                        param_grid = param_test1, \n",
    "                        scoring = 'accuracy', n_jobs = -1, iid=False , cv=5, verbose = 3)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  6.1min finished\n",
      "/Users/bennettcohen/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 4, 'min_child_weight': 2}, 0.6677600068275179)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# worked in intervals of 2 we look at +-1 from the optimal parameter value from above\n",
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[1,2]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=68, max_depth=5,\n",
    "                                                  min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test2, scoring='accuracy', n_jobs=-1, iid=False, cv=5, verbose = 3)\n",
    "gsearch2.fit(train[predictors],train[target])\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  6.1min finished\n",
      "/Users/bennettcohen/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.3}, 0.6693490326778619)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuning gamma\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=68, max_depth=4, \n",
    "                                                  min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test3, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.336735+0.00157148\ttest-error:0.36025+0.00400966\n",
      "[1]\ttrain-error:0.325536+0.00536517\ttest-error:0.353199+0.0073676\n",
      "[2]\ttrain-error:0.318857+0.00388242\ttest-error:0.347339+0.00692479\n",
      "[3]\ttrain-error:0.315529+0.00365669\ttest-error:0.344557+0.00703243\n",
      "[4]\ttrain-error:0.313096+0.00292453\ttest-error:0.342471+0.00639698\n",
      "[5]\ttrain-error:0.309222+0.00241401\ttest-error:0.346146+0.00894755\n",
      "[6]\ttrain-error:0.307335+0.00322426\ttest-error:0.342869+0.00618682\n",
      "[7]\ttrain-error:0.306367+0.00224368\ttest-error:0.341081+0.00847652\n",
      "[8]\ttrain-error:0.304405+0.00439689\ttest-error:0.341777+0.00857069\n",
      "[9]\ttrain-error:0.301847+0.00369603\ttest-error:0.339988+0.0106365\n",
      "[10]\ttrain-error:0.300333+0.00318606\ttest-error:0.340684+0.0100528\n",
      "[11]\ttrain-error:0.299613+0.00377988\ttest-error:0.340187+0.0092239\n",
      "[12]\ttrain-error:0.297676+0.00431628\ttest-error:0.340683+0.00787209\n",
      "[13]\ttrain-error:0.297279+0.00364048\ttest-error:0.341577+0.00921771\n",
      "[14]\ttrain-error:0.296335+0.00444925\ttest-error:0.339392+0.00887678\n",
      "[15]\ttrain-error:0.29482+0.00447807\ttest-error:0.339889+0.00831314\n",
      "[16]\ttrain-error:0.293579+0.00499993\ttest-error:0.339194+0.00433744\n",
      "[17]\ttrain-error:0.291766+0.00504363\ttest-error:0.33969+0.00603616\n",
      "[18]\ttrain-error:0.290276+0.00520807\ttest-error:0.340385+0.0051767\n",
      "[19]\ttrain-error:0.288439+0.00473019\ttest-error:0.340286+0.00511537\n",
      "[20]\ttrain-error:0.287048+0.00452167\ttest-error:0.340485+0.00573891\n",
      "[21]\ttrain-error:0.285211+0.00483611\ttest-error:0.339393+0.00708159\n",
      "[22]\ttrain-error:0.283398+0.00465884\ttest-error:0.338498+0.00740272\n",
      "[23]\ttrain-error:0.282405+0.00456228\ttest-error:0.339094+0.00732887\n",
      "[24]\ttrain-error:0.28022+0.00501398\ttest-error:0.338697+0.0082439\n",
      "[25]\ttrain-error:0.279376+0.00524863\ttest-error:0.337505+0.00757198\n",
      "[26]\ttrain-error:0.278034+0.00469878\ttest-error:0.337902+0.00722575\n",
      "[27]\ttrain-error:0.276222+0.00496327\ttest-error:0.338697+0.00774242\n",
      "[28]\ttrain-error:0.274533+0.0058805\ttest-error:0.337902+0.00833697\n",
      "[29]\ttrain-error:0.272969+0.00583423\ttest-error:0.338697+0.00736386\n",
      "[30]\ttrain-error:0.271728+0.00562039\ttest-error:0.336512+0.00736504\n",
      "[31]\ttrain-error:0.270883+0.00538522\ttest-error:0.33542+0.00771434\n",
      "[32]\ttrain-error:0.270088+0.00508363\ttest-error:0.334923+0.00794726\n",
      "[33]\ttrain-error:0.268599+0.00493231\ttest-error:0.334128+0.00808356\n",
      "[34]\ttrain-error:0.267928+0.00500194\ttest-error:0.332936+0.00737443\n",
      "[35]\ttrain-error:0.266438+0.00451319\ttest-error:0.33383+0.0094402\n",
      "[36]\ttrain-error:0.265097+0.00400091\ttest-error:0.333234+0.00846192\n",
      "[37]\ttrain-error:0.26321+0.00394661\ttest-error:0.334823+0.00881355\n",
      "[38]\ttrain-error:0.261248+0.00465973\ttest-error:0.333432+0.00825945\n",
      "[39]\ttrain-error:0.260181+0.00416306\ttest-error:0.333135+0.00829054\n",
      "[40]\ttrain-error:0.259039+0.00440975\ttest-error:0.333929+0.00973036\n",
      "[41]\ttrain-error:0.257176+0.00432643\ttest-error:0.333731+0.00909115\n",
      "[42]\ttrain-error:0.255959+0.00419428\ttest-error:0.334028+0.00822874\n",
      "[43]\ttrain-error:0.255314+0.00496974\ttest-error:0.333532+0.00640194\n",
      "[44]\ttrain-error:0.2537+0.00472739\ttest-error:0.334128+0.00644066\n",
      "[45]\ttrain-error:0.252583+0.00467519\ttest-error:0.334425+0.00677996\n",
      "[46]\ttrain-error:0.251838+0.00485024\ttest-error:0.334425+0.00769693\n",
      "[47]\ttrain-error:0.250546+0.00507518\ttest-error:0.334724+0.00788699\n",
      "[48]\ttrain-error:0.250199+0.00445662\ttest-error:0.335816+0.00779228\n",
      "[49]\ttrain-error:0.248659+0.00445216\ttest-error:0.336014+0.00754516\n",
      "[50]\ttrain-error:0.247443+0.00446698\ttest-error:0.336511+0.00802878\n",
      "[51]\ttrain-error:0.246896+0.00542497\ttest-error:0.337107+0.00737813\n",
      "[52]\ttrain-error:0.245158+0.00498539\ttest-error:0.335617+0.0077705\n",
      "[53]\ttrain-error:0.24342+0.00472299\ttest-error:0.336114+0.00692041\n",
      "[54]\ttrain-error:0.242327+0.00456974\ttest-error:0.336214+0.00687148\n",
      "[55]\ttrain-error:0.24121+0.00395321\ttest-error:0.337406+0.0073256\n",
      "[56]\ttrain-error:0.240242+0.00460293\ttest-error:0.338498+0.0074071\n",
      "[57]\ttrain-error:0.238652+0.00469154\ttest-error:0.337803+0.00836026\n",
      "[58]\ttrain-error:0.23751+0.00454713\ttest-error:0.337704+0.00754914\n",
      "[59]\ttrain-error:0.236095+0.0044947\ttest-error:0.33681+0.00741902\n",
      "[60]\ttrain-error:0.23525+0.00352941\ttest-error:0.336114+0.00754826\n",
      "[61]\ttrain-error:0.23386+0.00279141\ttest-error:0.336412+0.00905723\n",
      "[62]\ttrain-error:0.233289+0.00298526\ttest-error:0.33661+0.00884521\n",
      "[63]\ttrain-error:0.232295+0.00306634\ttest-error:0.336015+0.00931727\n",
      "[64]\ttrain-error:0.230781+0.0035497\ttest-error:0.335021+0.00941616\n",
      "[65]\ttrain-error:0.230036+0.00320099\ttest-error:0.336412+0.0097749\n",
      "[66]\ttrain-error:0.229738+0.00328663\ttest-error:0.338398+0.00813297\n",
      "[67]\ttrain-error:0.228844+0.00333754\ttest-error:0.337206+0.00801941\n",
      "[68]\ttrain-error:0.227851+0.00328741\ttest-error:0.337504+0.00795242\n",
      "[69]\ttrain-error:0.226361+0.00354665\ttest-error:0.337405+0.00725733\n",
      "[70]\ttrain-error:0.224449+0.00352714\ttest-error:0.337902+0.00720526\n",
      "[71]\ttrain-error:0.223108+0.00296611\ttest-error:0.337405+0.00757216\n",
      "[72]\ttrain-error:0.222189+0.0026887\ttest-error:0.336213+0.00827574\n",
      "[73]\ttrain-error:0.221295+0.00341219\ttest-error:0.335816+0.00836209\n",
      "[74]\ttrain-error:0.219507+0.00253188\ttest-error:0.336015+0.00845396\n",
      "[75]\ttrain-error:0.218092+0.0022547\ttest-error:0.337206+0.00913661\n",
      "[76]\ttrain-error:0.216776+0.00190793\ttest-error:0.337008+0.00960604\n",
      "[77]\ttrain-error:0.216205+0.00205972\ttest-error:0.336015+0.0086987\n",
      "[78]\ttrain-error:0.215336+0.00189759\ttest-error:0.336909+0.00792627\n",
      "[79]\ttrain-error:0.213672+0.00195361\ttest-error:0.337703+0.00867889\n",
      "[80]\ttrain-error:0.212604+0.00220927\ttest-error:0.337704+0.00869401\n",
      "[81]\ttrain-error:0.21181+0.0018119\ttest-error:0.336809+0.00989962\n",
      "[82]\ttrain-error:0.21027+0.00235148\ttest-error:0.338498+0.0095703\n",
      "[83]\ttrain-error:0.209624+0.00213783\ttest-error:0.339491+0.00846752\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.7195\n",
      "AUC Score (Train): 0.795083\n"
     ]
    }
   ],
   "source": [
    "# retuning estimators WITH NEW ESTIMATORS\n",
    "xgb2 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=4, min_child_weight=2,\n",
    "                     gamma=0.3, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n",
    "                     nthread=4, scale_pos_weight=1,seed=27)\n",
    "modelfit(xgb2, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  5.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  5.4min finished\n",
      "/Users/bennettcohen/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.8, 'subsample': 0.8}, 0.668652961312541)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuning col and subsample \n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(7,9)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(7,9)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=83, max_depth=4, \n",
    "                                                  min_child_weight=2, gamma=0.3, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test4, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch4.fit(train[predictors],train[target])\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# param_test6 = {\n",
    "#  'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "# }\n",
    "# gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=75, max_depth=3,\n",
    "#  min_child_weight=3, gamma=0.4, subsample=0.8, colsample_bytree=0.7,\n",
    "#  objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "#  param_grid = param_test6, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "# gsearch6.fit(train[predictors],train[target])\n",
    "# gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.336735+0.00157148\ttest-error:0.36025+0.00400966\n",
      "[1]\ttrain-error:0.325536+0.00536517\ttest-error:0.353199+0.0073676\n",
      "[2]\ttrain-error:0.318857+0.00388242\ttest-error:0.347339+0.00692479\n",
      "[3]\ttrain-error:0.315529+0.00365669\ttest-error:0.344756+0.00685535\n",
      "[4]\ttrain-error:0.313096+0.00292453\ttest-error:0.342471+0.00639698\n",
      "[5]\ttrain-error:0.309197+0.00246807\ttest-error:0.346146+0.00894755\n",
      "[6]\ttrain-error:0.30736+0.00318077\ttest-error:0.342968+0.00623605\n",
      "[7]\ttrain-error:0.306441+0.00224127\ttest-error:0.341081+0.00837338\n",
      "[8]\ttrain-error:0.304455+0.00440916\ttest-error:0.341876+0.0086247\n",
      "[9]\ttrain-error:0.30222+0.00427024\ttest-error:0.341279+0.00951443\n",
      "[10]\ttrain-error:0.30068+0.00418933\ttest-error:0.34128+0.0093351\n",
      "[11]\ttrain-error:0.299563+0.00388487\ttest-error:0.340286+0.00892278\n",
      "[12]\ttrain-error:0.297999+0.00435535\ttest-error:0.342272+0.00647406\n",
      "[13]\ttrain-error:0.29775+0.00415948\ttest-error:0.343266+0.00777052\n",
      "[14]\ttrain-error:0.295118+0.00458088\ttest-error:0.338796+0.00891153\n",
      "[15]\ttrain-error:0.293802+0.00462426\ttest-error:0.339392+0.00778677\n",
      "[16]\ttrain-error:0.291741+0.0043459\ttest-error:0.337903+0.0041083\n",
      "[17]\ttrain-error:0.289804+0.00424712\ttest-error:0.340783+0.00492907\n",
      "[18]\ttrain-error:0.289084+0.00474812\ttest-error:0.339193+0.0068566\n",
      "[19]\ttrain-error:0.287942+0.0045635\ttest-error:0.338895+0.00733806\n",
      "[20]\ttrain-error:0.285707+0.00438023\ttest-error:0.337505+0.00700509\n",
      "[21]\ttrain-error:0.284366+0.00439336\ttest-error:0.337902+0.00547588\n",
      "[22]\ttrain-error:0.283199+0.0039386\ttest-error:0.338498+0.00487094\n",
      "[23]\ttrain-error:0.282256+0.00317574\ttest-error:0.339094+0.00534063\n",
      "[24]\ttrain-error:0.281039+0.00388455\ttest-error:0.339988+0.00679761\n",
      "[25]\ttrain-error:0.2795+0.00423913\ttest-error:0.337803+0.00593896\n",
      "[26]\ttrain-error:0.277637+0.00440285\ttest-error:0.3381+0.00669058\n",
      "[27]\ttrain-error:0.276768+0.00485607\ttest-error:0.338001+0.00702146\n",
      "[28]\ttrain-error:0.275253+0.00507696\ttest-error:0.337306+0.00844981\n",
      "[29]\ttrain-error:0.273689+0.00526897\ttest-error:0.337505+0.00689852\n",
      "[30]\ttrain-error:0.272323+0.00440949\ttest-error:0.337008+0.00580695\n",
      "[31]\ttrain-error:0.270982+0.00409338\ttest-error:0.337405+0.00488548\n",
      "[32]\ttrain-error:0.269815+0.00358437\ttest-error:0.337704+0.0058366\n",
      "[33]\ttrain-error:0.268276+0.00377293\ttest-error:0.336114+0.00587996\n",
      "[34]\ttrain-error:0.26763+0.00353155\ttest-error:0.335618+0.00546874\n",
      "[35]\ttrain-error:0.265718+0.00370735\ttest-error:0.336809+0.00672798\n",
      "[36]\ttrain-error:0.264303+0.00398035\ttest-error:0.336412+0.00648715\n",
      "[37]\ttrain-error:0.262937+0.00346419\ttest-error:0.337306+0.0073648\n",
      "[38]\ttrain-error:0.260926+0.003376\ttest-error:0.337306+0.00757191\n",
      "[39]\ttrain-error:0.259759+0.00336205\ttest-error:0.336809+0.00725247\n",
      "[40]\ttrain-error:0.258666+0.00358289\ttest-error:0.33671+0.00767161\n",
      "[41]\ttrain-error:0.2574+0.00344947\ttest-error:0.336014+0.0077157\n",
      "[42]\ttrain-error:0.255811+0.00355509\ttest-error:0.337504+0.00752155\n",
      "[43]\ttrain-error:0.254867+0.00354672\ttest-error:0.336213+0.00710419\n",
      "[44]\ttrain-error:0.253824+0.00368007\ttest-error:0.335915+0.00675041\n",
      "[45]\ttrain-error:0.253278+0.00362914\ttest-error:0.337206+0.00657512\n",
      "[46]\ttrain-error:0.252036+0.00376372\ttest-error:0.336412+0.00837813\n",
      "[47]\ttrain-error:0.250224+0.00387025\ttest-error:0.336412+0.00798422\n",
      "[48]\ttrain-error:0.250124+0.00263822\ttest-error:0.336611+0.00859219\n",
      "[49]\ttrain-error:0.248138+0.003048\ttest-error:0.336412+0.00834056\n",
      "[50]\ttrain-error:0.24625+0.00274608\ttest-error:0.336412+0.00850449\n",
      "[51]\ttrain-error:0.245208+0.00312863\ttest-error:0.336312+0.00852361\n",
      "[52]\ttrain-error:0.244289+0.00371881\ttest-error:0.33671+0.00871881\n",
      "[53]\ttrain-error:0.242079+0.00426279\ttest-error:0.335518+0.00799615\n",
      "[54]\ttrain-error:0.241756+0.00434694\ttest-error:0.336511+0.00786109\n",
      "[55]\ttrain-error:0.24049+0.00309121\ttest-error:0.337207+0.00917769\n",
      "[56]\ttrain-error:0.239149+0.00389709\ttest-error:0.336412+0.00837077\n",
      "[57]\ttrain-error:0.237709+0.00436672\ttest-error:0.335518+0.00795335\n",
      "[58]\ttrain-error:0.236417+0.00441181\ttest-error:0.334525+0.00815961\n",
      "[59]\ttrain-error:0.23535+0.00444993\ttest-error:0.334327+0.00581675\n",
      "[60]\ttrain-error:0.235076+0.00394896\ttest-error:0.335121+0.00620424\n",
      "[61]\ttrain-error:0.234555+0.00387123\ttest-error:0.335617+0.00645599\n",
      "[62]\ttrain-error:0.233537+0.00448481\ttest-error:0.336213+0.00695474\n",
      "[63]\ttrain-error:0.232271+0.00420341\ttest-error:0.336909+0.00769982\n",
      "[64]\ttrain-error:0.231004+0.00379644\ttest-error:0.337405+0.00774863\n",
      "[65]\ttrain-error:0.230036+0.00342204\ttest-error:0.337207+0.00818805\n",
      "[66]\ttrain-error:0.229614+0.0028361\ttest-error:0.338398+0.00975801\n",
      "[67]\ttrain-error:0.228869+0.0029479\ttest-error:0.337604+0.00899264\n",
      "[68]\ttrain-error:0.227603+0.00353092\ttest-error:0.337405+0.00945676\n",
      "[69]\ttrain-error:0.225964+0.00320546\ttest-error:0.336611+0.00961447\n",
      "[70]\ttrain-error:0.223878+0.00366847\ttest-error:0.335518+0.00867855\n",
      "[71]\ttrain-error:0.223009+0.00365687\ttest-error:0.335916+0.00791189\n",
      "[72]\ttrain-error:0.22214+0.00345012\ttest-error:0.335916+0.00762203\n",
      "[73]\ttrain-error:0.220699+0.00347036\ttest-error:0.336412+0.00760692\n",
      "[74]\ttrain-error:0.219632+0.00327296\ttest-error:0.337604+0.00718382\n",
      "[75]\ttrain-error:0.219011+0.00346173\ttest-error:0.337405+0.00815742\n",
      "[76]\ttrain-error:0.21767+0.00297939\ttest-error:0.336908+0.0077863\n",
      "[77]\ttrain-error:0.21618+0.00257796\ttest-error:0.337802+0.0086101\n",
      "[78]\ttrain-error:0.21541+0.00286346\ttest-error:0.336809+0.00914009\n",
      "[79]\ttrain-error:0.213623+0.00266236\ttest-error:0.338001+0.00924681\n",
      "[80]\ttrain-error:0.212083+0.00165862\ttest-error:0.336809+0.0101622\n",
      "[81]\ttrain-error:0.211785+0.0015362\ttest-error:0.33661+0.00994202\n",
      "[82]\ttrain-error:0.210692+0.0019438\ttest-error:0.337008+0.00985729\n",
      "[83]\ttrain-error:0.209848+0.00227056\ttest-error:0.3381+0.00934898\n",
      "[84]\ttrain-error:0.208532+0.00279283\ttest-error:0.338299+0.0087742\n",
      "[85]\ttrain-error:0.207489+0.00286356\ttest-error:0.337107+0.00980079\n",
      "[86]\ttrain-error:0.206198+0.00242961\ttest-error:0.335617+0.0102425\n",
      "[87]\ttrain-error:0.205478+0.00325669\ttest-error:0.336313+0.0105167\n",
      "[88]\ttrain-error:0.204608+0.00389814\ttest-error:0.335419+0.011616\n",
      "[89]\ttrain-error:0.204112+0.0040106\ttest-error:0.335717+0.0113947\n",
      "[90]\ttrain-error:0.203169+0.00433353\ttest-error:0.334823+0.0112966\n",
      "[91]\ttrain-error:0.201778+0.00417923\ttest-error:0.335618+0.0109558\n",
      "[92]\ttrain-error:0.200685+0.00485956\ttest-error:0.335816+0.0106343\n",
      "[93]\ttrain-error:0.199022+0.00475216\ttest-error:0.33661+0.01088\n",
      "[94]\ttrain-error:0.197358+0.0039093\ttest-error:0.337008+0.00988425\n",
      "[95]\ttrain-error:0.196588+0.00410318\ttest-error:0.337306+0.0104926\n",
      "[96]\ttrain-error:0.195843+0.00389978\ttest-error:0.337802+0.0109577\n",
      "[97]\ttrain-error:0.194428+0.00377256\ttest-error:0.338299+0.0105125\n",
      "[98]\ttrain-error:0.193807+0.0044742\ttest-error:0.338696+0.00816177\n",
      "[99]\ttrain-error:0.192094+0.00400108\ttest-error:0.338299+0.0087393\n",
      "[100]\ttrain-error:0.19115+0.00450846\ttest-error:0.338299+0.0086833\n",
      "[101]\ttrain-error:0.190058+0.00461095\ttest-error:0.338994+0.00867125\n",
      "[102]\ttrain-error:0.188245+0.00495332\ttest-error:0.336809+0.00880141\n",
      "[103]\ttrain-error:0.187028+0.00527053\ttest-error:0.337008+0.00925458\n",
      "[104]\ttrain-error:0.186234+0.00520269\ttest-error:0.337207+0.00946103\n",
      "[105]\ttrain-error:0.185439+0.00440643\ttest-error:0.337108+0.00949783\n",
      "[106]\ttrain-error:0.184595+0.00447072\ttest-error:0.336512+0.00948742\n",
      "[107]\ttrain-error:0.183726+0.00448859\ttest-error:0.336909+0.0107428\n",
      "[108]\ttrain-error:0.182757+0.0051561\ttest-error:0.337008+0.0107709\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.7479\n",
      "AUC Score (Train): 0.827780\n"
     ]
    }
   ],
   "source": [
    "xgb3 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=2,\n",
    " gamma=0.3,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.01,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb3, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2192,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# xgb4 = XGBClassifier(\n",
    "#  learning_rate =0.01,\n",
    "#  n_estimators=5000,\n",
    "#  max_depth=3,\n",
    "#  min_child_weight=3,\n",
    "#  gamma=0.4,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.7,\n",
    "#  reg_alpha=0.01,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  seed=27)\n",
    "# modelfit(xgb4, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 467  574]\n",
      " [ 254 1222]]\n",
      "\n",
      "Accuracy :  0.671\n"
     ]
    }
   ],
   "source": [
    "xgb5 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=108,\n",
    " max_depth=4,\n",
    " min_child_weight=2,\n",
    " gamma=0.3,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.01,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "#modelfit(xgb5, train, predictors, useTrainCV = False)\n",
    "\n",
    "\n",
    "# XgBoost\n",
    "\n",
    "model_3 = xgb5\n",
    "model_3.fit(X_train, y_train)\n",
    "y_prob_3 = model_3.predict_proba(X_test)\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "outputs_3 = outputs(cm_3)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
