{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Importing Libraries & Functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load in our GAMES and TEAMS datasets and perform some simple cleaning to make them more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Loading + Cleaning Data \n",
    "games = pd.read_csv('FP1_DATA/games.csv')\n",
    "timestamp = pd.to_datetime(games['GAME_DATE_EST'])\n",
    "games.insert(0, 'TIMESTAMP', timestamp)\n",
    "season_list = np.arange(2007,2019)\n",
    "games = games[games['SEASON'].isin(season_list)].sort_values('TIMESTAMP')\n",
    "\n",
    "teams = pd.read_csv('FP1_DATA/teams.csv')\n",
    "teams = teams[['TEAM_ID','NICKNAME']]\n",
    "team_dict = teams.set_index('TEAM_ID').T.to_dict('list')\n",
    "games['HOME_TEAM_NAME'] = games['HOME_TEAM_ID'].map(team_dict)\n",
    "games['AWAY_TEAM_NAME'] = games['VISITOR_TEAM_ID'].map(team_dict)\n",
    "\n",
    "\n",
    "drop_cols = ['GAME_DATE_EST', 'GAME_STATUS_TEXT', 'HOME_TEAM_ID', \n",
    "             'VISITOR_TEAM_ID', 'TEAM_ID_home', 'TEAM_ID_away']\n",
    "games = games.drop(drop_cols, axis = 1).reset_index(drop = True)\n",
    "front_cols = ['TIMESTAMP', 'GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']\n",
    "temp_games = games[front_cols]\n",
    "games = pd.concat([temp_games, games.drop(front_cols, axis = 1)], axis = 1)\n",
    "games['HOME_TEAM_NAME'] = games['HOME_TEAM_NAME'].apply(lambda x: x[0])\n",
    "games['AWAY_TEAM_NAME'] = games['AWAY_TEAM_NAME'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player-Position Statistics\n",
    "\n",
    "Because we want to consider player/position contributions to the overall statistics, we use the DETAILS dataframe to convert players to positions, and then calculate the statistics in each game, grouped by each position, Guards, Forwards, and Center. We also then calculate Player Efficiency Rating (PER) using these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Defining the Position Stat Function\n",
    "def pos_stat(stat_list):\n",
    "    \n",
    "    \n",
    "    ### Takes in a list of statistics to consider from the DETAILS data frame and calculates\n",
    "    ### the stats by position in each home for the home and away team.\n",
    "    ### NOTE: THESE ARE NOT CUMULATIVE STATS!!!!!\n",
    "    \n",
    "\n",
    "    # Removing non-starters, getting rid of unnecessary columns\n",
    "    details = pd.read_csv('FP1_DATA/games_details.csv')\n",
    "    season_dict = games[['GAME_ID', 'SEASON']].set_index('GAME_ID').iloc[:,0].T.to_dict()\n",
    "    details['SEASON'] = details['GAME_ID'].map(season_dict)\n",
    "    details = details[details.notna()]\n",
    "    details = details[~details['START_POSITION'].isna()]\n",
    "    details['TEAM_NAME'] = details['TEAM_ID'].map(team_dict).apply(lambda x: x[0])\n",
    "    details['MINS'] = details['MIN'].str.split(':').apply(lambda x: float(x[0]) + (float(x[1])/60))\n",
    "    details = details.drop(['TEAM_ABBREVIATION', 'TEAM_CITY', 'COMMENT', 'MIN'], axis = 1)\n",
    "    temp_cols = ['GAME_ID','START_POSITION', 'TEAM_NAME']\n",
    "    final_cols = np.append(temp_cols, stat_list).flatten()\n",
    "    details = details[final_cols]\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    for stat in stat_list: \n",
    "        \n",
    "        \n",
    "        # Groupby the GAME, POSITION and TEAM and add up all stats by each position \n",
    "        df = details.copy()\n",
    "        df = df.groupby(['GAME_ID', 'START_POSITION', 'TEAM_NAME']).sum().reset_index()\n",
    "        df = pd.merge(df, games[['GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']], on = 'GAME_ID')\n",
    "\n",
    "        # Need to Re-Order the DF s.t. home and away are known \n",
    "        home_condition = (df['TEAM_NAME'] == df['HOME_TEAM_NAME'])\n",
    "        away_condition = (df['TEAM_NAME'] != df['HOME_TEAM_NAME'])\n",
    "\n",
    "        # Adding HOME Columns Manually \n",
    "        df['C_' + str(stat) + '_home'] = df[ home_condition & (df['START_POSITION'] == 'C')][stat]\n",
    "        df['F_' + str(stat) + '_home'] = df[ home_condition & (df['START_POSITION'] == 'F')][stat]\n",
    "        df['G_' + str(stat) + '_home'] = df[ home_condition & (df['START_POSITION'] == 'G')][stat]\n",
    "\n",
    "        # Adding AWAY Columns Manually \n",
    "        df['C_' + str(stat) + '_away'] = df[ away_condition & (df['START_POSITION'] == 'C')][stat]\n",
    "        df['F_' + str(stat) + '_away'] = df[ away_condition & (df['START_POSITION'] == 'F')][stat]\n",
    "        df['G_' + str(stat) + '_away'] = df[ away_condition & (df['START_POSITION'] == 'G')][stat]\n",
    "\n",
    "        # Grouping Again to Get Rid of NaN \n",
    "        df = df.groupby('GAME_ID', as_index = False).sum()\n",
    "        if stat_list.index(stat) > 0: \n",
    "            df = df.drop('GAME_ID', axis = 1)\n",
    "        # Append to final data frame \n",
    "        output_df = pd.concat([output_df, df], axis = 1)\n",
    "\n",
    "    output_df = output_df.drop(stat_list, axis = 1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Adding Columns for Position Stats using pos_stat Function\n",
    "stat_list = ['FGM', 'FGA','FG_PCT', 'FG3M', 'FG3A', \n",
    "             'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB',\n",
    "             'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', \n",
    "             'PF', 'PTS', 'PLUS_MINUS']\n",
    "pos_df = pos_stat(stat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Calculing Player Efficiency Rating\n",
    "# Formula: (PTS + REB + AST + STL + BLK − Missed FG − Missed FT - TO) / GP\n",
    "p1 = pos_df['C_PTS_home'] + pos_df['C_REB_home'] + pos_df['C_AST_home'] + pos_df['C_STL_home']\n",
    "missed_fg = (pos_df['C_FGA_home'] - pos_df['C_FGM_home'])\n",
    "missed_ft = pos_df['C_FTA_home'] - pos_df['C_FTM_home']\n",
    "p2 = (missed_fg + missed_ft + pos_df['C_TO_home'])\n",
    "per = (p1 - p2)/82\n",
    "\n",
    "for pos in ['C', 'F', 'G']: \n",
    "    p1 = pos_df[pos + '_PTS_home'] + pos_df[pos + '_REB_home'] + pos_df[pos + '_AST_home'] + pos_df[pos + '_STL_home']\n",
    "    missed_fg = (pos_df[pos + '_FGA_home'] - pos_df[pos + '_FGM_home'])\n",
    "    missed_ft = pos_df[pos + '_FTA_home'] - pos_df[pos + '_FTM_home']\n",
    "    p2 = (missed_fg + missed_ft + pos_df[pos + '_TO_home'])\n",
    "    per = (p1-p2)/82\n",
    "    pos_df[pos + '_PER_home'] = per\n",
    "    \n",
    "for pos in ['C', 'F', 'G']: \n",
    "    p1 = pos_df[pos + '_PTS_away'] + pos_df[pos + '_REB_away'] + pos_df[pos + '_AST_away'] + pos_df[pos + '_STL_away']\n",
    "    missed_fg = (pos_df[pos + '_FGA_away'] - pos_df[pos + '_FGM_away'])\n",
    "    missed_ft = pos_df[pos + '_FTA_away'] - pos_df[pos + '_FTM_away']\n",
    "    p2 = (missed_fg + missed_ft + pos_df[pos + '_TO_away'])\n",
    "    per = (p1-p2)/82\n",
    "    pos_df[pos + '_PER_away'] = per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Cumulative Statistics\n",
    "\n",
    "\n",
    "The bulk of our cumulative sum and average calculations are shown below. We first define functions to calculate the cumulative average statistics for the (1) Season, (2) Last 5 Games, and (3) Last 10 Games. Because a team's win or loss count is calculated as a sum, we also create functions to calculate those sums for the Season, Last 5 and Last 10 Games based off of the 'HOME_TEAM_WINS' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0,
     1,
     5,
     14,
     23,
     27,
     36,
     45,
     49,
     58
    ]
   },
   "outputs": [],
   "source": [
    "# Defining the Cumulative Stat Functions\n",
    "def cum_avg(arr): \n",
    "    temp_list = [np.mean(arr[:i]) for i in np.arange(len(arr)+1)][1:]\n",
    "    return np.append(np.nan, temp_list)[:-1]\n",
    "\n",
    "def cum_5_avg(arr): \n",
    "    means = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 5: \n",
    "            means = np.append(means, np.nan)\n",
    "        else: \n",
    "            means = np.append(means, np.mean(arr[i-5: i+1]))\n",
    "    return means\n",
    "\n",
    "def cum_10_avg(arr): \n",
    "    means = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 10: \n",
    "            means = np.append(means, np.nan)\n",
    "        else: \n",
    "            means = np.append(means, np.mean(arr[i-10: i+1]))\n",
    "    return means\n",
    "\n",
    "def cum_wins(arr): \n",
    "    temp_list = [np.sum(arr[:i]) for i in np.arange(len(arr)+1)][1:]\n",
    "    return np.append(np.nan, temp_list)[:-1]\n",
    "\n",
    "def cum_5_wins(arr): \n",
    "    wins = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 5: \n",
    "            wins = np.append(wins, np.nan)\n",
    "        else: \n",
    "            wins = np.append(wins, np.sum(arr[i-5: i+1]))\n",
    "    return wins\n",
    "\n",
    "def cum_10_wins(arr): \n",
    "    wins = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 10: \n",
    "            wins = np.append(wins, np.nan)\n",
    "        else: \n",
    "            wins = np.append(wins, np.sum(arr[i-10: i+1]))\n",
    "    return wins\n",
    "\n",
    "def cum_losses(arr): \n",
    "    temp_list = [np.count_nonzero(arr[:i]==0) for i in np.arange(len(arr)+1)][1:]\n",
    "    return np.append(np.nan, temp_list)[:-1]\n",
    "\n",
    "def cum_5_losses(arr): \n",
    "    losses = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 5: \n",
    "            losses = np.append(losses, np.nan)\n",
    "        else: \n",
    "            losses = np.append(losses, np.count_nonzero(arr[i-5: i+1]==0))\n",
    "    return losses\n",
    "\n",
    "def cum_10_losses(arr): \n",
    "    losses = []\n",
    "    for i in np.arange(len(arr)): \n",
    "        if i < 10: \n",
    "            losses = np.append(losses, np.nan)\n",
    "        else: \n",
    "            losses = np.append(losses, np.count_nonzero(arr[i-10: i+1]==0))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells do the same thing but for the HOME and AWAY statistics, respectively. For each HOME and AWAY stat, we do the following: \n",
    "\n",
    "(1) Calculate the Cumulative Season Average\n",
    "\n",
    "(2) Calculate the Cumulative Last 5 Game Average\n",
    "\n",
    "(3) Calculate the Cumulative Last 10 Game Average\n",
    "\n",
    "(4) Calculate the Cumulative Season Average for this specific combination of HOME and AWAY TEAM (Head-to-Head)\n",
    "\n",
    "(5) Calculate the Cumulative Last 5 Game Average for this specific combination of HOME and AWAY TEAM (Head-to-Head)\n",
    "\n",
    "(6) Calculate the Cumulative Last 10 Game Average for this specific combination of HOME and AWAY TEAM (Head-to-Head)\n",
    "\n",
    "\n",
    "We had to go about this in a rather unorthodox way using two grouped dataframes because using a nested for loop increased the run time by over 50x. We then combine these two dataframes together, using 'GAME_ID' to join. This turns our initial dataframe of 132 columns to 792 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 of 66 COMPLETE:  PTS_home -- TIME:  4.732546091079712\n",
      "# 2 of 66 COMPLETE:  FG_PCT_home -- TIME:  9.695076942443848\n",
      "# 3 of 66 COMPLETE:  FT_PCT_home -- TIME:  14.761112928390503\n",
      "# 4 of 66 COMPLETE:  FG3_PCT_home -- TIME:  19.93681502342224\n",
      "# 5 of 66 COMPLETE:  AST_home -- TIME:  25.567740201950073\n",
      "# 6 of 66 COMPLETE:  REB_home -- TIME:  30.941532135009766\n",
      "# 7 of 66 COMPLETE:  C_FGM_home -- TIME:  36.46212697029114\n",
      "# 8 of 66 COMPLETE:  F_FGM_home -- TIME:  42.01122498512268\n",
      "# 9 of 66 COMPLETE:  G_FGM_home -- TIME:  47.68910884857178\n",
      "# 10 of 66 COMPLETE:  C_FGA_home -- TIME:  53.532379150390625\n",
      "# 11 of 66 COMPLETE:  F_FGA_home -- TIME:  59.75461792945862\n",
      "# 12 of 66 COMPLETE:  G_FGA_home -- TIME:  65.97572493553162\n",
      "# 13 of 66 COMPLETE:  C_FG_PCT_home -- TIME:  72.31214499473572\n",
      "# 14 of 66 COMPLETE:  F_FG_PCT_home -- TIME:  78.80450510978699\n",
      "# 15 of 66 COMPLETE:  G_FG_PCT_home -- TIME:  85.44951796531677\n",
      "# 16 of 66 COMPLETE:  C_FG3M_home -- TIME:  92.15903091430664\n",
      "# 17 of 66 COMPLETE:  F_FG3M_home -- TIME:  99.03179001808167\n",
      "# 18 of 66 COMPLETE:  G_FG3M_home -- TIME:  106.04615688323975\n",
      "# 19 of 66 COMPLETE:  C_FG3A_home -- TIME:  113.1100070476532\n",
      "# 20 of 66 COMPLETE:  F_FG3A_home -- TIME:  120.63993501663208\n",
      "# 21 of 66 COMPLETE:  G_FG3A_home -- TIME:  128.50653100013733\n",
      "# 22 of 66 COMPLETE:  C_FG3_PCT_home -- TIME:  136.20415687561035\n",
      "# 23 of 66 COMPLETE:  F_FG3_PCT_home -- TIME:  143.93483114242554\n",
      "# 24 of 66 COMPLETE:  G_FG3_PCT_home -- TIME:  151.8971450328827\n",
      "# 25 of 66 COMPLETE:  C_FTM_home -- TIME:  159.69006609916687\n",
      "# 26 of 66 COMPLETE:  F_FTM_home -- TIME:  167.07637405395508\n",
      "# 27 of 66 COMPLETE:  G_FTM_home -- TIME:  174.62059688568115\n",
      "# 28 of 66 COMPLETE:  C_FTA_home -- TIME:  182.10178399085999\n",
      "# 29 of 66 COMPLETE:  F_FTA_home -- TIME:  189.88045191764832\n",
      "# 30 of 66 COMPLETE:  G_FTA_home -- TIME:  197.57915687561035\n",
      "# 31 of 66 COMPLETE:  C_FT_PCT_home -- TIME:  205.346195936203\n",
      "# 32 of 66 COMPLETE:  F_FT_PCT_home -- TIME:  213.21816396713257\n",
      "# 33 of 66 COMPLETE:  G_FT_PCT_home -- TIME:  221.12918281555176\n",
      "# 34 of 66 COMPLETE:  C_OREB_home -- TIME:  229.35324597358704\n",
      "# 35 of 66 COMPLETE:  F_OREB_home -- TIME:  237.59066581726074\n",
      "# 36 of 66 COMPLETE:  G_OREB_home -- TIME:  245.87727618217468\n",
      "# 37 of 66 COMPLETE:  C_DREB_home -- TIME:  254.53458881378174\n",
      "# 38 of 66 COMPLETE:  F_DREB_home -- TIME:  263.1116750240326\n",
      "# 39 of 66 COMPLETE:  G_DREB_home -- TIME:  272.0000171661377\n",
      "# 40 of 66 COMPLETE:  C_REB_home -- TIME:  280.865033864975\n",
      "# 41 of 66 COMPLETE:  F_REB_home -- TIME:  289.7501118183136\n",
      "# 42 of 66 COMPLETE:  G_REB_home -- TIME:  299.03335189819336\n",
      "# 43 of 66 COMPLETE:  C_AST_home -- TIME:  308.22829699516296\n",
      "# 44 of 66 COMPLETE:  F_AST_home -- TIME:  317.475172996521\n",
      "# 45 of 66 COMPLETE:  G_AST_home -- TIME:  326.8412981033325\n",
      "# 46 of 66 COMPLETE:  C_STL_home -- TIME:  336.6581370830536\n",
      "# 47 of 66 COMPLETE:  F_STL_home -- TIME:  346.2609899044037\n",
      "# 48 of 66 COMPLETE:  G_STL_home -- TIME:  355.999685049057\n",
      "# 49 of 66 COMPLETE:  C_BLK_home -- TIME:  366.0432939529419\n",
      "# 50 of 66 COMPLETE:  F_BLK_home -- TIME:  376.02791690826416\n",
      "# 51 of 66 COMPLETE:  G_BLK_home -- TIME:  386.0702030658722\n",
      "# 52 of 66 COMPLETE:  C_TO_home -- TIME:  396.69300293922424\n",
      "# 53 of 66 COMPLETE:  F_TO_home -- TIME:  407.72757291793823\n",
      "# 54 of 66 COMPLETE:  G_TO_home -- TIME:  418.47975397109985\n",
      "# 55 of 66 COMPLETE:  C_PF_home -- TIME:  429.37751603126526\n",
      "# 56 of 66 COMPLETE:  F_PF_home -- TIME:  440.09311509132385\n",
      "# 57 of 66 COMPLETE:  G_PF_home -- TIME:  450.8636300563812\n",
      "# 58 of 66 COMPLETE:  C_PTS_home -- TIME:  462.70912289619446\n",
      "# 59 of 66 COMPLETE:  F_PTS_home -- TIME:  474.4053599834442\n",
      "# 60 of 66 COMPLETE:  G_PTS_home -- TIME:  487.0005979537964\n",
      "# 61 of 66 COMPLETE:  C_PLUS_MINUS_home -- TIME:  499.56221413612366\n",
      "# 62 of 66 COMPLETE:  F_PLUS_MINUS_home -- TIME:  512.1199169158936\n",
      "# 63 of 66 COMPLETE:  G_PLUS_MINUS_home -- TIME:  525.0752880573273\n",
      "# 64 of 66 COMPLETE:  C_PER_home -- TIME:  537.8398959636688\n",
      "# 65 of 66 COMPLETE:  F_PER_home -- TIME:  551.0608150959015\n",
      "# 66 of 66 COMPLETE:  G_PER_home -- TIME:  564.3962941169739\n"
     ]
    }
   ],
   "source": [
    "# Cumulative Statistics: HOME\n",
    "df = games.copy()\n",
    "df = df.merge(pos_df, on = 'GAME_ID')\n",
    "df['TEAM_LIST'] = df['HOME_TEAM_NAME'] + ', ' + df['AWAY_TEAM_NAME']\n",
    "df_home = df.drop('AWAY_TEAM_NAME', axis = 1)\n",
    "df_home = df.sort_values('TIMESTAMP', ascending = True)\n",
    "column_list = df_home.columns[4:]\n",
    "home_stat_list = [stat for stat in column_list if 'home' in stat]\n",
    "\n",
    "t = time.time()\n",
    "for stat in home_stat_list: \n",
    "    \n",
    "    keep_cols = np.append(['TIMESTAMP', 'TEAM_LIST','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME'], stat)\n",
    "    df_home_temp = df_home[keep_cols]\n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1 = df_home_temp.groupby(['HOME_TEAM_NAME', 'GAME_ID']).mean()\n",
    "    df_2 = df_home_temp.groupby(['HOME_TEAM_NAME']).agg(list)\n",
    "\n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists = df_2.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists = df_2.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists = df_2.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals = cum_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals = cum_5_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals = cum_10_lists.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    df_1 = df_1.reset_index()\n",
    "    df_1['cum_' + str(stat)] = cum_vals\n",
    "    df_1['cum_5_' + str(stat)] = cum_5_vals\n",
    "    df_1['cum_10_' + str(stat)] = cum_10_vals\n",
    "    \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict = df_1.set_index('GAME_ID').iloc[:, -3].T.to_dict()\n",
    "    cum_dict_5 = df_1.set_index('GAME_ID').iloc[:, -2].T.to_dict()\n",
    "    cum_dict_10 = df_1.set_index('GAME_ID').iloc[:, -1].T.to_dict()\n",
    "\n",
    "    df_home['cum_' + str(stat)] = df['GAME_ID'].map(cum_dict)\n",
    "    df_home['cum_5_' + str(stat)] = df['GAME_ID'].map(cum_dict_5)\n",
    "    df_home['cum_10_' + str(stat)] = df['GAME_ID'].map(cum_dict_10)\n",
    "    \n",
    "    \n",
    "    # HEAD TO HEAD STATS ==> SAME THING BASICALLY \n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1_H2H = df_home.groupby(['TEAM_LIST', 'GAME_ID']).mean()\n",
    "    df_2_H2H = df_home.groupby('TEAM_LIST').agg(list)\n",
    "    \n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists_H2H = df_2_H2H.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists_H2H = df_2_H2H.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists_H2H = df_2_H2H.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals_H2H = cum_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    #df_1_H2H = df_1_H2H.reset_index()\n",
    "    df_1_H2H['cum_' + str(stat) + '_H2H'] = cum_vals_H2H\n",
    "    df_1_H2H['cum_5_' + str(stat) + '_H2H'] = cum_5_vals_H2H\n",
    "    df_1_H2H['cum_10_' + str(stat) + '_H2H'] = cum_10_vals_H2H\n",
    "    \n",
    "   \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -3].T.to_dict()\n",
    "    cum_5_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -2].T.to_dict()\n",
    "    cum_10_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -1].T.to_dict()\n",
    "    \n",
    "    df_home['cum_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_dict_H2H)\n",
    "    df_home['cum_5_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_5_dict_H2H)\n",
    "    df_home['cum_10_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_10_dict_H2H)\n",
    "  \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print('# ' + str(home_stat_list.index(stat)+1) + ' of ' + str(len(home_stat_list)) + ' COMPLETE: ', str(stat), '-- TIME: ', time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 of 66 COMPLETE:  PTS_away -- TIME:  4.816655158996582\n",
      "# 2 of 66 COMPLETE:  FG_PCT_away -- TIME:  9.50899600982666\n",
      "# 3 of 66 COMPLETE:  FT_PCT_away -- TIME:  14.484194040298462\n",
      "# 4 of 66 COMPLETE:  FG3_PCT_away -- TIME:  19.607362985610962\n",
      "# 5 of 66 COMPLETE:  AST_away -- TIME:  24.847387075424194\n",
      "# 6 of 66 COMPLETE:  REB_away -- TIME:  30.26409387588501\n",
      "# 7 of 66 COMPLETE:  C_FGM_away -- TIME:  35.743788957595825\n",
      "# 8 of 66 COMPLETE:  F_FGM_away -- TIME:  41.35866117477417\n",
      "# 9 of 66 COMPLETE:  G_FGM_away -- TIME:  47.139140129089355\n",
      "# 10 of 66 COMPLETE:  C_FGA_away -- TIME:  52.970470905303955\n",
      "# 11 of 66 COMPLETE:  F_FGA_away -- TIME:  58.956490993499756\n",
      "# 12 of 66 COMPLETE:  G_FGA_away -- TIME:  65.04403519630432\n",
      "# 13 of 66 COMPLETE:  C_FG_PCT_away -- TIME:  71.5125629901886\n",
      "# 14 of 66 COMPLETE:  F_FG_PCT_away -- TIME:  78.20596814155579\n",
      "# 15 of 66 COMPLETE:  G_FG_PCT_away -- TIME:  84.69293427467346\n",
      "# 16 of 66 COMPLETE:  C_FG3M_away -- TIME:  91.42034006118774\n",
      "# 17 of 66 COMPLETE:  F_FG3M_away -- TIME:  98.34503698348999\n",
      "# 18 of 66 COMPLETE:  G_FG3M_away -- TIME:  106.01250886917114\n",
      "# 19 of 66 COMPLETE:  C_FG3A_away -- TIME:  114.5474910736084\n",
      "# 20 of 66 COMPLETE:  F_FG3A_away -- TIME:  123.3448371887207\n",
      "# 21 of 66 COMPLETE:  G_FG3A_away -- TIME:  131.8079149723053\n",
      "# 22 of 66 COMPLETE:  C_FG3_PCT_away -- TIME:  140.44215083122253\n",
      "# 23 of 66 COMPLETE:  F_FG3_PCT_away -- TIME:  149.15179109573364\n",
      "# 24 of 66 COMPLETE:  G_FG3_PCT_away -- TIME:  158.22675395011902\n",
      "# 25 of 66 COMPLETE:  C_FTM_away -- TIME:  167.22784519195557\n",
      "# 26 of 66 COMPLETE:  F_FTM_away -- TIME:  176.34949803352356\n",
      "# 27 of 66 COMPLETE:  G_FTM_away -- TIME:  185.65676498413086\n",
      "# 28 of 66 COMPLETE:  C_FTA_away -- TIME:  194.9421420097351\n",
      "# 29 of 66 COMPLETE:  F_FTA_away -- TIME:  204.45041728019714\n",
      "# 30 of 66 COMPLETE:  G_FTA_away -- TIME:  214.49129915237427\n",
      "# 31 of 66 COMPLETE:  C_FT_PCT_away -- TIME:  224.46165084838867\n",
      "# 32 of 66 COMPLETE:  F_FT_PCT_away -- TIME:  234.3953721523285\n",
      "# 33 of 66 COMPLETE:  G_FT_PCT_away -- TIME:  244.97989106178284\n",
      "# 34 of 66 COMPLETE:  C_OREB_away -- TIME:  255.18730998039246\n",
      "# 35 of 66 COMPLETE:  F_OREB_away -- TIME:  264.47126817703247\n",
      "# 36 of 66 COMPLETE:  G_OREB_away -- TIME:  273.49217891693115\n",
      "# 37 of 66 COMPLETE:  C_DREB_away -- TIME:  282.9281189441681\n",
      "# 38 of 66 COMPLETE:  F_DREB_away -- TIME:  292.20760798454285\n",
      "# 39 of 66 COMPLETE:  G_DREB_away -- TIME:  301.4861340522766\n",
      "# 40 of 66 COMPLETE:  C_REB_away -- TIME:  311.6607029438019\n",
      "# 41 of 66 COMPLETE:  F_REB_away -- TIME:  321.65348196029663\n",
      "# 42 of 66 COMPLETE:  G_REB_away -- TIME:  332.44315910339355\n",
      "# 43 of 66 COMPLETE:  C_AST_away -- TIME:  342.8448979854584\n",
      "# 44 of 66 COMPLETE:  F_AST_away -- TIME:  352.99427604675293\n",
      "# 45 of 66 COMPLETE:  G_AST_away -- TIME:  363.7647511959076\n",
      "# 46 of 66 COMPLETE:  C_STL_away -- TIME:  374.22685408592224\n",
      "# 47 of 66 COMPLETE:  F_STL_away -- TIME:  385.01499009132385\n",
      "# 48 of 66 COMPLETE:  G_STL_away -- TIME:  395.6952350139618\n",
      "# 49 of 66 COMPLETE:  C_BLK_away -- TIME:  406.950119972229\n",
      "# 50 of 66 COMPLETE:  F_BLK_away -- TIME:  418.1836130619049\n",
      "# 51 of 66 COMPLETE:  G_BLK_away -- TIME:  430.0547981262207\n",
      "# 52 of 66 COMPLETE:  C_TO_away -- TIME:  442.0038981437683\n",
      "# 53 of 66 COMPLETE:  F_TO_away -- TIME:  454.32734513282776\n",
      "# 54 of 66 COMPLETE:  G_TO_away -- TIME:  466.3609290122986\n",
      "# 55 of 66 COMPLETE:  C_PF_away -- TIME:  478.45548486709595\n",
      "# 56 of 66 COMPLETE:  F_PF_away -- TIME:  490.4913680553436\n",
      "# 57 of 66 COMPLETE:  G_PF_away -- TIME:  502.55992698669434\n",
      "# 58 of 66 COMPLETE:  C_PTS_away -- TIME:  514.6256859302521\n",
      "# 59 of 66 COMPLETE:  F_PTS_away -- TIME:  526.9215860366821\n",
      "# 60 of 66 COMPLETE:  G_PTS_away -- TIME:  539.7647070884705\n",
      "# 61 of 66 COMPLETE:  C_PLUS_MINUS_away -- TIME:  553.0157041549683\n",
      "# 62 of 66 COMPLETE:  F_PLUS_MINUS_away -- TIME:  566.2632670402527\n",
      "# 63 of 66 COMPLETE:  G_PLUS_MINUS_away -- TIME:  579.8383350372314\n",
      "# 64 of 66 COMPLETE:  C_PER_away -- TIME:  593.600998878479\n",
      "# 65 of 66 COMPLETE:  F_PER_away -- TIME:  607.1849961280823\n",
      "# 66 of 66 COMPLETE:  G_PER_away -- TIME:  620.3899221420288\n"
     ]
    }
   ],
   "source": [
    "# Cumulative Statistics: AWAY\n",
    "df_away = df.drop('HOME_TEAM_NAME', axis = 1)\n",
    "df_away = df.sort_values('TIMESTAMP', ascending = True)\n",
    "column_list = df_away.columns[4:]\n",
    "away_stat_list = [stat for stat in column_list if 'away' in stat]\n",
    "\n",
    "t = time.time()\n",
    "for stat in away_stat_list: \n",
    "    \n",
    "    keep_cols = np.append(['TIMESTAMP', 'TEAM_LIST','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME'], stat)\n",
    "    df_away_temp = df_away[keep_cols]\n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1 = df_away_temp.groupby(['AWAY_TEAM_NAME', 'GAME_ID']).mean()\n",
    "    df_2 = df_away_temp.groupby(['AWAY_TEAM_NAME']).agg(list)\n",
    "\n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists = df_2.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists = df_2.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists = df_2.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals = cum_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals = cum_5_lists.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals = cum_10_lists.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    df_1 = df_1.reset_index()\n",
    "    df_1['cum_' + str(stat)] = cum_vals\n",
    "    df_1['cum_5_' + str(stat)] = cum_5_vals\n",
    "    df_1['cum_10_' + str(stat)] = cum_10_vals\n",
    "    \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict = df_1.set_index('GAME_ID').iloc[:, -3].T.to_dict()\n",
    "    cum_dict_5 = df_1.set_index('GAME_ID').iloc[:, -2].T.to_dict()\n",
    "    cum_dict_10 = df_1.set_index('GAME_ID').iloc[:, -1].T.to_dict()\n",
    "\n",
    "    df_away['cum_' + str(stat)] = df['GAME_ID'].map(cum_dict)\n",
    "    df_away['cum_5_' + str(stat)] = df['GAME_ID'].map(cum_dict_5)\n",
    "    df_away['cum_10_' + str(stat)] = df['GAME_ID'].map(cum_dict_10)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # HEAD TO HEAD STATS ==> SAME THING BASICALLY \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Two Grouped DFs\n",
    "    df_1_H2H = df_away.groupby(['TEAM_LIST', 'GAME_ID']).mean()\n",
    "    df_2_H2H = df_away.groupby('TEAM_LIST').agg(list)\n",
    "    \n",
    "    # Using Functions above to calulate the cumulative values as lists\n",
    "    cum_lists_H2H = df_2_H2H.apply(lambda x: cum_avg(x[stat]), axis = 1)\n",
    "    cum_5_lists_H2H = df_2_H2H.apply(lambda x: cum_5_avg(x[stat]), axis = 1)\n",
    "    cum_10_lists_H2H = df_2_H2H.apply(lambda x: cum_10_avg(x[stat]), axis = 1)\n",
    "\n",
    "    # Converting these nested lists to single lists\n",
    "    cum_vals_H2H = cum_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_5_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "    cum_10_vals_H2H = cum_5_lists_H2H.to_frame().explode(0)[0].to_numpy()\n",
    "\n",
    "    #df_1_H2H = df_1_H2H.reset_index()\n",
    "    df_1_H2H['cum_' + str(stat) + '_H2H'] = cum_vals_H2H\n",
    "    df_1_H2H['cum_5_' + str(stat) + '_H2H'] = cum_5_vals_H2H\n",
    "    df_1_H2H['cum_10_' + str(stat) + '_H2H'] = cum_10_vals_H2H\n",
    "    \n",
    "   \n",
    "    # Converting to Dictionaries and Adding to Large DF\n",
    "    cum_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -3].T.to_dict()\n",
    "    cum_5_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -2].T.to_dict()\n",
    "    cum_10_dict_H2H = df_1_H2H.droplevel(0).iloc[:, -1].T.to_dict()\n",
    "    \n",
    "    df_away['cum_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_dict_H2H)\n",
    "    df_away['cum_5_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_5_dict_H2H)\n",
    "    df_away['cum_10_' + str(stat) + '_H2H'] = df['GAME_ID'].map(cum_10_dict_H2H)\n",
    "    \n",
    "    \n",
    "    print('# ' + str(away_stat_list.index(stat)+1) + ' of ' + str(len(away_stat_list)) + ' COMPLETE: ', str(stat), '-- TIME: ', time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Combining the Home and Away DataFrames \n",
    "df_home_2 = df_home.copy()\n",
    "df_away_2 = df_away.copy()\n",
    "df_home_2 = df_home_2.drop(np.append(home_stat_list, away_stat_list,), axis = 1)\n",
    "df_away_2 = df_away_2.drop(np.append(home_stat_list, away_stat_list), axis = 1)\n",
    "\n",
    "df_clean = pd.merge(df_home_2, df_away_2, on = 'GAME_ID')\n",
    "xcol = [col for col in df_clean.columns if '_x' in col]\n",
    "ycol = [col for col in df_clean.columns if '_y' in col]\n",
    "df_clean = df_clean.drop(ycol, axis = 1)\n",
    "df_clean.columns = df_clean.columns.str.replace(r'_x$', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win & Loss Counters\n",
    "\n",
    "\n",
    "We then do the same thing but for the Win and Loss counter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Win-Loss Counter: HOME\n",
    "df_clean2 = df_clean.copy()\n",
    "season_dict = games[['GAME_ID', 'SEASON']].set_index('GAME_ID').iloc[:,0].T.to_dict()\n",
    "df_clean2['SEASON'] = df_clean2['GAME_ID'].map(season_dict)\n",
    "\n",
    "keep_cols = ['TIMESTAMP', 'SEASON', 'HOME_TEAM_WINS','GAME_ID', 'HOME_TEAM_NAME', 'AWAY_TEAM_NAME']\n",
    "df_clean2_temp = df_clean2[keep_cols]\n",
    "df_clean2_temp['AWAY_TEAM_WINS'] = df_clean2_temp['HOME_TEAM_WINS'].map({0:1, 1:0})\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['HOME_TEAM_NAME', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['HOME_TEAM_NAME']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_wins_home = df_g2['HOME_TEAM_WINS'].apply(cum_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_home = df_g2['HOME_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_home = df_g2['HOME_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_losses_home = df_g2['HOME_TEAM_WINS'].apply(cum_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_home = df_g2['HOME_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_home = df_g2['HOME_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_home'] = cum_wins_home\n",
    "df_clean2['cum_LOSSES_home'] = cum_losses_home\n",
    "df_clean2['cum_5_WINS_home'] = cum_5_wins_home\n",
    "df_clean2['cum_10_WINS_home'] = cum_10_wins_home\n",
    "df_clean2['cum_5_LOSSES_home'] = cum_5_losses_home\n",
    "df_clean2['cum_10_LOSSES_home'] = cum_10_losses_home\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Win-Loss Counter: AWAY\n",
    "\n",
    "\n",
    "df_clean2_temp['AWAY_TEAM_WINS'] = df_clean2_temp['HOME_TEAM_WINS'].map({0:1, 1:0})\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['AWAY_TEAM_NAME', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['AWAY_TEAM_NAME']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # AWAY WINS!\n",
    "cum_wins_away = df_g2['AWAY_TEAM_WINS'].apply(cum_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_away = df_g2['AWAY_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_away = df_g2['AWAY_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # AWAY LOSSES\n",
    "cum_losses_away = df_g2['AWAY_TEAM_WINS'].apply(cum_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_away = df_g2['AWAY_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_away = df_g2['AWAY_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_away'] = cum_wins_away\n",
    "df_clean2['cum_LOSSES_away'] = cum_losses_away\n",
    "df_clean2['cum_5_WINS_away'] = cum_5_wins_away\n",
    "df_clean2['cum_10_WINS_away'] = cum_10_wins_away\n",
    "df_clean2['cum_5_LOSSES_away'] = cum_5_losses_away\n",
    "df_clean2['cum_10_LOSSES_away'] = cum_10_losses_away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Win-Loss Counter: HOME HEAD-TO-HEAD \n",
    "\n",
    "#HOME HEAD TO HEAD Win/Loss Counter\n",
    "\n",
    "df_clean2_temp['AWAY_TEAM_WINS'] = df_clean2_temp['HOME_TEAM_WINS'].map({0:1, 1:0})\n",
    "df_clean2_temp['TEAM_LIST'] = df_clean2_temp['HOME_TEAM_NAME'] + ', ' + df_clean2_temp['AWAY_TEAM_NAME']\n",
    "\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['TEAM_LIST', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['TEAM_LIST']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_wins_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # HOME Losses!\n",
    "cum_losses_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_home_H2H = df_g2['HOME_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('HOME_TEAM_WINS')['HOME_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_home_H2H'] = cum_wins_home_H2H\n",
    "df_clean2['cum_LOSSES_home_H2H'] = cum_losses_home_H2H\n",
    "df_clean2['cum_5_WINS_home_H2H'] = cum_5_wins_home_H2H\n",
    "df_clean2['cum_10_WINS_home_H2H'] = cum_10_wins_home_H2H\n",
    "df_clean2['cum_5_LOSSES_home_H2H'] = cum_5_losses_home_H2H\n",
    "df_clean2['cum_10_LOSSES_home_H2H'] = cum_10_losses_home_H2H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Win-Loss Counter: AWAY HEAD-TO-HEAD\n",
    "\n",
    "# Two Grouped DFs\n",
    "df_g1 = df_clean2_temp.groupby(['TEAM_LIST', 'SEASON','GAME_ID']).mean()\n",
    "df_g2 = df_clean2_temp.groupby(['TEAM_LIST']).agg(list)\n",
    "\n",
    "# Using Functions above to calulate the cumulative values as lists\n",
    "\n",
    "    # HOME WINS!\n",
    "cum_wins_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_wins_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_5_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_wins_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_10_wins).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "    # HOME Losses!\n",
    "cum_losses_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_5_losses_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_5_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "cum_10_losses_away_H2H = df_g2['AWAY_TEAM_WINS'].apply(cum_10_losses).to_frame().explode('AWAY_TEAM_WINS')['AWAY_TEAM_WINS'].to_numpy()\n",
    "\n",
    "\n",
    "# #df_clean = df_clean.reset_index()\n",
    "df_clean2['cum_WINS_away_H2H'] = cum_wins_away_H2H\n",
    "df_clean2['cum_LOSSES_away_H2H'] = cum_losses_away_H2H\n",
    "df_clean2['cum_5_WINS_away_H2H'] = cum_5_wins_away_H2H\n",
    "df_clean2['cum_10_WINS_away_H2H'] = cum_10_wins_away_H2H\n",
    "df_clean2['cum_5_LOSSES_away_H2H'] = cum_5_losses_away_H2H\n",
    "df_clean2['cum_10_LOSSES_away_H2H'] = cum_10_losses_away_H2H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegas Betting Odds\n",
    "\n",
    "We then load in our ODDS datasets for the required seasons. This dataset required cleaning for the names and dates to make it usable to join with the rest of the cumulative statistics. We then merge DF_CLEAN2 with ODDS using the TIMESTAMP and HOME_TEAM_NAME as keys, losing a few rows along the way due to missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Cleaning the Odds DataFrame \n",
    "\n",
    "cols = [col for col in df_clean2.columns if np.any(['5' in col, '10' in col])]\n",
    "for col in cols: \n",
    "    new_col = re.sub(r'_\\d{1,2}_', '_', col)\n",
    "    df_clean2[col] = df_clean2[col].fillna(df_clean2[new_col])\n",
    "\n",
    "\n",
    "odds = pd.DataFrame()\n",
    "for season in season_list: \n",
    "    df = pd.read_excel('ODDS_DATA/odds_' + str(season) + '.xlsx', usecols = ['Date', 'Team', 'ML'])\n",
    "    df['Date'] = df['Date'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "\n",
    "    new_year_index = df[df['Date'].str.contains(r'01\\d{2}')].index[0]\n",
    "    year1 = np.repeat(str(season), new_year_index-1)\n",
    "    year2 = np.repeat(str(season+1), len(df) - len(year1))\n",
    "    df['Year'] = np.append(year1, year2)\n",
    "    df['TIMESTAMP'] = df['Year'] + df['Date']\n",
    "    df = df.drop('Date', axis = 1)\n",
    "    odds = odds.append(df)\n",
    "home_teams, home_odds = odds.iloc[1::2]['Team'], odds.iloc[1::2]['ML']\n",
    "away_teams, away_odds = odds.iloc[::2]['Team'], odds.iloc[::2]['ML']\n",
    "dates = odds.iloc[1::2]['TIMESTAMP']\n",
    "odds = pd.DataFrame({'TIMESTAMP' : list(dates), \n",
    "                   'HOME_TEAM_NAME' : list(home_teams),\n",
    "                   'AWAY_TEAM_NAME' : list(away_teams), \n",
    "                   'HOME_TEAM_ODDS' : list(home_odds), \n",
    "                   'AWAY_TEAM_ODDS' : list(away_odds)})\n",
    "odds['HOME_TEAM_NAME'] = odds['HOME_TEAM_NAME'].str.replace(r'([a-z])([A-Z])', r'\\1 \\2')\n",
    "odds['AWAY_TEAM_NAME'] = odds['AWAY_TEAM_NAME'].str.replace(r'([a-z])([A-Z])', r'\\1 \\2')\n",
    "\n",
    "odds['TIMESTAMP'] = pd.to_datetime(odds['TIMESTAMP'], format = '%Y%m%d', errors = 'coerce')\n",
    "teams = pd.read_csv('FP1_DATA/teams.csv', usecols = ['CITY', 'NICKNAME'])\n",
    "teams['CITY'][7] = 'LAClippers'\n",
    "teams['CITY'][8] = 'LALakers'\n",
    "\n",
    "odds['HOME_TEAM_NAME'] = odds['HOME_TEAM_NAME'].str.replace('New Jersey', 'Brooklyn')\n",
    "odds['AWAY_TEAM_NAME'] = odds['AWAY_TEAM_NAME'].str.replace('New Jersey', 'Brooklyn')\n",
    "\n",
    "team_city_dict = teams.set_index('CITY')['NICKNAME'].T.to_dict()\n",
    "odds['HOME_TEAM_NAME'] = odds['HOME_TEAM_NAME'].map(team_city_dict)\n",
    "odds['AWAY_TEAM_NAME'] = odds['AWAY_TEAM_NAME'].map(team_city_dict)\n",
    "\n",
    "new_df = pd.merge(df_clean2, odds, on = ['TIMESTAMP', 'HOME_TEAM_NAME'])\n",
    "\n",
    "cols = [col for col in new_df.columns if np.any(['5' in col, '10' in col])]\n",
    "for col in cols: \n",
    "    new_col = re.sub(r'_\\d{1,2}_', '_', col)\n",
    "    new_df[col] = new_df[col].fillna(new_df[new_col])\n",
    "\n",
    "new_df = new_df.fillna(0)\n",
    "new_df['SEASON'] = new_df['GAME_ID'].map(season_dict)\n",
    "\n",
    "cum_col_list = np.append([col for col in list(new_df.columns) if 'cum' in col], ['HOME_TEAM_WINS', 'SEASON', 'GAME_ID'])\n",
    "new_df = new_df[cum_col_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we convert this final dataframe to a CSV to load into our FP3_MODELS notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('FP1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15465, 819)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
