{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Importing Libraries & Functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.lib.display import Audio\n",
    "framerate = 4410\n",
    "# play_time_seconds = 3\n",
    "# t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "# audio_data = np.sin(2*np.pi*300*t) + np.sin(2*np.pi*240*t)\n",
    "# Audio(audio_data, rate=framerate, autoplay=True)\n",
    "\n",
    "#SciKit Learn \n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def outputs(cm):\n",
    "    acc = np.round((cm.ravel()[0]+cm.ravel()[3])/sum(cm.ravel()),4)\n",
    "    tpr = np.round(cm.ravel()[3] / (cm.ravel()[3] + cm.ravel()[2]),4)\n",
    "    fpr = np.round(cm.ravel()[1] / (cm.ravel()[1] + cm.ravel()[0]),4)\n",
    "    outputs = [acc, tpr, fpr]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data + Train/Test Split\n",
    "\n",
    "First, we load in the two CSVs from the FP1_CUM_STATS and FP2_XFACTOR notebooks and join them on the 'GAME_ID' as the key. We then set our training data to be all data from the 2010 Season to the 2016 Season, and our testing data to be all data from the 2017 Season to the 2018 Season. The cell below performs this task, and outputs the Counts and Percentages of Training and Testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 10216 rows -- 66.1%\n",
      "Validation Data: 2625 rows -- 17.0%\n",
      "Testing Data: 2624 rows -- 17.0%\n"
     ]
    }
   ],
   "source": [
    "# Splitting Data\n",
    "FP1 = pd.read_csv('FP1.csv')\n",
    "FP2 = pd.read_csv('FP2.csv')\n",
    "df = pd.merge(FP1, FP2, on = 'GAME_ID').rename(columns={'SEASON_y':'SEASON'})\n",
    "df = df.drop(['TIMESTAMP','HOME_TEAM_NAME'], axis=1)\n",
    "\n",
    "train_df = df[df['SEASON'].between(2007,2014)]\n",
    "val_df = df[df['SEASON'].between(2015,2016)]\n",
    "test_df = df[df['SEASON'].between(2017,2018)]\n",
    "\n",
    "train_df, val_df, test_df = train_df.drop(['SEASON', 'GAME_ID'], axis = 1), val_df.drop(['SEASON', 'GAME_ID'], axis = 1), test_df.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train, y_train = train_df.drop('HOME_TEAM_WINS', axis = 1), train_df['HOME_TEAM_WINS']\n",
    "X_val, y_val = val_df.drop('HOME_TEAM_WINS', axis = 1), val_df['HOME_TEAM_WINS']\n",
    "X_test, y_test = test_df.drop('HOME_TEAM_WINS', axis = 1), test_df['HOME_TEAM_WINS']\n",
    "\n",
    "train_prop = np.round(len(X_train) / (len(df)), 3)\n",
    "val_prop = np.round(len(X_val) / (len(df)), 3)\n",
    "test_prop = np.round(len(X_test) / (len(df)), 3)\n",
    "\n",
    "print('Training Data: ' + str(len(X_train)) + ' rows -- ' + str(np.round(train_prop*100,2)) + '%')\n",
    "print('Validation Data: ' + str(len(X_val)) + ' rows -- ' + str(np.round(val_prop*100,2)) + '%')\n",
    "print('Testing Data: ' + str(len(X_test)) + ' rows -- ' + str(np.round(test_prop*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#0: Baseline (Dummy) Model\n",
    "\n",
    "Our dummy model predicts the most frequent label in the training set, which is HOME_TEAM_WINS = 1. This is the same as always predicting the home team to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[   0 1079]\n",
      " [   0 1545]]\n",
      "\n",
      "Accuracy :  0.5888\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model \n",
    "model_0 = DummyClassifier(strategy = \"most_frequent\")\n",
    "model_0.fit(X_train, y_train)\n",
    "y_pred_0 = model_0.predict(X_test)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_0 = confusion_matrix(y_test, y_pred_0)\n",
    "outputs_0 = outputs(cm_0)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_0) \n",
    "print (\"\\nAccuracy : \", outputs_0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Feature Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Implementing PCA\n",
    "\n",
    "# # Fitting Scaler to X_train \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# # Apply transform to both the training set and the test set.\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Make an instance of the Model ==> 0.95 means 95% of variance explained\n",
    "# pca = PCA(n_components = 0.999999999999999)\n",
    "# pca.fit(X_train, y_train)\n",
    "# print('Number of Components: ', pca.n_components_)\n",
    "\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Scree Plots\n",
    "# yy = pca.explained_variance_ratio_\n",
    "# xx = np.arange(1,len(yy)+1)\n",
    "\n",
    "# sns.set_style('darkgrid')\n",
    "# fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "# plt.plot(xx, yy)\n",
    "# plt.xlabel('PCA #', fontsize = 16), plt.ylabel('Explained Variance %', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#1: Logistic Regression\n",
    "\n",
    "The cell below cretes a Logistic Regression model and calculates the outputs to be presented later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 470  609]\n",
      " [ 275 1270]]\n",
      "\n",
      "Accuracy :  0.6631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "model_1 = LogisticRegression(random_state = 69, tol = 0.00001, verbose = 3, n_jobs = -1)\n",
    "model_1.fit(X_train, y_train)\n",
    "y_prob_1 = model_1.predict_proba(X_test)\n",
    "y_pred_1 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_1[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_1 = confusion_matrix(y_test, y_pred_1)\n",
    "outputs_1 = outputs(cm_1)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_1) \n",
    "print (\"\\nAccuracy : \", outputs_1[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# cm_df = pd.DataFrame(cm_1).rename(columns = {0: 'TRUE HOME WINS', 1: 'TRUE AWAY WINS'})\n",
    "# cm_df = cm_df.rename({0: 'PREDICT HOME WINS', 1: 'PREDICT AWAY WINS'})\n",
    "# print(cm_df.to_latex())\n",
    "\n",
    "# out_df = pd.DataFrame(outputs_1).rename({0:'Accuracy', 1:'TPR', 2:'FPR'})\n",
    "# out_df = out_df.rename(columns = {0:'Test Set Statistic'})\n",
    "# print(out_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#2: Random Forest \n",
    "\n",
    "This cell below is a Random Forest model with cross-validation on various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features = 1 -- 0.5802397727966309\n",
      "Max Features = 9 -- 1.8410797119140625\n",
      "Max Features = 17 -- 3.8025858402252197\n",
      "Max Features = 26 -- 6.743449687957764\n",
      "Max Features = 34 -- 10.980271816253662\n",
      "Max Features = 43 -- 16.78975796699524\n",
      "Max Features = 51 -- 23.835094928741455\n",
      "Max Features = 59 -- 32.094812870025635\n",
      "Max Features = 68 -- 41.57308578491211\n",
      "Max Features = 76 -- 52.20741581916809\n",
      "Max Features = 85 -- 64.02583980560303\n",
      "Max Features = 93 -- 77.18632769584656\n",
      "Max Features = 101 -- 91.23670768737793\n",
      "Max Features = 110 -- 106.66675090789795\n",
      "Max Features = 118 -- 123.14095282554626\n",
      "Max Features = 127 -- 141.00074696540833\n",
      "Max Features = 135 -- 159.91439580917358\n",
      "Max Features = 143 -- 180.0077838897705\n",
      "Max Features = 152 -- 201.1083059310913\n",
      "Max Features = 160 -- 223.48351573944092\n",
      "Max Features = 169 -- 246.80750060081482\n",
      "Max Features = 177 -- 271.87873101234436\n",
      "Max Features = 185 -- 298.6088969707489\n",
      "Max Features = 194 -- 326.1380808353424\n",
      "Max Features = 202 -- 354.6783139705658\n",
      "Max Features = 211 -- 384.573184967041\n",
      "Max Features = 219 -- 416.34372878074646\n",
      "Max Features = 227 -- 448.5166697502136\n",
      "Max Features = 236 -- 482.3291337490082\n",
      "Max Features = 244 -- 517.3729407787323\n",
      "Max Features = 253 -- 553.4374940395355\n",
      "Max Features = 261 -- 590.8779168128967\n",
      "Max Features = 269 -- 628.7457978725433\n",
      "Max Features = 278 -- 668.7507898807526\n",
      "Max Features = 286 -- 709.7746548652649\n",
      "Max Features = 295 -- 751.2523508071899\n",
      "Max Features = 303 -- 794.9787018299103\n",
      "Max Features = 311 -- 839.4167568683624\n",
      "Max Features = 320 -- 884.6059958934784\n",
      "Max Features = 328 -- 931.528021812439\n",
      "Max Features = 337 -- 979.7658040523529\n",
      "Max Features = 345 -- 1028.7342216968536\n",
      "Max Features = 353 -- 1079.1699867248535\n",
      "Max Features = 362 -- 1131.5174379348755\n",
      "Max Features = 370 -- 1184.150809764862\n",
      "Max Features = 379 -- 1238.1036758422852\n",
      "Max Features = 387 -- 1292.813824892044\n",
      "Max Features = 395 -- 1348.9389517307281\n",
      "Max Features = 404 -- 1406.7602717876434\n",
      "Max Features = 412 -- 1465.3755478858948\n",
      "Max Features = 421 -- 1525.8545727729797\n",
      "Max Features = 429 -- 1587.7599108219147\n",
      "Max Features = 438 -- 1651.3288969993591\n",
      "Max Features = 446 -- 1714.7107598781586\n",
      "Max Features = 454 -- 1780.662032842636\n",
      "Max Features = 463 -- 1852.0152928829193\n",
      "Max Features = 471 -- 1927.9077968597412\n",
      "Max Features = 480 -- 2002.152851819992\n",
      "Max Features = 488 -- 2075.9107937812805\n",
      "Max Features = 496 -- 2152.603185892105\n",
      "Max Features = 505 -- 2229.7059237957\n",
      "Max Features = 513 -- 2297.4847910404205\n",
      "Max Features = 522 -- 2358.702922821045\n",
      "Max Features = 530 -- 2420.246971845627\n",
      "Max Features = 538 -- 2484.212243795395\n",
      "Max Features = 547 -- 2549.9500069618225\n",
      "Max Features = 555 -- 2617.571552991867\n",
      "Max Features = 564 -- 2685.7341997623444\n",
      "Max Features = 572 -- 2759.846480846405\n",
      "Max Features = 580 -- 2837.452246904373\n",
      "Max Features = 589 -- 2918.7955248355865\n",
      "Max Features = 597 -- 2998.437531709671\n",
      "Max Features = 606 -- 3079.705198764801\n",
      "Max Features = 614 -- 3160.8326346874237\n",
      "Max Features = 622 -- 3242.9868018627167\n",
      "Max Features = 631 -- 3325.994483947754\n",
      "Max Features = 639 -- 3411.1923110485077\n",
      "Max Features = 648 -- 3500.153830766678\n",
      "Max Features = 656 -- 3587.0360667705536\n",
      "Max Features = 664 -- 3676.964289665222\n",
      "Max Features = 673 -- 3766.2062866687775\n",
      "Max Features = 681 -- 3856.099604845047\n",
      "Max Features = 690 -- 3947.3090398311615\n",
      "Max Features = 698 -- 4040.380526781082\n",
      "Max Features = 706 -- 4136.131063699722\n",
      "Max Features = 715 -- 4231.4691326618195\n",
      "Max Features = 723 -- 4325.781622886658\n",
      "Max Features = 732 -- 4424.386868953705\n",
      "Max Features = 740 -- 4523.103457927704\n",
      "Max Features = 748 -- 4624.648224830627\n",
      "Max Features = 757 -- 4726.159118652344\n",
      "Max Features = 765 -- 4829.640272855759\n",
      "Max Features = 774 -- 4935.779028892517\n",
      "Max Features = 782 -- 5043.665444850922\n",
      "Max Features = 790 -- 5152.98162984848\n",
      "Max Features = 799 -- 5263.7143568992615\n",
      "Max Features = 807 -- 5379.108921766281\n",
      "Max Features = 816 -- 5492.800011873245\n",
      "Max Features = 824 -- 5611.36653470993\n",
      "Max Features = 833 -- 5734.106413841248\n",
      "Optimal Max Features:  93\n",
      "Optimal Validation Accuracy:  0.6777\n",
      "Elapsed Time:  95 min  5734.107 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "model_2_acc_list = []\n",
    "max_feature_list = np.linspace(1, len(X_train.columns), 100, dtype = 'int32')\n",
    "\n",
    "for i in max_feature_list: \n",
    "    \n",
    "    model_2 = RandomForestClassifier(max_features = i, n_estimators = 250, random_state = 69, \n",
    "                                     n_jobs = -1, min_samples_leaf = 5)\n",
    "    model_2.fit(X_train, y_train)\n",
    "   \n",
    "    y_prob_2 = model_2.predict_proba(X_val)\n",
    "    y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_val.index)\n",
    "    \n",
    "    # Confusion Matrix & Outputs \n",
    "    cm_2 = confusion_matrix(y_val, y_pred_2)\n",
    "    accuracy = outputs(cm_2)[0]\n",
    "    \n",
    "    model_2_acc_list.append(accuracy)\n",
    "    print(f'Max Features = {str(i)} -- {time.time()-tic}')\n",
    "    \n",
    "opt_max_features = max_feature_list[np.argmax(model_2_acc_list)]\n",
    "    \n",
    "print('Optimal Max Features: ', opt_max_features)\n",
    "print('Optimal Validation Accuracy: ', np.max(model_2_acc_list))\n",
    "\n",
    "toc = time.time()\n",
    "minutes, seconds = np.floor((toc-tic)/60), np.round((toc-tic) - (60*minutes),3)\n",
    "print(f'Elapsed Time:  {int(minutes)} min  {seconds} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 370  709]\n",
      " [ 216 1329]]\n",
      "\n",
      "Accuracy :  0.6475\n"
     ]
    }
   ],
   "source": [
    "# Run our Best Model \n",
    "\n",
    "model_2 = RandomForestClassifier(max_features = opt_max_features, n_estimators = 200, random_state = 69, \n",
    "                                 n_jobs = -1, min_samples_split = 10, min_samples_leaf = 10)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "y_prob_2 = model_2.predict_proba(X_test)\n",
    "y_pred_2 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_2[:,1]], index = y_test.index)\n",
    "# \n",
    "# Confusion Matrix & Outputs \n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "outputs_2 = outputs(cm_2)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_2) \n",
    "print (\"\\nAccuracy : \", outputs_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# max_features_RF = RF_model.cv_results_['param_max_features'].data\n",
    "# acc_scores_RF = RF_model.cv_results_['mean_test_score']\n",
    "\n",
    "# sns.set_style('darkgrid')\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.title('\\nMean Cross-Validation Accuracy vs. max_features\\n', fontsize=16)\n",
    "# plt.xlabel('max_features', fontsize=16)\n",
    "# plt.ylabel('Mean Cross-Validation Accuracy', fontsize=16)\n",
    "# plt.plot(max_features_RF, acc_scores_RF, linewidth=3, color='orange')\n",
    "# plt.scatter(max_features_RF[acc_scores_RF.argmax(axis=0)], np.max(acc_scores_RF), s=125, marker='o')\n",
    "# plt.grid(True, which='both')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#3: XGBoost\n",
    "\n",
    "The cells below go through an entire process of creating and cross-validating an XGBoost model. First, we set default parameters and using the XGB Cross-Validation function to determine the optimal number of trees for the specified learning rate. We then use this number of estimators to cross-validate and select an approximate values for *max_depth* and *min_child_weight*, which we then find more optimally by shrinking the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Combining train and val\n",
    "\n",
    "train_df_3 = df[df['SEASON'].between(2007,2016)]\n",
    "\n",
    "train_df_3 = train_df_3.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train_3, y_train_3 = train_df_3.drop('HOME_TEAM_WINS', axis = 1), train_df_3['HOME_TEAM_WINS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Defining the Cross-Validation Function \n",
    "def modelfit(alg, train, predictors, useTrainCV = True, cv_folds = 5, early_stopping_rounds = 50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train[predictors].values, label=train[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold = cv_folds,\n",
    "                          metrics = 'error', early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "        alg.set_params(n_estimators = cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train[predictors], train['HOME_TEAM_WINS'], eval_metric = 'error')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train[predictors])\n",
    "    train_predprob = alg.predict_proba(train[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(train['HOME_TEAM_WINS'].values, train_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(train['HOME_TEAM_WINS'], train_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the first cell below and the final number it outputs in square brackets is the n_estimators that you plug into gridsearchcv object below.\n",
    "\n",
    "STEP \\#1 : Calculate n_estimators for learning_rate = 0.1 (VERY HIGH)\n",
    "\n",
    "STEP \\#2 : Use Cross-Validation to tune the other hyperparameters\n",
    "\n",
    "STEP \\#3 : Now reduce the learning rate to 0.01 or something smaller and calculate the new n_estimators for the hyperparameters found above. Obviously, this will mean much more trees are used.\n",
    "\n",
    "STEP \\#4 : Run our final model using the learning rate set above, the hyperparameters found using cross-validation, and the corresponding (now higher) n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.30743+0.00701\ttest-error:0.34211+0.00763\n",
      "[1]\ttrain-error:0.29597+0.00441\ttest-error:0.33541+0.01080\n",
      "[2]\ttrain-error:0.28857+0.00521\ttest-error:0.33284+0.00777\n",
      "[3]\ttrain-error:0.28421+0.00481\ttest-error:0.33043+0.00546\n",
      "[4]\ttrain-error:0.28035+0.00391\ttest-error:0.32786+0.00736\n",
      "[5]\ttrain-error:0.27778+0.00277\ttest-error:0.32934+0.00575\n",
      "[6]\ttrain-error:0.27632+0.00419\ttest-error:0.32871+0.00749\n",
      "[7]\ttrain-error:0.27459+0.00516\ttest-error:0.32614+0.00784\n",
      "[8]\ttrain-error:0.27217+0.00425\ttest-error:0.32404+0.01012\n",
      "[9]\ttrain-error:0.26955+0.00450\ttest-error:0.32326+0.00956\n",
      "[10]\ttrain-error:0.26778+0.00359\ttest-error:0.32256+0.00879\n",
      "[11]\ttrain-error:0.26598+0.00419\ttest-error:0.32311+0.00988\n",
      "[12]\ttrain-error:0.26474+0.00307\ttest-error:0.32069+0.00922\n",
      "[13]\ttrain-error:0.26308+0.00306\ttest-error:0.32030+0.00915\n",
      "[14]\ttrain-error:0.26020+0.00282\ttest-error:0.31952+0.00832\n",
      "[15]\ttrain-error:0.25859+0.00295\ttest-error:0.32100+0.00865\n",
      "[16]\ttrain-error:0.25621+0.00329\ttest-error:0.31976+0.00959\n",
      "[17]\ttrain-error:0.25524+0.00322\ttest-error:0.31898+0.00874\n",
      "[18]\ttrain-error:0.25284+0.00370\ttest-error:0.31836+0.00815\n",
      "[19]\ttrain-error:0.25037+0.00228\ttest-error:0.31711+0.00957\n",
      "[20]\ttrain-error:0.24842+0.00251\ttest-error:0.31742+0.00882\n",
      "[21]\ttrain-error:0.24729+0.00255\ttest-error:0.31719+0.00890\n",
      "[22]\ttrain-error:0.24486+0.00183\ttest-error:0.31750+0.00947\n",
      "[23]\ttrain-error:0.24223+0.00224\ttest-error:0.31750+0.00867\n",
      "[24]\ttrain-error:0.24095+0.00251\ttest-error:0.31711+0.00693\n",
      "[25]\ttrain-error:0.23978+0.00260\ttest-error:0.31625+0.00830\n",
      "[26]\ttrain-error:0.23799+0.00274\ttest-error:0.31672+0.00836\n",
      "[27]\ttrain-error:0.23590+0.00239\ttest-error:0.31602+0.00856\n",
      "[28]\ttrain-error:0.23419+0.00264\ttest-error:0.31617+0.00859\n",
      "[29]\ttrain-error:0.23224+0.00260\ttest-error:0.31586+0.00832\n",
      "[30]\ttrain-error:0.23131+0.00296\ttest-error:0.31571+0.00953\n",
      "[31]\ttrain-error:0.22960+0.00333\ttest-error:0.31586+0.00988\n",
      "[32]\ttrain-error:0.22763+0.00354\ttest-error:0.31415+0.00967\n",
      "[33]\ttrain-error:0.22670+0.00411\ttest-error:0.31508+0.00936\n",
      "[34]\ttrain-error:0.22485+0.00508\ttest-error:0.31586+0.00867\n",
      "[35]\ttrain-error:0.22383+0.00485\ttest-error:0.31703+0.00761\n",
      "[36]\ttrain-error:0.22185+0.00532\ttest-error:0.31672+0.00770\n",
      "[37]\ttrain-error:0.21963+0.00500\ttest-error:0.31594+0.00744\n",
      "[38]\ttrain-error:0.21883+0.00491\ttest-error:0.31524+0.00760\n",
      "[39]\ttrain-error:0.21696+0.00561\ttest-error:0.31602+0.00664\n",
      "[40]\ttrain-error:0.21484+0.00547\ttest-error:0.31656+0.00592\n",
      "[41]\ttrain-error:0.21355+0.00500\ttest-error:0.31734+0.00514\n",
      "[42]\ttrain-error:0.21102+0.00474\ttest-error:0.31874+0.00589\n",
      "[43]\ttrain-error:0.20968+0.00471\ttest-error:0.31742+0.00695\n",
      "[44]\ttrain-error:0.20804+0.00473\ttest-error:0.31664+0.00768\n",
      "[45]\ttrain-error:0.20658+0.00440\ttest-error:0.31789+0.00720\n",
      "[46]\ttrain-error:0.20440+0.00399\ttest-error:0.31750+0.00785\n",
      "[47]\ttrain-error:0.20255+0.00447\ttest-error:0.31882+0.00939\n",
      "[48]\ttrain-error:0.20100+0.00448\ttest-error:0.31773+0.00840\n",
      "[49]\ttrain-error:0.19948+0.00415\ttest-error:0.31820+0.00675\n",
      "[50]\ttrain-error:0.19749+0.00362\ttest-error:0.31726+0.00823\n",
      "[51]\ttrain-error:0.19547+0.00406\ttest-error:0.31726+0.00725\n",
      "[52]\ttrain-error:0.19395+0.00422\ttest-error:0.31711+0.00841\n",
      "[53]\ttrain-error:0.19261+0.00467\ttest-error:0.31688+0.00838\n",
      "[54]\ttrain-error:0.19109+0.00410\ttest-error:0.31688+0.00807\n",
      "[55]\ttrain-error:0.18955+0.00464\ttest-error:0.31703+0.00700\n",
      "[56]\ttrain-error:0.18834+0.00460\ttest-error:0.31828+0.00839\n",
      "[57]\ttrain-error:0.18731+0.00477\ttest-error:0.31758+0.00717\n",
      "[58]\ttrain-error:0.18540+0.00461\ttest-error:0.31695+0.00575\n",
      "[59]\ttrain-error:0.18390+0.00477\ttest-error:0.31750+0.00587\n",
      "[60]\ttrain-error:0.18235+0.00501\ttest-error:0.31649+0.00578\n",
      "[61]\ttrain-error:0.18141+0.00497\ttest-error:0.31672+0.00647\n",
      "[62]\ttrain-error:0.17978+0.00428\ttest-error:0.31625+0.00566\n",
      "[63]\ttrain-error:0.17843+0.00368\ttest-error:0.31625+0.00530\n",
      "[64]\ttrain-error:0.17709+0.00379\ttest-error:0.31789+0.00566\n",
      "[65]\ttrain-error:0.17627+0.00380\ttest-error:0.31789+0.00557\n",
      "[66]\ttrain-error:0.17520+0.00359\ttest-error:0.31703+0.00612\n",
      "[67]\ttrain-error:0.17403+0.00412\ttest-error:0.31602+0.00608\n",
      "[68]\ttrain-error:0.17142+0.00377\ttest-error:0.31664+0.00558\n",
      "[69]\ttrain-error:0.17026+0.00399\ttest-error:0.31649+0.00656\n",
      "[70]\ttrain-error:0.16911+0.00370\ttest-error:0.31820+0.00552\n",
      "[71]\ttrain-error:0.16802+0.00326\ttest-error:0.31812+0.00615\n",
      "[72]\ttrain-error:0.16613+0.00327\ttest-error:0.31812+0.00631\n",
      "[73]\ttrain-error:0.16488+0.00293\ttest-error:0.31843+0.00710\n",
      "[74]\ttrain-error:0.16420+0.00305\ttest-error:0.31882+0.00616\n",
      "[75]\ttrain-error:0.16266+0.00325\ttest-error:0.31781+0.00662\n",
      "[76]\ttrain-error:0.16163+0.00317\ttest-error:0.31781+0.00635\n",
      "[77]\ttrain-error:0.16040+0.00269\ttest-error:0.31688+0.00699\n",
      "[78]\ttrain-error:0.15918+0.00236\ttest-error:0.31719+0.00770\n",
      "[79]\ttrain-error:0.15807+0.00315\ttest-error:0.31703+0.00746\n",
      "[80]\ttrain-error:0.15598+0.00394\ttest-error:0.31711+0.00740\n",
      "[81]\ttrain-error:0.15437+0.00405\ttest-error:0.31727+0.00767\n",
      "[82]\ttrain-error:0.15271+0.00413\ttest-error:0.31664+0.00797\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.758\n",
      "AUC Score (Train): 0.835561\n"
     ]
    }
   ],
   "source": [
    "# Determining Optimal Number of Estimators\n",
    "target = 'HOME_TEAM_WINS'\n",
    "train = pd.concat([X_train_3, y_train_3], axis = 1)\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "\n",
    "model_3 = XGBClassifier(learning_rate = 0.1, n_estimators = 1000, max_depth = 5, min_child_weight = 1,\n",
    "                     gamma = 0, subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic',\n",
    "                     nthread = 4, scale_pos_weight = 1, seed = 69)\n",
    "modelfit(model_3, train, X_train.columns, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:  6.3min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:05:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Approx. Parameters: \n",
      " {'max_depth': 3, 'min_child_weight': 3} 0.6634234989674395\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validating MAX_DEPTH and MIN_CHILD_WEIGHT\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=82, gamma=0, \n",
    "                                                  subsample=0.8, colsample_bytree=0.8, \n",
    "                                                  objective= 'binary:logistic', nthread=4, \n",
    "                                                  scale_pos_weight=1, seed=27), \n",
    "                        param_grid = param_test1, \n",
    "                        scoring = 'accuracy', n_jobs = -1, iid=False , cv=5, verbose = 3)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "print('Best Approx. Parameters: \\n', gsearch1.best_params_, gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  45 | elapsed:  1.3min remaining:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:33:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Parameters: \n",
      " {'max_depth': 2, 'min_child_weight': 3} 0.6674737069953398\n"
     ]
    }
   ],
   "source": [
    "# Reducing Search Space\n",
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[2,3,4]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=82, gamma=0, subsample=0.8, \n",
    "                                                  colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test2, scoring='accuracy', n_jobs=-1, iid=False, cv=5, verbose = 3)\n",
    "gsearch2.fit(train[predictors],train[target])\n",
    "print('Best Parameters: \\n', gsearch2.best_params_, gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tune *gamma* in the same way and then calculate the new optimal number of parameters for the new set of hyperparameters. We then tune *subsample*, *colsample_bytree*, and *reg_alpha* in the same way as the rest with the new optimal estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  25 | elapsed:   28.9s remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  25 | elapsed:   29.1s remaining:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  25 | elapsed:   46.1s remaining:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   46.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:34:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.0}, 0.6674737069953398)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validating GAMMA\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=82, max_depth=2, \n",
    "                                                  min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', nthread=4, \n",
    "                                                  scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test3, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.34569+0.00356\ttest-error:0.35021+0.00850\n",
      "[1]\ttrain-error:0.34242+0.00348\ttest-error:0.34694+0.01157\n",
      "[2]\ttrain-error:0.33590+0.00651\ttest-error:0.34257+0.00894\n",
      "[3]\ttrain-error:0.33235+0.00343\ttest-error:0.33525+0.01369\n",
      "[4]\ttrain-error:0.32994+0.00409\ttest-error:0.33596+0.00954\n",
      "[5]\ttrain-error:0.32719+0.00269\ttest-error:0.33222+0.01084\n",
      "[6]\ttrain-error:0.32605+0.00258\ttest-error:0.33191+0.00894\n",
      "[7]\ttrain-error:0.32540+0.00152\ttest-error:0.32973+0.01075\n",
      "[8]\ttrain-error:0.32404+0.00173\ttest-error:0.32972+0.00819\n",
      "[9]\ttrain-error:0.32313+0.00142\ttest-error:0.32902+0.00993\n",
      "[10]\ttrain-error:0.32112+0.00161\ttest-error:0.32887+0.00861\n",
      "[11]\ttrain-error:0.31925+0.00157\ttest-error:0.32832+0.00851\n",
      "[12]\ttrain-error:0.31976+0.00107\ttest-error:0.32677+0.00939\n",
      "[13]\ttrain-error:0.31915+0.00109\ttest-error:0.32568+0.00929\n",
      "[14]\ttrain-error:0.31799+0.00118\ttest-error:0.32474+0.00996\n",
      "[15]\ttrain-error:0.31691+0.00177\ttest-error:0.32451+0.00963\n",
      "[16]\ttrain-error:0.31713+0.00189\ttest-error:0.32497+0.00971\n",
      "[17]\ttrain-error:0.31575+0.00161\ttest-error:0.32474+0.01028\n",
      "[18]\ttrain-error:0.31491+0.00223\ttest-error:0.32552+0.01086\n",
      "[19]\ttrain-error:0.31419+0.00184\ttest-error:0.32443+0.01016\n",
      "[20]\ttrain-error:0.31341+0.00161\ttest-error:0.32466+0.00987\n",
      "[21]\ttrain-error:0.31322+0.00160\ttest-error:0.32404+0.01010\n",
      "[22]\ttrain-error:0.31209+0.00228\ttest-error:0.32381+0.01045\n",
      "[23]\ttrain-error:0.31154+0.00210\ttest-error:0.32342+0.01015\n",
      "[24]\ttrain-error:0.31090+0.00172\ttest-error:0.32124+0.00979\n",
      "[25]\ttrain-error:0.31080+0.00192\ttest-error:0.32124+0.00972\n",
      "[26]\ttrain-error:0.31031+0.00165\ttest-error:0.31983+0.00894\n",
      "[27]\ttrain-error:0.30975+0.00164\ttest-error:0.31983+0.00891\n",
      "[28]\ttrain-error:0.30944+0.00189\ttest-error:0.31983+0.00872\n",
      "[29]\ttrain-error:0.30792+0.00213\ttest-error:0.32022+0.00921\n",
      "[30]\ttrain-error:0.30734+0.00187\ttest-error:0.31983+0.00871\n",
      "[31]\ttrain-error:0.30732+0.00178\ttest-error:0.32022+0.00992\n",
      "[32]\ttrain-error:0.30667+0.00164\ttest-error:0.32015+0.01068\n",
      "[33]\ttrain-error:0.30681+0.00160\ttest-error:0.31983+0.01091\n",
      "[34]\ttrain-error:0.30597+0.00181\ttest-error:0.32015+0.01078\n",
      "[35]\ttrain-error:0.30537+0.00180\ttest-error:0.32069+0.01016\n",
      "[36]\ttrain-error:0.30502+0.00166\ttest-error:0.32124+0.00961\n",
      "[37]\ttrain-error:0.30473+0.00209\ttest-error:0.32054+0.01044\n",
      "[38]\ttrain-error:0.30463+0.00149\ttest-error:0.31952+0.00986\n",
      "[39]\ttrain-error:0.30401+0.00135\ttest-error:0.31945+0.01058\n",
      "[40]\ttrain-error:0.30346+0.00210\ttest-error:0.31890+0.01037\n",
      "[41]\ttrain-error:0.30307+0.00191\ttest-error:0.31804+0.00993\n",
      "[42]\ttrain-error:0.30294+0.00214\ttest-error:0.31843+0.01079\n",
      "[43]\ttrain-error:0.30313+0.00217\ttest-error:0.31921+0.01032\n",
      "[44]\ttrain-error:0.30243+0.00206\ttest-error:0.31758+0.01044\n",
      "[45]\ttrain-error:0.30187+0.00188\ttest-error:0.31726+0.01087\n",
      "[46]\ttrain-error:0.30159+0.00219\ttest-error:0.31680+0.01053\n",
      "[47]\ttrain-error:0.30151+0.00247\ttest-error:0.31680+0.00991\n",
      "[48]\ttrain-error:0.30142+0.00285\ttest-error:0.31688+0.01078\n",
      "[49]\ttrain-error:0.30070+0.00252\ttest-error:0.31617+0.01024\n",
      "[50]\ttrain-error:0.30029+0.00264\ttest-error:0.31610+0.01073\n",
      "[51]\ttrain-error:0.30015+0.00246\ttest-error:0.31680+0.00999\n",
      "[52]\ttrain-error:0.29943+0.00267\ttest-error:0.31656+0.00977\n",
      "[53]\ttrain-error:0.29889+0.00181\ttest-error:0.31656+0.00977\n",
      "[54]\ttrain-error:0.29838+0.00194\ttest-error:0.31617+0.01059\n",
      "[55]\ttrain-error:0.29793+0.00177\ttest-error:0.31586+0.01022\n",
      "[56]\ttrain-error:0.29768+0.00201\ttest-error:0.31532+0.00976\n",
      "[57]\ttrain-error:0.29708+0.00215\ttest-error:0.31524+0.00988\n",
      "[58]\ttrain-error:0.29680+0.00250\ttest-error:0.31454+0.00975\n",
      "[59]\ttrain-error:0.29618+0.00184\ttest-error:0.31571+0.00969\n",
      "[60]\ttrain-error:0.29606+0.00218\ttest-error:0.31524+0.00933\n",
      "[61]\ttrain-error:0.29540+0.00174\ttest-error:0.31524+0.00952\n",
      "[62]\ttrain-error:0.29554+0.00191\ttest-error:0.31477+0.00929\n",
      "[63]\ttrain-error:0.29571+0.00190\ttest-error:0.31485+0.00945\n",
      "[64]\ttrain-error:0.29569+0.00224\ttest-error:0.31540+0.00904\n",
      "[65]\ttrain-error:0.29548+0.00204\ttest-error:0.31469+0.00946\n",
      "[66]\ttrain-error:0.29567+0.00198\ttest-error:0.31508+0.00829\n",
      "[67]\ttrain-error:0.29513+0.00216\ttest-error:0.31462+0.00941\n",
      "[68]\ttrain-error:0.29478+0.00256\ttest-error:0.31462+0.00994\n",
      "[69]\ttrain-error:0.29435+0.00253\ttest-error:0.31454+0.01023\n",
      "[70]\ttrain-error:0.29400+0.00234\ttest-error:0.31438+0.01032\n",
      "[71]\ttrain-error:0.29394+0.00248\ttest-error:0.31470+0.00962\n",
      "[72]\ttrain-error:0.29351+0.00246\ttest-error:0.31508+0.00961\n",
      "[73]\ttrain-error:0.29330+0.00219\ttest-error:0.31423+0.00892\n",
      "[74]\ttrain-error:0.29305+0.00194\ttest-error:0.31508+0.00977\n",
      "[75]\ttrain-error:0.29305+0.00187\ttest-error:0.31555+0.00960\n",
      "[76]\ttrain-error:0.29238+0.00192\ttest-error:0.31501+0.00941\n",
      "[77]\ttrain-error:0.29219+0.00188\ttest-error:0.31532+0.00918\n",
      "[78]\ttrain-error:0.29182+0.00189\ttest-error:0.31508+0.00964\n",
      "[79]\ttrain-error:0.29180+0.00234\ttest-error:0.31462+0.00984\n",
      "[80]\ttrain-error:0.29137+0.00244\ttest-error:0.31392+0.00950\n",
      "[81]\ttrain-error:0.29122+0.00241\ttest-error:0.31446+0.01031\n",
      "[82]\ttrain-error:0.29114+0.00245\ttest-error:0.31454+0.01061\n",
      "[83]\ttrain-error:0.29051+0.00211\ttest-error:0.31431+0.00991\n",
      "[84]\ttrain-error:0.29007+0.00201\ttest-error:0.31446+0.00981\n",
      "[85]\ttrain-error:0.28964+0.00177\ttest-error:0.31384+0.00967\n",
      "[86]\ttrain-error:0.28941+0.00247\ttest-error:0.31368+0.01007\n",
      "[87]\ttrain-error:0.28925+0.00237\ttest-error:0.31446+0.01036\n",
      "[88]\ttrain-error:0.28863+0.00227\ttest-error:0.31493+0.01018\n",
      "[89]\ttrain-error:0.28794+0.00228\ttest-error:0.31493+0.01015\n",
      "[90]\ttrain-error:0.28757+0.00254\ttest-error:0.31579+0.00989\n",
      "[91]\ttrain-error:0.28732+0.00258\ttest-error:0.31610+0.00953\n",
      "[92]\ttrain-error:0.28697+0.00271\ttest-error:0.31649+0.00903\n",
      "[93]\ttrain-error:0.28666+0.00286\ttest-error:0.31579+0.00857\n",
      "[94]\ttrain-error:0.28586+0.00239\ttest-error:0.31524+0.00816\n",
      "[95]\ttrain-error:0.28555+0.00267\ttest-error:0.31563+0.00837\n",
      "[96]\ttrain-error:0.28539+0.00288\ttest-error:0.31571+0.00802\n",
      "[97]\ttrain-error:0.28541+0.00283\ttest-error:0.31540+0.00844\n",
      "[98]\ttrain-error:0.28530+0.00252\ttest-error:0.31462+0.00822\n",
      "[99]\ttrain-error:0.28467+0.00273\ttest-error:0.31454+0.00794\n",
      "[100]\ttrain-error:0.28493+0.00271\ttest-error:0.31477+0.00771\n",
      "[101]\ttrain-error:0.28467+0.00281\ttest-error:0.31516+0.00750\n",
      "[102]\ttrain-error:0.28473+0.00279\ttest-error:0.31485+0.00750\n",
      "[103]\ttrain-error:0.28448+0.00284\ttest-error:0.31508+0.00731\n",
      "[104]\ttrain-error:0.28432+0.00314\ttest-error:0.31540+0.00771\n",
      "[105]\ttrain-error:0.28409+0.00270\ttest-error:0.31532+0.00817\n",
      "[106]\ttrain-error:0.28337+0.00255\ttest-error:0.31477+0.00762\n",
      "[107]\ttrain-error:0.28298+0.00262\ttest-error:0.31407+0.00763\n",
      "[108]\ttrain-error:0.28286+0.00283\ttest-error:0.31423+0.00780\n",
      "[109]\ttrain-error:0.28288+0.00330\ttest-error:0.31384+0.00806\n",
      "[110]\ttrain-error:0.28242+0.00318\ttest-error:0.31423+0.00805\n",
      "[111]\ttrain-error:0.28222+0.00298\ttest-error:0.31361+0.00746\n",
      "[112]\ttrain-error:0.28212+0.00316\ttest-error:0.31384+0.00713\n",
      "[113]\ttrain-error:0.28173+0.00334\ttest-error:0.31438+0.00681\n",
      "[114]\ttrain-error:0.28119+0.00288\ttest-error:0.31438+0.00618\n",
      "[115]\ttrain-error:0.28070+0.00318\ttest-error:0.31431+0.00646\n",
      "[116]\ttrain-error:0.28024+0.00341\ttest-error:0.31477+0.00673\n",
      "[117]\ttrain-error:0.27988+0.00299\ttest-error:0.31438+0.00599\n",
      "[118]\ttrain-error:0.27967+0.00260\ttest-error:0.31345+0.00651\n",
      "[119]\ttrain-error:0.27967+0.00259\ttest-error:0.31353+0.00605\n",
      "[120]\ttrain-error:0.27918+0.00256\ttest-error:0.31353+0.00508\n",
      "[121]\ttrain-error:0.27893+0.00250\ttest-error:0.31361+0.00487\n",
      "[122]\ttrain-error:0.27899+0.00220\ttest-error:0.31298+0.00443\n",
      "[123]\ttrain-error:0.27813+0.00250\ttest-error:0.31298+0.00395\n",
      "[124]\ttrain-error:0.27794+0.00281\ttest-error:0.31384+0.00439\n",
      "[125]\ttrain-error:0.27798+0.00302\ttest-error:0.31353+0.00432\n",
      "[126]\ttrain-error:0.27796+0.00299\ttest-error:0.31275+0.00521\n",
      "[127]\ttrain-error:0.27743+0.00300\ttest-error:0.31259+0.00585\n",
      "[128]\ttrain-error:0.27728+0.00276\ttest-error:0.31290+0.00591\n",
      "[129]\ttrain-error:0.27689+0.00275\ttest-error:0.31290+0.00677\n",
      "[130]\ttrain-error:0.27700+0.00300\ttest-error:0.31259+0.00709\n",
      "[131]\ttrain-error:0.27650+0.00290\ttest-error:0.31220+0.00683\n",
      "[132]\ttrain-error:0.27644+0.00282\ttest-error:0.31189+0.00699\n",
      "[133]\ttrain-error:0.27599+0.00284\ttest-error:0.31220+0.00685\n",
      "[134]\ttrain-error:0.27587+0.00236\ttest-error:0.31322+0.00662\n",
      "[135]\ttrain-error:0.27595+0.00271\ttest-error:0.31275+0.00762\n",
      "[136]\ttrain-error:0.27584+0.00237\ttest-error:0.31275+0.00788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137]\ttrain-error:0.27566+0.00253\ttest-error:0.31298+0.00764\n",
      "[138]\ttrain-error:0.27496+0.00269\ttest-error:0.31267+0.00797\n",
      "[139]\ttrain-error:0.27465+0.00276\ttest-error:0.31283+0.00789\n",
      "[140]\ttrain-error:0.27414+0.00230\ttest-error:0.31314+0.00769\n",
      "[141]\ttrain-error:0.27428+0.00258\ttest-error:0.31329+0.00850\n",
      "[142]\ttrain-error:0.27397+0.00239\ttest-error:0.31275+0.00816\n",
      "[143]\ttrain-error:0.27385+0.00226\ttest-error:0.31275+0.00819\n",
      "[144]\ttrain-error:0.27377+0.00233\ttest-error:0.31306+0.00823\n",
      "[145]\ttrain-error:0.27367+0.00231\ttest-error:0.31228+0.00764\n",
      "[146]\ttrain-error:0.27317+0.00223\ttest-error:0.31189+0.00739\n",
      "[147]\ttrain-error:0.27303+0.00233\ttest-error:0.31166+0.00810\n",
      "[148]\ttrain-error:0.27276+0.00261\ttest-error:0.31283+0.00804\n",
      "[149]\ttrain-error:0.27278+0.00227\ttest-error:0.31314+0.00747\n",
      "[150]\ttrain-error:0.27243+0.00245\ttest-error:0.31189+0.00803\n",
      "[151]\ttrain-error:0.27245+0.00274\ttest-error:0.31267+0.00813\n",
      "[152]\ttrain-error:0.27229+0.00230\ttest-error:0.31314+0.00767\n",
      "[153]\ttrain-error:0.27223+0.00247\ttest-error:0.31306+0.00802\n",
      "[154]\ttrain-error:0.27184+0.00238\ttest-error:0.31283+0.00805\n",
      "[155]\ttrain-error:0.27144+0.00276\ttest-error:0.31244+0.00793\n",
      "[156]\ttrain-error:0.27116+0.00263\ttest-error:0.31236+0.00755\n",
      "[157]\ttrain-error:0.27093+0.00259\ttest-error:0.31213+0.00899\n",
      "[158]\ttrain-error:0.27083+0.00240\ttest-error:0.31267+0.00832\n",
      "[159]\ttrain-error:0.27050+0.00282\ttest-error:0.31228+0.00868\n",
      "[160]\ttrain-error:0.26996+0.00282\ttest-error:0.31205+0.00852\n",
      "[161]\ttrain-error:0.27001+0.00294\ttest-error:0.31228+0.00883\n",
      "[162]\ttrain-error:0.26982+0.00296\ttest-error:0.31220+0.00908\n",
      "[163]\ttrain-error:0.26970+0.00300\ttest-error:0.31220+0.00882\n",
      "[164]\ttrain-error:0.26945+0.00279\ttest-error:0.31189+0.00864\n",
      "[165]\ttrain-error:0.26898+0.00263\ttest-error:0.31213+0.00866\n",
      "[166]\ttrain-error:0.26826+0.00271\ttest-error:0.31213+0.00854\n",
      "[167]\ttrain-error:0.26822+0.00257\ttest-error:0.31205+0.00866\n",
      "[168]\ttrain-error:0.26816+0.00273\ttest-error:0.31205+0.00859\n",
      "[169]\ttrain-error:0.26789+0.00297\ttest-error:0.31103+0.00770\n",
      "[170]\ttrain-error:0.26746+0.00271\ttest-error:0.31127+0.00815\n",
      "[171]\ttrain-error:0.26696+0.00270\ttest-error:0.31111+0.00807\n",
      "[172]\ttrain-error:0.26711+0.00259\ttest-error:0.31096+0.00791\n",
      "[173]\ttrain-error:0.26692+0.00243\ttest-error:0.31127+0.00837\n",
      "[174]\ttrain-error:0.26668+0.00261\ttest-error:0.31119+0.00878\n",
      "[175]\ttrain-error:0.26670+0.00252\ttest-error:0.31072+0.00846\n",
      "[176]\ttrain-error:0.26633+0.00270\ttest-error:0.31119+0.00790\n",
      "[177]\ttrain-error:0.26639+0.00270\ttest-error:0.31088+0.00867\n",
      "[178]\ttrain-error:0.26585+0.00231\ttest-error:0.31111+0.00940\n",
      "[179]\ttrain-error:0.26559+0.00250\ttest-error:0.31096+0.00968\n",
      "[180]\ttrain-error:0.26530+0.00243\ttest-error:0.31096+0.00996\n",
      "[181]\ttrain-error:0.26526+0.00268\ttest-error:0.31189+0.00963\n",
      "[182]\ttrain-error:0.26522+0.00261\ttest-error:0.31197+0.00982\n",
      "[183]\ttrain-error:0.26497+0.00264\ttest-error:0.31166+0.01005\n",
      "[184]\ttrain-error:0.26464+0.00224\ttest-error:0.31127+0.00940\n",
      "[185]\ttrain-error:0.26478+0.00235\ttest-error:0.31119+0.00917\n",
      "[186]\ttrain-error:0.26482+0.00214\ttest-error:0.31205+0.00927\n",
      "[187]\ttrain-error:0.26437+0.00214\ttest-error:0.31181+0.00918\n",
      "[188]\ttrain-error:0.26435+0.00231\ttest-error:0.31127+0.00859\n",
      "[189]\ttrain-error:0.26408+0.00222\ttest-error:0.31065+0.00860\n",
      "[190]\ttrain-error:0.26392+0.00225\ttest-error:0.31096+0.00853\n",
      "[191]\ttrain-error:0.26388+0.00226\ttest-error:0.31057+0.00839\n",
      "[192]\ttrain-error:0.26295+0.00256\ttest-error:0.31018+0.00844\n",
      "[193]\ttrain-error:0.26250+0.00216\ttest-error:0.31072+0.00805\n",
      "[194]\ttrain-error:0.26221+0.00244\ttest-error:0.31057+0.00771\n",
      "[195]\ttrain-error:0.26217+0.00250\ttest-error:0.31072+0.00785\n",
      "[196]\ttrain-error:0.26182+0.00247\ttest-error:0.31041+0.00813\n",
      "[197]\ttrain-error:0.26203+0.00227\ttest-error:0.31072+0.00761\n",
      "[198]\ttrain-error:0.26193+0.00211\ttest-error:0.31018+0.00815\n",
      "[199]\ttrain-error:0.26166+0.00221\ttest-error:0.31057+0.00781\n",
      "[200]\ttrain-error:0.26145+0.00233\ttest-error:0.31119+0.00780\n",
      "[201]\ttrain-error:0.26102+0.00226\ttest-error:0.31096+0.00795\n",
      "[202]\ttrain-error:0.26092+0.00227\ttest-error:0.31111+0.00841\n",
      "[203]\ttrain-error:0.26075+0.00218\ttest-error:0.31103+0.00877\n",
      "[204]\ttrain-error:0.26012+0.00238\ttest-error:0.31103+0.00867\n",
      "[205]\ttrain-error:0.25999+0.00205\ttest-error:0.31103+0.00859\n",
      "[206]\ttrain-error:0.25987+0.00220\ttest-error:0.31080+0.00802\n",
      "[207]\ttrain-error:0.25997+0.00230\ttest-error:0.31080+0.00847\n",
      "[208]\ttrain-error:0.25968+0.00181\ttest-error:0.31096+0.00949\n",
      "[209]\ttrain-error:0.25966+0.00177\ttest-error:0.31103+0.00988\n",
      "[210]\ttrain-error:0.25950+0.00188\ttest-error:0.31096+0.01000\n",
      "[211]\ttrain-error:0.25944+0.00231\ttest-error:0.31119+0.01012\n",
      "[212]\ttrain-error:0.25872+0.00246\ttest-error:0.31166+0.01031\n",
      "[213]\ttrain-error:0.25882+0.00228\ttest-error:0.31119+0.01022\n",
      "[214]\ttrain-error:0.25859+0.00186\ttest-error:0.31142+0.00998\n",
      "[215]\ttrain-error:0.25855+0.00186\ttest-error:0.31150+0.00946\n",
      "[216]\ttrain-error:0.25816+0.00192\ttest-error:0.31096+0.00997\n",
      "[217]\ttrain-error:0.25794+0.00200\ttest-error:0.31111+0.00965\n",
      "[218]\ttrain-error:0.25810+0.00236\ttest-error:0.31127+0.01009\n",
      "[219]\ttrain-error:0.25750+0.00225\ttest-error:0.31103+0.01020\n",
      "[220]\ttrain-error:0.25740+0.00220\ttest-error:0.31049+0.00978\n",
      "[221]\ttrain-error:0.25711+0.00243\ttest-error:0.31049+0.00971\n",
      "[222]\ttrain-error:0.25697+0.00238\ttest-error:0.31088+0.01041\n",
      "[223]\ttrain-error:0.25656+0.00239\ttest-error:0.31088+0.01045\n",
      "[224]\ttrain-error:0.25631+0.00255\ttest-error:0.31080+0.01100\n",
      "[225]\ttrain-error:0.25586+0.00255\ttest-error:0.31135+0.01041\n",
      "[226]\ttrain-error:0.25574+0.00294\ttest-error:0.31142+0.01016\n",
      "[227]\ttrain-error:0.25530+0.00283\ttest-error:0.31127+0.01058\n",
      "[228]\ttrain-error:0.25500+0.00270\ttest-error:0.31103+0.01023\n",
      "[229]\ttrain-error:0.25465+0.00290\ttest-error:0.31057+0.01023\n",
      "[230]\ttrain-error:0.25442+0.00299\ttest-error:0.31018+0.01023\n",
      "[231]\ttrain-error:0.25450+0.00298\ttest-error:0.31010+0.01025\n",
      "[232]\ttrain-error:0.25446+0.00310\ttest-error:0.31033+0.01015\n",
      "[233]\ttrain-error:0.25446+0.00319\ttest-error:0.30987+0.00952\n",
      "[234]\ttrain-error:0.25413+0.00299\ttest-error:0.31033+0.01015\n",
      "[235]\ttrain-error:0.25378+0.00275\ttest-error:0.31026+0.01034\n",
      "[236]\ttrain-error:0.25341+0.00285\ttest-error:0.31041+0.01077\n",
      "[237]\ttrain-error:0.25382+0.00297\ttest-error:0.31119+0.01106\n",
      "[238]\ttrain-error:0.25343+0.00306\ttest-error:0.31072+0.01122\n",
      "[239]\ttrain-error:0.25302+0.00321\ttest-error:0.31041+0.01164\n",
      "[240]\ttrain-error:0.25245+0.00320\ttest-error:0.31033+0.01056\n",
      "[241]\ttrain-error:0.25259+0.00322\ttest-error:0.31041+0.01059\n",
      "[242]\ttrain-error:0.25261+0.00333\ttest-error:0.31135+0.01113\n",
      "[243]\ttrain-error:0.25267+0.00337\ttest-error:0.31065+0.01084\n",
      "[244]\ttrain-error:0.25265+0.00311\ttest-error:0.31072+0.01117\n",
      "[245]\ttrain-error:0.25228+0.00306\ttest-error:0.31189+0.01046\n",
      "[246]\ttrain-error:0.25238+0.00283\ttest-error:0.31158+0.01065\n",
      "[247]\ttrain-error:0.25238+0.00285\ttest-error:0.31213+0.01117\n",
      "[248]\ttrain-error:0.25236+0.00279\ttest-error:0.31197+0.01109\n",
      "[249]\ttrain-error:0.25206+0.00293\ttest-error:0.31236+0.01060\n",
      "[250]\ttrain-error:0.25193+0.00354\ttest-error:0.31205+0.01026\n",
      "[251]\ttrain-error:0.25177+0.00317\ttest-error:0.31104+0.01039\n",
      "[252]\ttrain-error:0.25144+0.00305\ttest-error:0.31166+0.01071\n",
      "[253]\ttrain-error:0.25103+0.00327\ttest-error:0.31174+0.01069\n",
      "[254]\ttrain-error:0.25058+0.00299\ttest-error:0.31197+0.01066\n",
      "[255]\ttrain-error:0.25070+0.00295\ttest-error:0.31181+0.01100\n",
      "[256]\ttrain-error:0.25012+0.00323\ttest-error:0.31135+0.01137\n",
      "[257]\ttrain-error:0.24996+0.00305\ttest-error:0.31119+0.01133\n",
      "[258]\ttrain-error:0.24981+0.00324\ttest-error:0.31096+0.01161\n",
      "[259]\ttrain-error:0.24940+0.00315\ttest-error:0.31096+0.01178\n",
      "[260]\ttrain-error:0.24940+0.00321\ttest-error:0.31166+0.01210\n",
      "[261]\ttrain-error:0.24866+0.00277\ttest-error:0.31150+0.01266\n",
      "[262]\ttrain-error:0.24846+0.00294\ttest-error:0.31158+0.01243\n",
      "[263]\ttrain-error:0.24825+0.00281\ttest-error:0.31205+0.01232\n",
      "[264]\ttrain-error:0.24799+0.00287\ttest-error:0.31174+0.01218\n",
      "[265]\ttrain-error:0.24761+0.00269\ttest-error:0.31205+0.01085\n",
      "[266]\ttrain-error:0.24766+0.00284\ttest-error:0.31251+0.01112\n",
      "[267]\ttrain-error:0.24743+0.00258\ttest-error:0.31213+0.01208\n",
      "[268]\ttrain-error:0.24749+0.00269\ttest-error:0.31236+0.01276\n",
      "[269]\ttrain-error:0.24692+0.00306\ttest-error:0.31220+0.01220\n",
      "[270]\ttrain-error:0.24692+0.00311\ttest-error:0.31251+0.01211\n",
      "[271]\ttrain-error:0.24653+0.00318\ttest-error:0.31236+0.01150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272]\ttrain-error:0.24614+0.00301\ttest-error:0.31275+0.01144\n",
      "[273]\ttrain-error:0.24593+0.00294\ttest-error:0.31228+0.01207\n",
      "[274]\ttrain-error:0.24574+0.00274\ttest-error:0.31228+0.01196\n",
      "[275]\ttrain-error:0.24529+0.00269\ttest-error:0.31251+0.01136\n",
      "[276]\ttrain-error:0.24541+0.00288\ttest-error:0.31236+0.01172\n",
      "[277]\ttrain-error:0.24517+0.00282\ttest-error:0.31259+0.01145\n",
      "[278]\ttrain-error:0.24484+0.00265\ttest-error:0.31267+0.01200\n",
      "[279]\ttrain-error:0.24504+0.00245\ttest-error:0.31259+0.01170\n",
      "[280]\ttrain-error:0.24504+0.00265\ttest-error:0.31283+0.01185\n",
      "[281]\ttrain-error:0.24455+0.00259\ttest-error:0.31251+0.01183\n",
      "[282]\ttrain-error:0.24441+0.00266\ttest-error:0.31298+0.01039\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.7371\n",
      "AUC Score (Train): 0.806295\n"
     ]
    }
   ],
   "source": [
    "# Determining NEW Optimal Number of Estimators\n",
    "xgb2 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=2, min_child_weight=3,\n",
    "                     gamma=0.0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n",
    "                     nthread=4, scale_pos_weight=1,seed=27)\n",
    "modelfit(xgb2, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  1.6min remaining:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  1.7min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  2.3min remaining:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:46:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'subsample': 0.7}, 0.6591398886071529)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validating SUBSAMPLE and COLSAMPLE_BYTREE\n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(7,9)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(7,9)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=282, max_depth=2, \n",
    "                                                  min_child_weight=3, gamma=0.0, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test4, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch4.fit(train[predictors],train[target])\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  25 | elapsed:  1.5min remaining: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  25 | elapsed:  1.6min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  25 | elapsed:  2.3min remaining:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:50:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 100}, 0.6756480636003925)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validating REG_ALPHA\n",
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=282, max_depth=2, \n",
    "                                                  min_child_weight=3, gamma=0.0, subsample=0.7, \n",
    "                                                  colsample_bytree=0.7, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test6, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch6.fit(train[predictors],train[target])\n",
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  35 | elapsed:  1.5min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  35 | elapsed:  3.2min remaining:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:55:59] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 100}, 0.6756480636003925)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validating REG_ALPHA\n",
    "param_test6 = {\n",
    " 'reg_alpha':[1, 2, 5, 10, 25, 50, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=282, max_depth=2, \n",
    "                                                  min_child_weight=3, gamma=0.0, subsample=0.7, \n",
    "                                                  colsample_bytree=0.7, objective= 'binary:logistic', \n",
    "                                                  nthread=4, scale_pos_weight=1,seed=27), \n",
    "                        param_grid = param_test6, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose = 3)\n",
    "gsearch6.fit(train[predictors],train[target])\n",
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the optimal number of trees for our final set of hyperparameters, but decrease the learning rate to 0.01 to get our final hyperparameters and run this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.35611+0.00759\ttest-error:0.36321+0.01368\n",
      "[1]\ttrain-error:0.34729+0.00540\ttest-error:0.35208+0.01187\n",
      "[2]\ttrain-error:0.34178+0.00800\ttest-error:0.34725+0.01427\n",
      "[3]\ttrain-error:0.33975+0.00624\ttest-error:0.34655+0.01388\n",
      "[4]\ttrain-error:0.33732+0.00264\ttest-error:0.34530+0.01364\n",
      "[5]\ttrain-error:0.33599+0.00301\ttest-error:0.34522+0.01395\n",
      "[6]\ttrain-error:0.33529+0.00270\ttest-error:0.34491+0.01446\n",
      "[7]\ttrain-error:0.33337+0.00169\ttest-error:0.34172+0.01755\n",
      "[8]\ttrain-error:0.33292+0.00135\ttest-error:0.34141+0.01555\n",
      "[9]\ttrain-error:0.33202+0.00135\ttest-error:0.34156+0.01610\n",
      "[10]\ttrain-error:0.33220+0.00138\ttest-error:0.34180+0.01615\n",
      "[11]\ttrain-error:0.33226+0.00094\ttest-error:0.34125+0.01588\n",
      "[12]\ttrain-error:0.33165+0.00111\ttest-error:0.34071+0.01417\n",
      "[13]\ttrain-error:0.33173+0.00129\ttest-error:0.33907+0.01395\n",
      "[14]\ttrain-error:0.33144+0.00177\ttest-error:0.34008+0.01154\n",
      "[15]\ttrain-error:0.33167+0.00140\ttest-error:0.33876+0.01034\n",
      "[16]\ttrain-error:0.33173+0.00155\ttest-error:0.33876+0.01052\n",
      "[17]\ttrain-error:0.33146+0.00134\ttest-error:0.33969+0.01251\n",
      "[18]\ttrain-error:0.33087+0.00144\ttest-error:0.33899+0.01015\n",
      "[19]\ttrain-error:0.33052+0.00127\ttest-error:0.33814+0.01160\n",
      "[20]\ttrain-error:0.33117+0.00128\ttest-error:0.33821+0.01234\n",
      "[21]\ttrain-error:0.33099+0.00137\ttest-error:0.33814+0.01280\n",
      "[22]\ttrain-error:0.33109+0.00143\ttest-error:0.33814+0.01308\n",
      "[23]\ttrain-error:0.33031+0.00184\ttest-error:0.33845+0.01280\n",
      "[24]\ttrain-error:0.33023+0.00209\ttest-error:0.33837+0.01220\n",
      "[25]\ttrain-error:0.33037+0.00208\ttest-error:0.33821+0.01219\n",
      "[26]\ttrain-error:0.33011+0.00149\ttest-error:0.33736+0.01166\n",
      "[27]\ttrain-error:0.33033+0.00157\ttest-error:0.33697+0.01211\n",
      "[28]\ttrain-error:0.32982+0.00105\ttest-error:0.33712+0.01094\n",
      "[29]\ttrain-error:0.33009+0.00143\ttest-error:0.33650+0.01017\n",
      "[30]\ttrain-error:0.32982+0.00148\ttest-error:0.33619+0.01112\n",
      "[31]\ttrain-error:0.32965+0.00150\ttest-error:0.33549+0.01128\n",
      "[32]\ttrain-error:0.32959+0.00151\ttest-error:0.33471+0.01099\n",
      "[33]\ttrain-error:0.32947+0.00164\ttest-error:0.33370+0.00991\n",
      "[34]\ttrain-error:0.32930+0.00159\ttest-error:0.33401+0.01044\n",
      "[35]\ttrain-error:0.32928+0.00154\ttest-error:0.33432+0.01012\n",
      "[36]\ttrain-error:0.32945+0.00204\ttest-error:0.33448+0.01057\n",
      "[37]\ttrain-error:0.32924+0.00257\ttest-error:0.33440+0.01046\n",
      "[38]\ttrain-error:0.32943+0.00194\ttest-error:0.33448+0.00989\n",
      "[39]\ttrain-error:0.32978+0.00170\ttest-error:0.33487+0.00995\n",
      "[40]\ttrain-error:0.33015+0.00151\ttest-error:0.33471+0.01014\n",
      "[41]\ttrain-error:0.32976+0.00130\ttest-error:0.33526+0.01004\n",
      "[42]\ttrain-error:0.32971+0.00139\ttest-error:0.33619+0.01086\n",
      "[43]\ttrain-error:0.33013+0.00155\ttest-error:0.33596+0.01122\n",
      "[44]\ttrain-error:0.33002+0.00174\ttest-error:0.33557+0.01054\n",
      "[45]\ttrain-error:0.33002+0.00168\ttest-error:0.33533+0.01096\n",
      "[46]\ttrain-error:0.32994+0.00170\ttest-error:0.33541+0.01106\n",
      "[47]\ttrain-error:0.32976+0.00176\ttest-error:0.33564+0.01092\n",
      "[48]\ttrain-error:0.32984+0.00169\ttest-error:0.33487+0.01045\n",
      "[49]\ttrain-error:0.32996+0.00161\ttest-error:0.33502+0.01076\n",
      "[50]\ttrain-error:0.32984+0.00211\ttest-error:0.33494+0.01005\n",
      "[51]\ttrain-error:0.32976+0.00212\ttest-error:0.33471+0.01040\n",
      "[52]\ttrain-error:0.32978+0.00221\ttest-error:0.33549+0.01014\n",
      "[53]\ttrain-error:0.32980+0.00226\ttest-error:0.33533+0.00994\n",
      "[54]\ttrain-error:0.32984+0.00189\ttest-error:0.33549+0.01018\n",
      "[55]\ttrain-error:0.33046+0.00201\ttest-error:0.33479+0.00966\n",
      "[56]\ttrain-error:0.32998+0.00152\ttest-error:0.33510+0.00965\n",
      "[57]\ttrain-error:0.33000+0.00180\ttest-error:0.33525+0.00896\n",
      "[58]\ttrain-error:0.33008+0.00176\ttest-error:0.33510+0.00955\n",
      "[59]\ttrain-error:0.32976+0.00185\ttest-error:0.33510+0.00974\n",
      "[60]\ttrain-error:0.32965+0.00181\ttest-error:0.33510+0.00922\n",
      "[61]\ttrain-error:0.32965+0.00181\ttest-error:0.33572+0.00931\n",
      "[62]\ttrain-error:0.32984+0.00200\ttest-error:0.33541+0.01006\n",
      "[63]\ttrain-error:0.32980+0.00180\ttest-error:0.33487+0.00979\n",
      "[64]\ttrain-error:0.32982+0.00163\ttest-error:0.33432+0.00983\n",
      "[65]\ttrain-error:0.32945+0.00180\ttest-error:0.33385+0.00974\n",
      "[66]\ttrain-error:0.32918+0.00202\ttest-error:0.33463+0.00955\n",
      "[67]\ttrain-error:0.32918+0.00183\ttest-error:0.33448+0.00953\n",
      "[68]\ttrain-error:0.32941+0.00178\ttest-error:0.33416+0.00939\n",
      "[69]\ttrain-error:0.32951+0.00189\ttest-error:0.33385+0.00944\n",
      "[70]\ttrain-error:0.32974+0.00212\ttest-error:0.33370+0.00930\n",
      "[71]\ttrain-error:0.32947+0.00222\ttest-error:0.33292+0.00992\n",
      "[72]\ttrain-error:0.32961+0.00210\ttest-error:0.33315+0.01053\n",
      "[73]\ttrain-error:0.32980+0.00209\ttest-error:0.33339+0.01046\n",
      "[74]\ttrain-error:0.32965+0.00190\ttest-error:0.33354+0.01136\n",
      "[75]\ttrain-error:0.32943+0.00212\ttest-error:0.33370+0.01155\n",
      "[76]\ttrain-error:0.32918+0.00203\ttest-error:0.33409+0.01095\n",
      "[77]\ttrain-error:0.32953+0.00207\ttest-error:0.33409+0.01081\n",
      "[78]\ttrain-error:0.32959+0.00229\ttest-error:0.33370+0.01037\n",
      "[79]\ttrain-error:0.32916+0.00224\ttest-error:0.33354+0.01052\n",
      "[80]\ttrain-error:0.32920+0.00198\ttest-error:0.33378+0.01026\n",
      "[81]\ttrain-error:0.32912+0.00192\ttest-error:0.33362+0.01026\n",
      "[82]\ttrain-error:0.32912+0.00205\ttest-error:0.33354+0.00999\n",
      "[83]\ttrain-error:0.32900+0.00195\ttest-error:0.33362+0.00958\n",
      "[84]\ttrain-error:0.32912+0.00178\ttest-error:0.33300+0.00935\n",
      "[85]\ttrain-error:0.32893+0.00180\ttest-error:0.33300+0.00925\n",
      "[86]\ttrain-error:0.32906+0.00216\ttest-error:0.33323+0.00962\n",
      "[87]\ttrain-error:0.32893+0.00209\ttest-error:0.33370+0.00919\n",
      "[88]\ttrain-error:0.32885+0.00210\ttest-error:0.33385+0.00897\n",
      "[89]\ttrain-error:0.32877+0.00231\ttest-error:0.33401+0.00919\n",
      "[90]\ttrain-error:0.32848+0.00225\ttest-error:0.33448+0.00905\n",
      "[91]\ttrain-error:0.32844+0.00203\ttest-error:0.33494+0.00851\n",
      "[92]\ttrain-error:0.32838+0.00220\ttest-error:0.33518+0.00839\n",
      "[93]\ttrain-error:0.32844+0.00218\ttest-error:0.33518+0.00778\n",
      "[94]\ttrain-error:0.32817+0.00202\ttest-error:0.33455+0.00818\n",
      "[95]\ttrain-error:0.32834+0.00208\ttest-error:0.33471+0.00833\n",
      "[96]\ttrain-error:0.32848+0.00199\ttest-error:0.33448+0.00837\n",
      "[97]\ttrain-error:0.32850+0.00207\ttest-error:0.33448+0.00890\n",
      "[98]\ttrain-error:0.32854+0.00226\ttest-error:0.33424+0.00931\n",
      "[99]\ttrain-error:0.32873+0.00242\ttest-error:0.33416+0.00924\n",
      "[100]\ttrain-error:0.32873+0.00249\ttest-error:0.33432+0.00907\n",
      "[101]\ttrain-error:0.32860+0.00233\ttest-error:0.33440+0.00872\n",
      "[102]\ttrain-error:0.32840+0.00242\ttest-error:0.33401+0.00859\n",
      "[103]\ttrain-error:0.32823+0.00242\ttest-error:0.33370+0.00886\n",
      "[104]\ttrain-error:0.32840+0.00232\ttest-error:0.33385+0.00891\n",
      "[105]\ttrain-error:0.32836+0.00228\ttest-error:0.33354+0.00897\n",
      "[106]\ttrain-error:0.32834+0.00214\ttest-error:0.33315+0.00909\n",
      "[107]\ttrain-error:0.32805+0.00201\ttest-error:0.33354+0.00889\n",
      "[108]\ttrain-error:0.32772+0.00218\ttest-error:0.33393+0.00891\n",
      "[109]\ttrain-error:0.32793+0.00237\ttest-error:0.33378+0.00920\n",
      "[110]\ttrain-error:0.32784+0.00217\ttest-error:0.33385+0.00916\n",
      "[111]\ttrain-error:0.32782+0.00184\ttest-error:0.33409+0.00911\n",
      "[112]\ttrain-error:0.32788+0.00203\ttest-error:0.33416+0.00908\n",
      "[113]\ttrain-error:0.32786+0.00200\ttest-error:0.33393+0.00924\n",
      "[114]\ttrain-error:0.32803+0.00207\ttest-error:0.33416+0.00932\n",
      "[115]\ttrain-error:0.32791+0.00207\ttest-error:0.33409+0.00935\n",
      "[116]\ttrain-error:0.32776+0.00184\ttest-error:0.33385+0.00891\n",
      "[117]\ttrain-error:0.32762+0.00169\ttest-error:0.33377+0.00928\n",
      "[118]\ttrain-error:0.32772+0.00175\ttest-error:0.33424+0.00924\n",
      "[119]\ttrain-error:0.32753+0.00186\ttest-error:0.33393+0.00939\n",
      "[120]\ttrain-error:0.32774+0.00177\ttest-error:0.33362+0.00946\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.6712\n",
      "AUC Score (Train): 0.708581\n"
     ]
    }
   ],
   "source": [
    "# Determining Optimal Number of Trees w/ New (LOWER) Learning Rate \n",
    "\n",
    "xgb3 = XGBClassifier(learning_rate = 0.005, n_estimators = 5000, max_depth = 2, min_child_weight = 3, gamma = 0.0, \n",
    "                     subsample = 0.7, colsample_bytree = 0.7, reg_alpha = 100, objective = 'binary:logistic', \n",
    "                     nthread=4, scale_pos_weight = 1, seed = 69)\n",
    "modelfit(xgb3, train, predictors, useTrainCV = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:58:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 497  582]\n",
      " [ 316 1229]]\n",
      "\n",
      "Accuracy :  0.6578\n"
     ]
    }
   ],
   "source": [
    "# Running our Final XgBoost Model\n",
    "xgb5 = XGBClassifier(learning_rate = 0.005, n_estimators = 120, max_depth = 2, min_child_weight = 3, gamma = 0.0,\n",
    "                     subsample = 0.7, colsample_bytree = 0.7, reg_alpha = 100, objective = 'binary:logistic',\n",
    "                     nthread = 4, scale_pos_weight = 1, seed = 69)\n",
    "\n",
    "model_3 = xgb5\n",
    "model_3.fit(X_train_3, y_train_3)\n",
    "y_prob_3 = model_3.predict_proba(X_test)\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "outputs_3 = outputs(cm_3)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting (round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  27 | elapsed: 10.8min remaining: 37.9min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  27 | elapsed: 31.0min remaining: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 50.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:16:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"max_leaf_nodes\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:16:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Optimal Hyperparameters:  {'max_leaf_nodes': 4, 'n_estimators': 1000} 0.6676366672208885\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 511  568]\n",
      " [ 334 1211]]\n",
      "\n",
      "Accuracy :  0.6562\n"
     ]
    }
   ],
   "source": [
    "# Running our Final XgBoost Model\n",
    "grid = {'n_estimators': [500, 1000, 2500], \n",
    "        'max_leaf_nodes': [4, 5, 6]}\n",
    "\n",
    "model_3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.005, objective= 'binary:logistic', seed=6969), \n",
    "                        param_grid = grid, scoring='accuracy', n_jobs=-1, iid=False, cv=3, verbose = 3)\n",
    "model_3.fit(train[predictors],train[target])\n",
    "print('Optimal Hyperparameters: ', model_3.best_params_, model_3.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "y_prob_3 = model_3.best_estimator_.predict_proba(X_test)\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "outputs_3 = outputs(cm_3)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Running our Final XgBoost Model\n",
    "# grid = {'n_estimators': [485], \n",
    "#         'max_leaf_nodes': [2,3,4]}\n",
    "\n",
    "# model_3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.005, objective= 'binary:logistic', seed=6969), \n",
    "#                         param_grid = grid, scoring='accuracy', n_jobs=-1, iid=False, cv=3, verbose = 3)\n",
    "# model_3.fit(train[predictors],train[target])\n",
    "# print('Optimal Hyperparameters: ', model_3.best_params_, model_3.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "# y_prob_3 = model_3.best_estimator_.predict_proba(X_test)\n",
    "# y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# # Confusion Matrix & Outputs \n",
    "# cm_3 = confusion_matrix(y_test, y_pred_3)\n",
    "# outputs_3 = outputs(cm_3)\n",
    "# print (\"\\nConfusion Matrix : \\n\", cm_3) \n",
    "# print (\"\\nAccuracy : \", outputs_3[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \\#4: Neural Network\n",
    "\n",
    "The cells below go through an entire process of creating and cross-validating our neural network model using Sci-Kit Learn's MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 588  491]\n",
      " [ 429 1116]]\n",
      "\n",
      "Accuracy :  0.6494\n"
     ]
    }
   ],
   "source": [
    "# Neural Network \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# model_3 = MLPClassifier(hidden_layer_sizes =(75,), max_iter = 200,\n",
    "#                         activation = 'relu', solver = 'adam', random_state = 69)\n",
    "# model_3.fit(X_train, y_train)\n",
    "# y_prob_3 = model_3.predict_proba(X_test)\n",
    "# y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_3[:,1]], index = y_test.index)\n",
    "\n",
    "# grid = {'hidden_layer_sizes': [(50,),(100,),(200,),(400,),(800,),(50,50),(100,50)]}\n",
    "\n",
    "model_4 = MLPClassifier(max_iter = 100, alpha = 0.0001, activation = 'relu', hidden_layer_sizes=(25,), \n",
    "                        solver = 'adam', random_state = 69)\n",
    "# model_4 = GridSearchCV(model_4, grid, n_jobs = -1, cv = 2, verbose = 3)\n",
    "model_4.fit(X_train, y_train)\n",
    "\n",
    "y_prob_4 = model_4.predict_proba(X_test)\n",
    "# print(model_4.best_params_)\n",
    "# y_pred_4 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_4[:,1]])\n",
    "y_pred_3 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_4[:,1]], index = y_test.index)\n",
    "# Confusion Matrix & Outputs \n",
    "cm_4 = confusion_matrix(y_test, y_pred_4)\n",
    "outputs_4 = outputs(cm_4)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_4) \n",
    "print (\"\\nAccuracy : \", outputs_4[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Reloading the data\n",
    "\n",
    "# # Splitting Data\n",
    "# FP1 = pd.read_csv('FP1.csv')\n",
    "# FP2 = pd.read_csv('FP2.csv')\n",
    "# df = pd.merge(FP1, FP2, on = 'GAME_ID')\n",
    "# df = df.drop(['TIMESTAMP','HOME_TEAM_NAME'], axis=1)\n",
    "\n",
    "# train_df = df[df['SEASON'].between(2010,2016)]\n",
    "# val_df = df[df['SEASON'] == 2017]\n",
    "# test_df = df[df['SEASON'] == 2018]\n",
    "\n",
    "\n",
    "# train_df, val_df, test_df = train_df.drop(['SEASON', 'GAME_ID'], axis = 1), val_df.drop(['SEASON', 'GAME_ID'], axis = 1), test_df.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "# X_train, y_train = train_df.drop('HOME_TEAM_WINS', axis = 1), train_df['HOME_TEAM_WINS']\n",
    "# X_val, y_val = val_df.drop('HOME_TEAM_WINS', axis = 1), val_df['HOME_TEAM_WINS']\n",
    "# X_test, y_test = test_df.drop('HOME_TEAM_WINS', axis = 1), test_df['HOME_TEAM_WINS']\n",
    "\n",
    "# train_prop = np.round(len(X_train) / (len(df)), 3)\n",
    "# val_prop = np.round(len(X_val) / (len(df)), 3)\n",
    "# test_prop = np.round(len(X_test) / (len(df)), 3)\n",
    "\n",
    "# print('Training Data: ' + str(len(X_train)) + ' rows -- ' + str(np.round(train_prop*100,2)) + '%')\n",
    "# print('Validation Data: ' + str(len(X_val)) + ' rows -- ' + str(np.round(val_prop*100,2)) + '%')\n",
    "# print('Testing Data: ' + str(len(X_test)) + ' rows -- ' + str(np.round(test_prop*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Creating blend_df \n",
    "y_prob_1_val = model_1.predict_proba(X_val)\n",
    "y_prob_2_val = model_2.predict_proba(X_val)\n",
    "y_prob_3_val = model_3.predict_proba(X_val)\n",
    "y_prob_4_val = model_4.predict_proba(X_val)\n",
    "\n",
    "\n",
    "\n",
    "val_df_blend = pd.DataFrame({'Logistic Regression': list(y_prob_1_val[:,1]), \n",
    "                             'Random Forest': list(y_prob_2_val[:,1]), \n",
    "                             'XGBoost': list(y_prob_3_val[:,1]), \n",
    "                             'Neural Network': y_prob_4_val[:,1], \n",
    "                             'HOME_TEAM_WINS': y_val})\n",
    "test_df_blend = pd.DataFrame({'Logistic Regression': y_prob_1[:,1], \n",
    "                             'Random Forest': y_prob_2[:,1], \n",
    "                             'XGBoost': y_prob_3[:,1], \n",
    "                             'Neural Network': y_prob_4[:,1], \n",
    "                             'HOME_TEAM_WINS': y_test})\n",
    "X_val_blend, y_val_blend = val_df_blend.drop('HOME_TEAM_WINS', axis=1), val_df_blend['HOME_TEAM_WINS']\n",
    "X_test_blend, y_test_blend = test_df_blend.drop('HOME_TEAM_WINS', axis=1), test_df_blend['HOME_TEAM_WINS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix : \n",
      " [[ 490  589]\n",
      " [ 289 1256]]\n",
      "\n",
      "Accuracy :  0.6654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Blended Logistic Regression\n",
    "\n",
    "model_5 = LogisticRegression(random_state = 69,  verbose = 3, n_jobs = -1)\n",
    "model_5.fit(X_val_blend, y_val_blend)\n",
    "y_prob_5 = model_5.predict_proba(X_test_blend)\n",
    "y_pred_5 = pd.Series([1 if x > 0.5 else 0 for x in y_prob_5[:,1]], index = y_test_blend.index)\n",
    "\n",
    "# Confusion Matrix & Outputs \n",
    "cm_5 = confusion_matrix(y_test_blend, y_pred_5)\n",
    "outputs_5 = outputs(cm_5)\n",
    "print (\"\\nConfusion Matrix : \\n\", cm_5) \n",
    "print (\"\\nAccuracy : \", outputs_5[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Results\n",
    "\n",
    "The cell below calculates the Accuracy (our primary performance metric), TPR, and FPR for each of the four models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Generating the Output Table\n",
    "# df = pd.DataFrame({'Baseline Model': np.round(outputs_0,4),\n",
    "#                   'Logistic Regression': np.round(outputs_1,4),\n",
    "#                   'Random Forest': np.round(outputs_2,4),\n",
    "#                   'XGBoost': np.round(outputs_3,4),\n",
    "#                   'Neural Network': np.round(outputs_4,4)}, index = ['Accuracy', 'TPR', 'FPR'], ).T\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'n_estimators': np.linspace(100, 5000, 5, dtype='int32'),  # np.logspace(6, 12, num=7, base=2, dtype='int32'),\n",
    "               'learning_rate': [0.01],\n",
    "               'max_leaf_nodes': np.linspace(2, 10, 8, dtype='int32'),\n",
    "               'max_depth': [20],\n",
    "               'min_samples_leaf': [15],\n",
    "               'random_state': [88]} \n",
    "\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_cv = GridSearchCV(gbc, param_grid=grid_values, cv=5)\n",
    "gbc_cv.fit(X_train, y_train)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('time:', round(toc-tic, 2),'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chris XGBoosting Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Combining train and val\n",
    "\n",
    "train_df_3 = df[df['SEASON'].between(2007,2016)]\n",
    "\n",
    "train_df_3 = train_df_3.drop(['SEASON', 'GAME_ID'], axis = 1)\n",
    "X_train_3, y_train_3 = train_df_3.drop('HOME_TEAM_WINS', axis = 1), train_df_3['HOME_TEAM_WINS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.1min\n"
     ]
    }
   ],
   "source": [
    "## Model Training ##\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "grid_values_boost = {'n_estimators': [10, 50, 100, 250, 500, 1000, 2000, 4000, 5000], \n",
    "#                      'n_estimators': np.linspace(1000, 10000, 10, dtype='int32'),\n",
    "                     'learning_rate': [0.005],\n",
    "                     'max_leaf_nodes': [2, 4, 6, 8],\n",
    "                     'max_depth': [20],\n",
    "                     'min_samples_leaf':[15], \n",
    "                     'random_state': [69]}    \n",
    "\n",
    "boost = GradientBoostingClassifier()\n",
    "# cv = KFold(n_splits=5,random_state=333,shuffle=False) \n",
    "boost_cv = GridSearchCV(boost, param_grid=grid_values_boost, \n",
    "                        scoring='accuracy', cv=5, verbose=2, n_jobs=-1)\n",
    "boost_cv.fit(X_train_3,y_train_3)\n",
    "toc = time.time()\n",
    "print('Time: '+str((toc-tic)/60)+' min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
